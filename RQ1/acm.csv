type,id,abstract,address,articleno,author,booktitle,doi,isbn,issn,issue_date,journal,keywords,location,note,number,numpages,pages,publisher,series,title,url,volume,year
inproceedings,10.1145/3539618.3591784,"Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.","New York, NY, USA",,"Zhang, Zhao and Guan, Zhanpeng and Zhang, Fuwei and Zhuang, Fuzhen and An, Zhulin and Wang, Fei and Xu, Yongjun",Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval,10.1145/3539618.3591784,9781450394086,,,,"bilevel optimization, knowledge graph embedding, link prediction","Taipei, Taiwan",,,11,867–877,Association for Computing Machinery,SIGIR '23,Weighted Knowledge Graph Embedding,https://doi.org/10.1145/3539618.3591784,,2023
inproceedings,10.1145/3711896.3737046,"Multivariate Time Series Forecasting (MTSF) involves predicting future values of multiple interrelated time series. Recently, deep learning-based MTSF models have gained significant attention for their promising ability to mine semantics (global and local information) within MTS data. However, these models are pervasively susceptible to missing values caused by malfunctioning data collectors. These missing values not only disrupt the semantics of MTS, but their distribution also changes over time. Nevertheless, existing models lack robustness to such issues, leading to suboptimal forecasting performance. To this end, in this paper, we propose Multi-View Representation Learning (Merlin), which can help existing models achieve semantic alignment between incomplete observations with different missing rates and complete observations in MTS. Specifically, Merlin consists of two key modules: offline knowledge distillation and multi-view contrastive learning. The former utilizes a teacher model to guide a student model in mining semantics from incomplete observations, similar to those obtainable from complete observations. The latter improves the student model's robustness by learning from positive/negative data pairs constructed from incomplete observations with different missing rates, ensuring semantic alignment across different missing rates. Therefore, Merlin is capable of effectively enhancing the robustness of existing models against unfixed missing rates while preserving forecasting accuracy. Experiments on four real-world datasets demonstrate the superiority of Merlin.","New York, NY, USA",,"Yu, Chengqing and Wang, Fei and Yang, Chuanguang and Shao, Zezhi and Sun, Tao and Qian, Tangwen and Wei, Wei and An, Zhulin and Xu, Yongjun",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737046,9798400714542,,,,"multi-view contrastive learning, multi-view representation learning, multivariate time series forecasting with unfixed missing rates, offline knowledge distillation","Toronto ON, Canada",,,12,3633–3644,Association for Computing Machinery,KDD '25,Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates,https://doi.org/10.1145/3711896.3737046,,2025
article,10.1145/3718091,"Real-world time series data is inherently complex, noisy, and exhibits abrupt changes, posing various challenges in data modeling. Given the ubiquity and importance of time series data, accurately forecasting change points, instead of the overall predictive performance, has become increasingly attractive as it assists in risk mitigation and loss prevention. In this task, we argue that the past and future interactions involving the target points determine the comprehensive structure contributing to abrupt changes. However, traditional left-to-right auto-regressive approaches only consider the historical sequence, resulting in a flawed learning process and limited performance.In this article, we extend the teacher-student learning and propose a novel Self-optimizing Teacher and Auto-matching Student framework (named ST-AS) to predict change points in time series data. Our framework models change-point representations specific to the target points by integrating future knowledge while avoiding data leakage. Specifically, we design a Gumbel-enhanced filter for our self-optimizing teacher, which constructs selected and filtered sub-groups to derive discriminative representations using a positive-unlabeled learning strategy. Given this well-trained teacher, we propose an adaptive pattern matcher for our auto-matching student model, which learns missing information by automatically aligning relevant features. After that, a novel two-stage dual-guided learning process is then designed to mimic teacher’s decision-making behavior and enhance student’s excavate capability. Finally, we conduct extensive experiments on four real-world datasets to demonstrate that our proposed ST-AS exhibits significantly better prediction performance compared to existing state-of-the-art alternatives.","New York, NY, USA",72,"Fan, Jinxiao and Wang, Pengfei and Liu, Liang and Ma, Huadong",,10.1145/3718091,,2157-6904,June 2025,ACM Trans. Intell. Syst. Technol.,"Time series forecasting, Teacher-student framework, Representation learning",,,3,25,,Association for Computing Machinery,,Self-Optimizing Teacher and Auto-Matching Student Framework for Change-Point Representation Learning in Time Series Forecasting,https://doi.org/10.1145/3718091,16,2025
inproceedings,10.1145/3746252.3761007,"Multivariate time series (MTS) forecasting is crucial for predicting the future states of complexly coupled variables based on historical observations. To effectively capture the intricate interdependencies within MTS, graph-based methods have emerged as powerful tools. However, existing graph construction methods often produce structures that fail to preserve key temporal and cross-variable dependencies, introducing redundant or irrelevant connections. To address these challenges, we propose a structural entropy-based approach for MTS forecasting. The approach optimizes graph structures by reducing structural redundancy, thereby improving forecasting accuracy. Initially, we represent the temporal dependences by constructing an encoding tree incrementally. Through hierarchical organization of time steps, the temporal evolution is adaptively captured. Subsequently, we give the community-aware representation by building an encoding tree over the variables in MTS, extracting homogeneous communities from the tree structure while integrating community influence to better capture inter-variable dependencies. Finally,we present a training algorithm designed to generate accurate predictions for MTS, accompanied by a unified loss function that integrates forecasting inaccuracies with variations in structural entropy. Empirical findings on real-world datasets substantiate that our approach outperforms state-of-the-art models in capturing dependencies and enhancing forecasting precision.","New York, NY, USA",,"Li, Xinhui and Yue, Kun and Yu, Lixing and Yang, Peizhong",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761007,9798400720406,,,,"encoding tree, interseries correlation, intra-series correlation, multivariate time series forecasting, structural entropy","Seoul, Republic of Korea",,,10,1696–1705,Association for Computing Machinery,CIKM '25,Structural Entropy-based Multivariate Time Series Forecasting,https://doi.org/10.1145/3746252.3761007,,2025
inproceedings,10.1145/3690624.3709202,"Leveraging graph structures for time-series forecasting has garnered significant attention due to their effective relationship modeling between nodes and their associated time-series. However, in scenarios entities communicate in a broadcasting manner, graph models fall short of pairwise modeling. Hypergraph models address this by capturing beyond-pairwise interactions among node time-series. Nevertheless, most hypergraph models overlook the dynamics between nodes and their incident hyperedges, assuming constant node-hyperedge connections. In this paper, we introduce a novel model, Probabilistic Hypergraph Recurrent Neural Networks (PHRNN), which leverages node-hyperedge dynamics for accurate time-series forecasting. PHRNN associates each time-series with a node and models node interactions on a hypergraph, capturing beyond-pairwise interactions. Moreover, PHRNN learns a probabilistic hypergraph in which node-hyperedge relations are modeled as probabilistic distributions instead of fixed values, capturing dynamic node-hyperedge relations. PHRNN further integrates a prior knowledge KNN hypergraph as regularization when learning the probabilistic hypergraph structure. To the best of our knowledge, PHRNN is the first time-series forecasting model that incorporates hypergraph modeling and probabilistic relationship modeling. Forecasting results from extensive experiments show that PHRNN outperforms state-of-the-art graph and hypergraph baselines on real-world datasets.","New York, NY, USA",,"Chen, Hongjie and Rossi, Ryan A. and Kim, Sungchul and Mahadik, Kanak and Eldardiry, Hoda",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709202,9798400712456,,,,"hypergraph neural networks, probabilistic modeling, time-series forecasting","Toronto ON, Canada",,,12,82–93,Association for Computing Machinery,KDD '25,Probabilistic Hypergraph Recurrent Neural Networks for Time-series Forecasting,https://doi.org/10.1145/3690624.3709202,,2025
inproceedings,10.1145/3711896.3737137,"Recent successes in diffusion probabilistic models have demonstrated their strength in modeling and generating different types of data, paving the way for their application in generative time series forecasting. However, most existing diffusion based approaches rely on sequential models and unimodal latent variables to capture global dependencies and model entire observable data, resulting in difficulties when it comes to highly stochastic time series data. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model that integrates the diffusion process into time series modeling stage and utilizes the representational power of the stochastic latent spaces to capture the variability of the stochastic time series data. Specifically, the model applies diffusion module at each time step within the sequential framework and learns a step-wise, data-driven prior for generative diffusion process. These features enable the model to effectively capture complex temporal dynamics and the multi-modal nature of the highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model for probabilistic time series forecasting, particularly in scenarios with high stochasticity. Additionally, with a real-world surgical use case, we highlight the model's potential in a medical application.","New York, NY, USA",,"Liu, Yuansan and Wijewickrema, Sudanthi and Hu, Dongting and Bester, Christofer and O'Leary, Stephen and Bailey, James",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737137,9798400714542,,,,"diffusion probabilistic model, latent representation learning, stochastic time series, time series forecasting","Toronto ON, Canada",,,12,1939–1950,Association for Computing Machinery,KDD '25,Stochastic Diffusion: A Diffusion Based Model for Stochastic Time Series Forecasting,https://doi.org/10.1145/3711896.3737137,,2025
inproceedings,10.1145/3759928.3759960,"In recent years, Transformer architectures have attracted considerable attention in multivariate time series forecasting due to their powerful sequence modeling capabilities. However, existing methods primarily focus on temporal dependencies while often overlooking complex lagged relationships among variables, where the current state of a variable is mainly influenced by the historical state of other variables at certain previous time steps, indicating strong lagged correlations among them. To address this limitation, we propose STLAformer, a novel model based on a Segment-based Temporal Lag Attention (STLA) mechanism. By partitioning multivariate time series into local segments and introducing a delay-aware attention module, STLAformer effectively captures cross-variable lagged dependencies. Additionally, we design a Temporal Preservation Segmentation Embedding (TPSE) module to independently encode each segment, preserving local dynamic features, and incorporate learnable temporal encoding to enhance the model's representation of temporal structures. Experimental results demonstrate that our approach achieves significant improvements in modeling capability and forecasting accuracy over existing Transformer variants on multiple real-world multivariate time series forecasting tasks, especially exhibiting stronger generalization ability when handling complex lag dependencies.","New York, NY, USA",,"Zhao, Yuhui and Sun, Lijie","Proceedings of the 2nd International Conference on Image Processing, Machine Learning, and Pattern Recognition",10.1145/3759928.3759960,9798400715884,,,,"Multivariate Dependency, Temporal Lag Attention, Time Series Forecasting, Transformer","
",,,6,186–191,Association for Computing Machinery,IPMLP '25,STLAformer: Segment-based Temporal Lag Attention effective for time series forecasting,https://doi.org/10.1145/3759928.3759960,,2025
article,10.1145/3643035,"Domain generalization aims to design models that can effectively generalize to unseen target domains by learning from observed source domains. Domain generalization poses a significant challenge for time series data, due to varying data distributions and temporal dependencies. Existing approaches to domain generalization are not designed for time series data, which often results in suboptimal or unstable performance when confronted with diverse temporal patterns and complex data characteristics. We propose a novel approach to tackle the problem of domain generalization in time series forecasting. We focus on a scenario where time series domains share certain common attributes and exhibit no abrupt distribution shifts. Our method revolves around the incorporation of a key regularization term into an existing time series forecasting model: domain discrepancy regularization. In this way, we aim to enforce consistent performance across different domains that exhibit distinct patterns. We calibrate the regularization term by investigating the performance within individual domains and propose the domain discrepancy regularization with domain difficulty awareness. We demonstrate the effectiveness of our method on multiple datasets, including synthetic and real-world time series datasets from diverse domains such as retail, transportation, and finance. Our method is compared against traditional methods, deep learning models, and domain generalization approaches to provide comprehensive insights into its performance. In these experiments, our method showcases superior performance, surpassing both the base model and competing domain generalization models across all datasets. Furthermore, our method is highly general and can be applied to various time series models.","New York, NY, USA",113,"Deng, Songgaojun and Sprangers, Olivier and Li, Ming and Schelter, Sebastian and de Rijke, Maarten",,10.1145/3643035,,1556-4681,June 2024,ACM Trans. Knowl. Discov. Data,"Time series forecasting, domain generalization, regularization",,,5,24,,Association for Computing Machinery,,Domain Generalization in Time Series Forecasting,https://doi.org/10.1145/3643035,18,2024
inproceedings,10.1145/3746252.3761261,"Financial time-series forecasting is critical for maintaining economic stability, guiding informed policymaking, and promoting sustainable investment practices. However, it remains challenging due to various underlying pattern shifts. These shifts arise primarily from three sources: temporal non-stationarity (distribution changes over time), multi-domain diversity (distinct patterns across financial domains such as stocks, commodities, and futures), and varying temporal resolutions (patterns differing across per-second, hourly, daily, or weekly indicators). While recent deep learning methods attempt to address these complexities, they frequently suffer from overfitting and typically require extensive domain-specific fine-tuning. To overcome these limitations, we introduce FinCast, the first foundation model specifically designed for financial time-series forecasting, trained on large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot performance, effectively capturing diverse patterns without domain-specific fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate that FinCast surpasses existing state-of-the-art methods, highlighting its strong generalization capabilities.","New York, NY, USA",,"Zhu, Zhuohang and Chen, Haodong and Qu, Qiang and Chung, Vera",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761261,9798400720406,,,,"computational finance, decoder-only transformer, financial timeseries forecast, foundation models, mixture of experts","Seoul, Republic of Korea",,,11,4539–4549,Association for Computing Machinery,CIKM '25,FinCast: A Foundation Model for Financial Time-Series Forecasting,https://doi.org/10.1145/3746252.3761261,,2025
inproceedings,10.1145/3746252.3761055,"Multivariate long-term time series forecasting is pivotal across numerous domains, yet precise predictions require a differentiated assessment of historical time segments due to their varying influence on future trends. Patch-based Transformer frameworks show promise for capturing local temporal patterns. However, they face limitations with static patching, which disrupts temporal continuity, fails to adapt to shifts between periodic and volatile patterns, and overlooks dynamic interactions between time segments and variables. To address these limitations, we propose Entropy-Aware Patch Transformer (EAPformer) which dynamically segments time series for differentiated assessments of historical patterns. Specifically, we overcome static patching limitations by leveraging temporal entropy to dynamically adjust patch boundaries through a two-stage policy, achieving interpretable and context-sensitive segmentation. Subsequently, we adapt EAPformer to periodic and volatile dynamics by employing entropy-aware segmentation that captures distinct temporal patterns across diverse segments. Finally, we further capture dynamic interactions across time segments and variables by introducing a multi-dimensional dependency learning architecture. Additionally, a gated fusion mechanism integrates local and global patterns, enhancing robustness. Extensive experiments on eight public benchmarks demonstrate that EAPformer outperforms state-of-the-art models, achieving superior accuracy across all metrics.","New York, NY, USA",,"Ling, Jiahao and Yang, Xuan and Gong, Shimin and Gu, Bo",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761055,9798400720406,,,,"entropy-aware, multivariate long-term time series forecasting, patch, transformer","Seoul, Republic of Korea",,,11,1850–1860,Association for Computing Machinery,CIKM '25,EAPformer: Entropy-Aware Patch Transformer for Multivariate Long-Term Time Series Forecasting,https://doi.org/10.1145/3746252.3761055,,2025
inproceedings,10.1145/3746252.3761360,"Time series forecasting has witnessed significant advancements through deep learning techniques. However, most existing methods struggle in non-stationary environments, where data distributions evolve over time due to concept drift. To address the challenge of non-stationarity in time series, various stabilization techniques have been proposed to mitigate temporal variations. Nonetheless, these methods operate at the instance level, assuming a homogeneous distribution across all time steps within an instance and relying on fixed statistical normalization. This limits their ability to effectively capture fine-grained distributional shifts.In this paper, we introduce AdaPatch, a novel forecasting model specifically designed to tackle non-stationary multivariate time series. AdaPatch addresses intra-instance distributional shifts by adopting an adaptive scheme for patch-level encoding and normalization, which makes the model capture fine-grained temporal variations more effectively. To further enhance the quality of representations, AdaPatch incorporates a patch reconstruction branch and jointly optimizes a reconstruction loss alongside the forecasting objective. This auxiliary path serves as an implicit regularization mechanism, guiding the encoder to retain meaningful local temporal structures. Furthermore, to enable AdaPatch to better model complex local dynamics, we propose a patch-based predictive decoding strategy that leverages the decoder from the reconstruction branch to replace conventional point-wise forecasting with a more structured patch-level prediction mechanism. Extensive experiments conducted on six real-world multivariate time series datasets demonstrate that AdaPatch achieves superior performance compared to several state-of-the-art baselines, highlighting its effectiveness and strong generalization capability. Our code and data are publicly available at https://github.com/iuaku/AdaPatch.","New York, NY, USA",,"Liu, Kun and Duan, Zhongjie and Chen, Cen and Wang, Yanhao and Cheng, Dawei and Liang, Yuqi",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761360,9798400720406,,,,"adaptive normalization, multi-layer perceptron, multivariate time series forecasting, non-stationarity, patch-level modeling","Seoul, Republic of Korea",,,10,1882–1891,Association for Computing Machinery,CIKM '25,AdaPatch: Adaptive Patch-Level Modeling for Non-Stationary Time Series Forecasting,https://doi.org/10.1145/3746252.3761360,,2025
article,10.1145/3630637,"Many real-world applications require precise and fast time-series forecasting. Recent trends in time-series forecasting models are shifting from LSTM-based models to Transformer-based models. However, the Transformer-based model has a limited ability to represent sequential relationships in time-series data. In addition, the transformer-based model suffers from slow training and inference speed due to the bottleneck incurred by a deep encoder and step-by-step decoder inference. To address these problems, we propose a time-series forecasting optimized Transformer model, called TS-Fastformer. TS-Fastformer introduces three new optimizations: First, we propose a Sub Window Tokenizer for compressing input in a simple manner. The Sub Window Tokenizer reduces the length of input sequences to mitigate the complexity of self-attention and enables both single and multi-sequence learning. Second, we propose Time-series Pre-trained Encoder to extract effective representations through pre-training. This optimization enables TS-Fastformer to capture both seasonal and trend representations as well as to mitigate bottlenecks of conventional transformer models. Third, we propose the Past Attention Decoder to forecast target by incorporating past long short-term dependency patterns. Furthermore, Past Attention Decoder achieves high performance improvement by removing a trend distribution that changes over a long period. We evaluate the efficiency of our model with extensive experiments using seven real-world datasets and compare our model to six representative time-series forecasting approaches. The results show that the proposed TS-Fastformer reduces MSE by 10.1\% compared to state-of-the-art model and demonstrates 21.6\% faster training time compared to the existing fastest transformer, respectively.","New York, NY, USA",24,"Lee, Sangwon and Hong, Junho and Liu, Ling and Choi, Wonik",,10.1145/3630637,,2157-6904,April 2024,ACM Trans. Intell. Syst. Technol.,"Deep learning, transformer, time-series forecasting, time-series representation",,,2,20,,Association for Computing Machinery,,TS-Fastformer: Fast Transformer for Time-series Forecasting,https://doi.org/10.1145/3630637,15,2024
inproceedings,10.1145/3637528.3671855,"Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.  To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enablinga single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretraining time series large models. We make our codes publicly availablefootnotehttps://github.com/icantnamemyself/GPHT.","New York, NY, USA",,"Liu, Zhiding and Yang, Jiqian and Cheng, Mingyue and Luo, Yucong and Li, Zhi",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671855,9798400704901,,,,"deep learning, pretraining, time series forecasting","Barcelona, Spain",,,11,2003–2013,Association for Computing Machinery,KDD '24,Generative Pretrained Hierarchical Transformer for Time Series Forecasting,https://doi.org/10.1145/3637528.3671855,,2024
article,10.1145/3701038,"Multivariate time series (MTS) forecasting has attracted much attention in many intelligent applications. It is not a trivial task, as we need to consider both intra-variable dependencies and inter-variable dependencies. However, existing works are designed for specific scenarios and require much domain knowledge and expert efforts, which is difficult to transfer between different scenarios. In this article, we propose a scale-aware neural architecture search framework for MTS forecasting (SNAS4MTF). A multi-scale decomposition module transforms raw time series into multi-scale sub-series, which can preserve multi-scale temporal patterns. An adaptive graph learning module infers the different inter-variable dependencies under different time scales without any prior knowledge. For MTS forecasting, a search space is designed to capture both intra-variable dependencies and inter-variable dependencies at each time scale. The multi-scale decomposition, adaptive graph learning, and neural architecture search modules are jointly learned in an end-to-end framework. Extensive experiments on two real-world datasets demonstrate that SNAS4MTF achieves a promising performance compared with the state-of-the-art methods.","New York, NY, USA",11,"Chen, Donghui and Chen, Ling and Shang, Zongjiang and Zhang, Youdong and Wen, Bo and Yang, Chenghu",,10.1145/3701038,,1556-4681,January 2025,ACM Trans. Knowl. Discov. Data,"Multivariate time series forecasting, neural architecture search, graph learning, multi-scale decomposition",,,1,23,,Association for Computing Machinery,,Scale-Aware Neural Architecture Search for Multivariate Time Series Forecasting,https://doi.org/10.1145/3701038,19,2024
inproceedings,10.1145/3746252.3761410,"Time series forecasting is critical across various domains, such as weather, finance and real estate forecasting, as accurate forecasts support informed decision-making and risk mitigation. While recent deep learning models have improved predictive capabilities, they often overlook time-lagged cross-correlations between related sequences, which are crucial for capturing complex temporal relationships. To address this, we propose the Time-Lagged Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances forecasting accuracy by effectively integrating time-lagged cross-correlated sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW) algorithm to capture lagged correlations and a contrastive learning-based encoder to efficiently approximate SSDTW distances. Experimental results on weather, finance and real estate time series datasets demonstrate the effectiveness of our framework. On the weather dataset, SSDTW reduces mean squared error (MSE) by 16.01\% compared with single-sequence methods, while the contrastive learning encoder (CLE) further decreases MSE by 17.88\%. On the stock dataset, SSDTW achieves a 9.95\% MSE reduction, and CLE reduces it by 6.13\%. For the real estate dataset, SSDTW and CLE reduce MSE by 21.29\% and 8.62\%, respectively. Additionally, the contrastive learning approach decreases SSDTW computational time by approximately 99\%, ensuring scalability and real-time applicability across multiple time series forecasting tasks.","New York, NY, USA",,"Wu, Jianfei and Yang, Wenmian and Liu, Bingning and Jia, Weijia",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761410,9798400720406,,,,"contrastive learning, machine learning, time-lagged cross-correlation, time-series forecasting","Seoul, Republic of Korea",,,10,3386–3395,Association for Computing Machinery,CIKM '25,TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations,https://doi.org/10.1145/3746252.3761410,,2025
inproceedings,10.1145/3711896.3737442,"Time Series Forecasting (TSF) is key functionality in numerous fields, such as financial investment, weather services, and energy management. Although increasingly capable TSF methods occur, many of them require domain-specific data collection and model training and do not generalize well when applied in other domains. Time Series Foundation Models (TSFMs) that are pre-trained on massive heterogeneous time series data aim to overcome these limitations. The prospects for generalizability have spurred the development of a new generation of TSFMs. This study proposes a benchmark, TSFM-Bench, to facilitate comprehensive and unified evaluation of TSFMs. TSFM-Bench covers a wide range of TSFMs, including those based on large language models and those pre-trained on time series data. TSFM-Bench supports multiple forecasting scenarios, including zero-shot, few-shot, and full-shot, enabling assessment across the full range of adaptation strategies. TSFM-Bench also provides a standardized experimental protocols for critical evaluation processes such as dataset splitting, loading, normalization, and few-shot sampling, facilitating consistency and fairness. We report on an extensive evaluation of TSFMs across a diverse range of datasets spanning multiple domains and exhibiting varied statistical characteristics. Specifically, we identify pros and cons and inherent limitations of existing TSFMs, and we propose potential directions for new model designs.","New York, NY, USA",,"Li, Zhe and Qiu, Xiangfei and Chen, Peng and Wang, Yihang and Cheng, Hanyin and Shu, Yang and Hu, Jilin and Guo, Chenjuan and Zhou, Aoying and Jensen, Christian S. and Yang, Bin",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737442,9798400714542,,,,"benchmark, foundation models, time series forecasting","Toronto ON, Canada",,,12,5595–5606,Association for Computing Machinery,KDD '25,TSFM-Bench: A Comprehensive and Unified Benchmark of Foundation Models for Time Series Forecasting,https://doi.org/10.1145/3711896.3737442,,2025
inproceedings,10.1145/3746252.3761173,"Multi-variate time series (MTS) forecasting is crucial for various applications. Existing methods have shown promising results owing to their strong ability to capture intra- and inter-variate dependencies. However, these methods often overlook lead-lag dependencies at multiple grouping scales, failing to capture hierarchical lead-lag effects in complex systems. To this end, we propose MillGNN, a novel graph neural network-based method that learns multiple grouping scale lead-lag dependencies for MTS forecasting, which can comprehensively capture lead-lag effects considering variate-wise and group-wise dynamics and decays. Specifically, MillGNN introduces two key innovations: (1) a scale-specific lead-lag graph learning module that integrates cross-correlation coefficients and dynamic decaying features derived from real-time inputs and time lags to learn lead-lag dependencies for each scale, which can model evolving lead-lag dependencies with statistical interpretability and data-driven flexibility; (2) a hierarchical lead-lag message passing module that passes lead-lag messages at multiple grouping scales in a structured way to simultaneously propagate intra- and inter-scale lead-lag effects, which can capture multi-scale lead-lag effects with a balance of comprehensiveness and efficiency. Experimental results on 11 datasets demonstrate the superiority of MillGNN for long-term and short-term MTS forecasting, compared with 16 state-of-the-art methods.","New York, NY, USA",,"Wu, Binqing and Shang, Zongjiang and Huang, Jianlong and Chen, Ling",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761173,9798400720406,,,,"hypergraph modeling, multivariate time series forecasting, spatial temporal graph neural network","Seoul, Republic of Korea",,,11,3344–3354,Association for Computing Machinery,CIKM '25,MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting,https://doi.org/10.1145/3746252.3761173,,2025
inproceedings,10.1145/3627673.3679757,"Deep models for Multivariate Time Series (MTS) forecasting have recently demonstrated significant success. Channel-dependent models capture complex dependencies that channel-independent models cannot capture. However, the number of channels in real-world applications outpaces the capabilities of existing channel-dependent models, and contrary to common expectations, some models underperform the channel-independent models in handling high-dimensional data, which raises questions about the performance of channel-dependent models. To address this, our study first investigates the reasons behind the suboptimal performance of these channel-dependent models on high-dimensional MTS data. Our analysis reveals that two primary issues lie in the introduced noise from unrelated series that increases the difficulty of capturing the crucial inter-channel dependencies, and challenges in training strategies due to high-dimensional data. To address these issues, we propose STHD, the Scalable Transformer for High-Dimensional Multivariate Time Series Forecasting. STHD has three components: a) Relation Matrix Sparsity that limits the noise introduced and alleviates the memory issue; b) ReIndex applied as a training strategy to enable a more flexible batch size setting and increase the diversity of training data; and c) Transformer that handles 2-D inputs and captures channel dependencies. These components jointly enable STHD to manage the high-dimensional MTS while maintaining computational feasibility. Furthermore, experimental results show STHD's considerable improvement on three high-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source code and dataset are publicly available https://github.com/xinzzzhou/ScalableTransformer4HighDimensionMTSF.git.","New York, NY, USA",,"Zhou, Xin and Wang, Weiqing and Buntine, Wray and Qu, Shilin and Sriramulu, Abishek and Tan, Weicong and Bergmeir, Christoph",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679757,9798400704369,,,,"forecasting accuracy, high-dimensional time series, multivariate time series forecasting","Boise, ID, USA",,,12,3515–3526,Association for Computing Machinery,CIKM '24,Scalable Transformer for High Dimensional Multivariate Time Series Forecasting,https://doi.org/10.1145/3627673.3679757,,2024
inproceedings,10.1145/3768292.3770429,"Financial time series prediction has long been a topic of interest, and with the rise of machine learning, numerous models have been proposed to improve forecasting accuracy. However, existing approaches largely overlook the topological structure inherent in financial data. In this paper, we propose Topology-aware Financial Generative Adversarial Networks (TF-GAN), a topology-aware adversarial forecasting framework that incorporates topological data analysis (TDA) into the training process. Building upon the ForGAN and Fin-GAN frameworks, TF-GAN introduces a novel topology-aware loss function based on persistent homology that encourages generated sequences to preserve the structural patterns of real financial data. To the best of our knowledge, we are among the first to design and implement a differentiable topological loss function with full gradient support in PyTorch, enabling end-to-end training. Experiments on 5-minute interval stock data for Amazon and IBM show that our method significantly improves both prediction accuracy (RMSE, MAE) and financial performance (Sharpe Ratio, cumulative PnL). TF-GAN improves the Sharpe Ratio by up to 318\% compared to the original ForGAN, and achieves up to 295\% higher Sharpe Ratios compared to Fin-GAN across diverse financial loss functions. The proposed TF-GAN framework demonstrates that incorporating topological structure into the adversarial learning loop yields more robust and profitable predictions, offering a new direction for structure-aware financial forecasting.","New York, NY, USA",,"Karbasian, Mohammadyasin and Ahangarzadeh, Amir and Manshaei, Mohammad Hossein and Zahabi, Sayed Jalal",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3770429,9798400722202,,,,"Generative Adversarial Networks (GANs), Topological Data Analysis (TDA), Financial Time Series Forecasting, Persistent Homology, High-Frequency Trading, Differentiable Topological Loss Function","
",,,9,229–237,Association for Computing Machinery,ICAIF '25,TF-GAN: Topology-Aware Generative Adversarial Network for Financial Time Series Forecasting,https://doi.org/10.1145/3768292.3770429,,2025
inproceedings,10.1145/3746252.3761515,"In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over 100,000 promoters, achieving a 9.29\% improvement in GMV and a 5.89\% increase in sales volume.","New York, NY, USA",,"Wang, Zhe and Yang, Yaming and Guan, Ziyu and Tong, Bin and Wang, Rui and Zhao, Wei and Deng, Hongbo",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761515,9798400720406,,,,"affiliate marketing, dynamic network, time series","Seoul, Republic of Korea",,,8,6177–6184,Association for Computing Machinery,CIKM '25,Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing,https://doi.org/10.1145/3746252.3761515,,2025
inproceedings,10.1145/3746252.3761143,"Multivariate Time Series Forecasting (MTSF) plays a critical role in diverse practical applications. Although Transformer-based models have recently achieved impressive results in this field, their performance is still hindered by three core challenges: complex temporal dependencies, diverse inter-variable correlations, and patterns that span multiple time scales. To address these issues, we propose MSOFormer-a Multi-scale Transformer with Orthogonal Embedding and Frequency Modeling. Specifically, the Dynamic Frequency Filter adaptively weights frequency components across variables based on input characteristics, enabling full-spectrum modeling and precise extraction of key frequency patterns. To improve inter-variable representation, we introduce Orthogonal Embedding, a novel projection strategy for queries and keys that enhances feature diversity in channel-wise self-attention. In addition, Multi-scale Patch Embedding captures temporal features across different scales, providing a comprehensive time series representation. To evaluate MTSF in cloud-native environments, we construct the first three Cloud Kafka cluster datasets, specifically curated for elastic message queue scaling scenarios. Extensive experiments across eleven real-world benchmark datasets demonstrate that MSOFormer consistently outperforms existing state-of-the-art methods, highlighting its effectiveness and broad applicability.","New York, NY, USA",,"Shi, Qin and Xu, Chu and Hu, Zongtang and Shen, Dong and Sun, Dapeng and Quan, Lijun",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761143,9798400720406,,,,"cloud kafka cluster datasets, frequency domain learning, multi-scale modeling, multivariate time series forecasting, orthogonal embedding","Seoul, Republic of Korea",,,11,2674–2684,Association for Computing Machinery,CIKM '25,MSOFormer: Multi-scale Transformer with Orthogonal Embedding and Frequency Modeling for Multivariate Time Series Forecasting,https://doi.org/10.1145/3746252.3761143,,2025
article,10.1145/3735651,"Long-term time-series forecasting (LTSF) is fundamental to various real-world applications, where Transformer-based models have become the dominant framework due to their ability to capture long-range dependencies. However, these models often experience overfitting due to data redundancy in rolling forecasting settings, limiting their generalization ability particularly evident in longer sequences with highly similar adjacent data. In this work, we introduce CLMFormer, a novel framework that mitigates redundancy through curriculum learning and a memory-driven decoder. Specifically, we progressively introduce Bernoulli noise to the training samples, which effectively breaks the high similarity between adjacent data points. This curriculum-driven noise introduction aids the memory-driven decoder by supplying more diverse and representative training data, enhancing the decoder’s ability to model seasonal tendencies and dependencies in the time-series data. To further enhance forecasting accuracy, we introduce a memory-driven decoder. This component enables the model to capture seasonal tendencies and dependencies in the time-series data and leverages temporal relationships to facilitate the forecasting process. Extensive experiments on six real-world LTSF benchmarks show that CLMFormer consistently improves Transformer-based models by up to 30\%, demonstrating its effectiveness in long-horizon forecasting.","New York, NY, USA",,"Li, Mingjie and Liu, Rui and Shi, Guangsi and Han, Mingfei and Li, Changlin and Yao, Lina and Chang, Xiaojun and Chen, Ling",,10.1145/3735651,,2157-6904,,ACM Trans. Intell. Syst. Technol.,"Long-term time series forecasting, Transformer, Curriculum learning, Memory module",,Just Accepted,,,,Association for Computing Machinery,,Mitigating Data Redundancy to Revitalize Transformer-based Long-Term Time Series Forecasting System,https://doi.org/10.1145/3735651,,2025
article,10.1145/3653447,"Multivariate time series (MTS) forecasting is crucial in many real-world applications. To achieve accurate MTS forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this article, we reexamine intra- and inter-series relationships from the perspective of mutual information and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and inter-series relationships among time series data concurrently, a coupled variable representation module aimed at encoding diverse variable patterns, and an inference module facilitating predictions through one forward step. Extensive experiments conducted on seven real-world datasets demonstrate that our proposed DeepCN achieves superior performance compared with the state-of-the-art baselines.","New York, NY, USA",127,"Yi, Kun and Zhang, Qi and He, Hui and Shi, Kaize and Hu, Liang and An, Ning and Niu, Zhendong",,10.1145/3653447,,1046-8188,September 2024,ACM Trans. Inf. Syst.,"Multivariate time series forecasting, deep coupling network, mutual information",,,5,28,,Association for Computing Machinery,,Deep Coupling Network for Multivariate Time Series Forecasting,https://doi.org/10.1145/3653447,42,2024
inproceedings,10.1145/3731715.3733272,"A growing body of recent researches have migrated graph structure learning (GSL) to the multivariate time series forecasting (MTSF), which lays the foundation for the promotion of ''Generalized Graph'' for multimedia MTSF applications. In other words, we expect generalized graph to encompass the learning of inter-variable, inter-temporal and latent correlations, becoming a universal tool for multivariate correlations learning. However, due to the heterogeneity of multivariate time series in distribution, graph learning inevitably captures inaccurate relationships, which requires the quality of graph learning; Meanwhile MTSF often requires instant predictions for decision-making in real-world, which also challenges the speed of GSL. To solve these challenges, we propose AGGA-MVFLN, namely Adaptive Generalized Graph Accompanied Multi-View Frequency Learning Network. Specifically, we introduce an adaptive generalized graph structure from multi-view (global and local) to capture diverse ''spatio-temporal patterns''. Subsequently, we utilize the Fast Fourier Transform to map them into the frequency domain, and enhance the quality of the generalized graph by collaboratively learning the complementarities and differences through reconstructed ''spatio-temporal patterns'' and error-driven supervised training of adaptive graph. The benefits are: (1) The frequency domain can disentangle complex temporal patterns, making the process of learning multivariate relationships more robust. (2) Multi-view learning can significantly reduce training time by preset and seamless integration (i.e., the multi-task loss form). (3) ''Generalized Graph'' can be regarded as universal component for multivariate correlation learning. Evaluation of 9 real-world datasets confirms the superiority of AGGA-MVFLN over SOTA benchmark.","New York, NY, USA",,"Lei, Jierui and Chen, Fangzheng and Tang, Haina",Proceedings of the 2025 International Conference on Multimedia Retrieval,10.1145/3731715.3733272,9798400718779,,,,"adaptive graph learning, frequency domain analysis, multi-view learning, multivariate time series forecasting","Chicago, IL, USA",,,9,653–661,Association for Computing Machinery,ICMR '25,AGGA-MVFLN: Multivariate Time Series Forecasting via Adaptive Generalized Graph Accompanied with Multi-View Learning in Frequency Domain,https://doi.org/10.1145/3731715.3733272,,2025
article,10.14778/3705829.3705842,"Accurate long-term forecasting from multivariate time series has important real-world applications. However, achieving this so is challenging. Thus, analyses reveal that time series that span long durations often exhibit dynamic and disrupted correlations. State-of-the-art methods employ attention mechanisms to capture dynamic correlations, but they often do not contend well with disrupted correlations, which reduces prediction accuracy. We introduce local and global information concepts and then leverage these in a Memory Guided Transformer, called the Memformer. By integrating patch-wise recurrent graph learning and global attention, the Memformer aims to capture dynamic correlations and take disrupted correlations into account. We also integrate a so-called Alternating Memory Enhancer into the Memformer to capture correlations between local and global information. We report on experiments that offer insight into the effectiveness of the Memformer at capturing dynamic correlations and its robustness to disrupted correlations. The experiments offer evidence that the new method is capable of advancing the state-of-the-art in forecasting accuracy on real-world datasets.",,,"Cheng, Yunyao and Guo, Chenjuan and Yang, Bin and Yu, Haomin and Zhao, Kai and Jensen, Christian S.",,10.14778/3705829.3705842,,2150-8097,October 2024,Proc. VLDB Endow.,,,,2,14,239–252,VLDB Endowment,,A Memory Guided Transformer for Time Series Forecasting,https://doi.org/10.14778/3705829.3705842,18,2024
inproceedings,10.1145/3711896.3737157,"Recent deep learning models for Long-term Time Series Forecasting (LTSF) often emphasize complex, handcrafted designs, while simpler architectures like linear models or MLPs have often outperformed these intricate solutions. In this paper, we revisit and organize the core ideas behind several key techniques, such as redundancy reduction and multi-scale modeling, which are frequently employed in advanced LTSF models. Our goal is to streamline these ideas for more efficient deep learning utilization. To this end, we introduce TimeCapsule, a model built around the principle of high-dimensional information compression that unifies these techniques in a generalized yet simplified framework. Specifically, we model time series as a 3D tensor, incorporating temporal, variate, and level dimensions, and leverage mode production to capture multi-mode dependencies while achieving dimensionality compression. We propose an internal forecast within the compressed representation domain, supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the learning of predictive representations. Extensive experiments on challenging benchmarks demonstrate the versatility of our method, showing that TimeCapsule can achieve state-of-the-art performance. The code is available at: https://github.com/Luoauoa/TimeCapsule.git.","New York, NY, USA",,"Lu, Yihang and Xu, Yangyang and Qin, Qitao and Meng, Xianwei",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737157,9798400714542,,,,"deep learning, information tensor modeling, multivariate long-term time series forecasting","Toronto ON, Canada",,,12,1987–1998,Association for Computing Machinery,KDD '25,TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations,https://doi.org/10.1145/3711896.3737157,,2025
inproceedings,10.1145/3711896.3736854,"Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. Additionally, we introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. To evaluate the effectiveness of STEV, we benchmark EVTSF performance on three real-world datasets from various domains and compare it against three potential solutions employing state-of-the-art (SOTA) MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, especially in handling expanding variables. Notably, STEV, with only 5\% of observations during the expanding period, is on par with SOTA MTSF models trained with complete data. Further exploration of various expanding scenarios underscores the generalizability of STEV in real-world applications.","New York, NY, USA",,"Ma, Minbo and Tang, Kai and Li, Huan and Teng, Fei and Zhang, Dalin and Li, Tianrui",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736854,9798400714542,,,,"contrastive learning, expanding-variate time series forecasting, spatio-temporal graph neural networks","Toronto ON, Canada",,,12,2054–2065,Association for Computing Machinery,KDD '25,Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning,https://doi.org/10.1145/3711896.3736854,,2025
inproceedings,10.1145/3690624.3709254,"Forecasting complex time series is an important yet challenging problem that involves various industrial applications. Recently, masked time-series modeling has been proposed to effectively model temporal dependencies for forecasting by reconstructing masked segments from unmasked ones. However, since the semantic information in time series is involved in intricate temporal variations generated by multiple time series components, simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach. Specifically, we propose ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components to produce multiple masked seasonal series based on inherent multi-periodicity and a sub-series masking strategy for trend components to mask temporal regions that share similar variations. The proposed masking method presents an effective pre-training task for learning intricate temporal variations and dependencies. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations. Experimental results show that our proposed ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.","New York, NY, USA",,"Seo, Hyunwoo and Lim, Chiehyeon",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709254,9798400712456,,,,"masked modeling, seasonal-trend decomposition, self-supervised learning, time series forecasting","Toronto ON, Canada",,,12,1209–1220,Association for Computing Machinery,KDD '25,ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting,https://doi.org/10.1145/3690624.3709254,,2025
inproceedings,10.1145/3589334.3645434,"Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability.","New York, NY, USA",,"Liu, Xu and Hu, Junfeng and Li, Yuan and Diao, Shizhe and Liang, Yuxuan and Hooi, Bryan and Zimmermann, Roger",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645434,9798400701719,,,,"language models, time series forecasting","Singapore, Singapore",,,12,4095–4106,Association for Computing Machinery,WWW '24,UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting,https://doi.org/10.1145/3589334.3645434,,2024
inproceedings,10.1145/3746252.3761281,"In multivariate time series (MTS) forecasting, many deep learning based methods have been proposed for modeling dependencies at multiple spatial (inter-variate) or temporal (intra-variate) scales. However, existing methods may fail to model dependencies across multiple spatial-temporal scales (ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In this work, we propose ST-Hyper to model the high-order dependencies across multiple ST-scales through adaptive hypergraph modeling. Specifically, we introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph Modeling (AHM) module that learns a sparse hypergraph to capture robust high-order dependencies among features. In addition, we interact with these features through tri-phase hypergraph propagation, which can comprehensively capture multi-scale spatial-temporal dynamics. Experimental results on six real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art performance, outperforming the best baselines with an average MAE reduction of 3.8\% and 6.8\% for long-term and short-term forecasting, respectively. Code is available at https://anonymous.4open.science/ST-Hyper-83E7.","New York, NY, USA",,"Wu, Binqing and Huang, Jianlong and Shang, Zongjiang and Chen, Ling",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761281,9798400720406,,,,"hypergraph modeling, multivariate time series forecasting, spatial temporal graph neural network","Seoul, Republic of Korea",,,11,3333–3343,Association for Computing Machinery,CIKM '25,ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting,https://doi.org/10.1145/3746252.3761281,,2025
inproceedings,10.1145/3746252.3761199,"Time-series forecasting is critical to highly data-dependent domains such as energy, healthcare, and transportation. Although Large Language Models have recently been explored for this task, their performance is hindered by a modality gap: numerical sequences poorly align with text-based inputs, and direct alignment often introduces noise. In contrast, human experts rarely predict directly from numbers; they first inspect line charts to recognize overall patterns and then apply simple models for forecasting. Inspired by this workflow, we propose VisMoE, a Vision-Language-Model-driven Mixture-of-Experts framework. In VisMoE, Each sequence is transformed into a line-chart image, enabling a VLM to classify it into distinct temporal regimes. Based on this classification, VisMoE routes the sequence to lightweight specialized experts operating alongside a global predictor, whose outputs are fused for final forecasts. This human-inspired design preserves semantic understanding, reduces modality misalignment, and improves computational efficiency. Extensive experiments across multiple benchmarks demonstrate that VisMoE achieves state-of-the-art forecasting accuracy while remaining highly efficient. Our code is available at https://github.com/Liu905169/VisMoE.","New York, NY, USA",,"Liu, Xingyu and Gao, Min and Wang, Zongwei and Bai, Yinbing",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761199,9798400720406,,,,"mixture-of-experts, time series prediction, vision-language models","Seoul, Republic of Korea",,,11,1914–1924,Association for Computing Machinery,CIKM '25,Seeing Sequences like Humans: Pattern Classification Driven Time-Series Forecasting via Vision Language Models,https://doi.org/10.1145/3746252.3761199,,2025
article,10.1145/3599728,"Time series forecasting in the sensing system aims to predict future values based on historical records that sensors have collected. Previous works, however, usually focus on improving model structure or algorithm for better performance but the perspective of learning proper numeric representations is overlooked. The inappropriate and coarse numeric representations are not expressive enough to capture the intrinsic characteristics of numbers, which will obviously degrade the prediction performance. In this article, we propose Num2vec, an algorithmic framework to learn numeric representations. Specifically, Num2vec lists three main logic characteristics of numbers: arithmetic, direction, and periodicity. By representing numbers into a transition space, Num2vec can translates numbers agilely to different Internet of Things tasks through selecting the corresponding characteristics. According to such a design, Num2vec enjoys flexible numeric representations to fit different Internet of Things time series tasks. Extensive experiments on four real-world datasets show that the approach achieves the best performance when compared with state-of-the-art baselines.","New York, NY, USA",94,"Fan, Jinxiao and Wang, Pengfei and Fan, Yu and Liu, Liang and Ma, Huadong",,10.1145/3599728,,1550-4859,November 2023,ACM Trans. Sen. Netw.,"Time series forecasting, representation learning, IoT, sensing system",,,4,23,,Association for Computing Machinery,,Num2vec: Pre-Training Numeric Representations for Time Series Forecasting in the Sensing System,https://doi.org/10.1145/3599728,19,2023
inproceedings,10.1145/3637528.3671961,"Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a &lt;u&gt;H&lt;/u&gt;eterogeneity-&lt;u&gt;I&lt;/u&gt;nformed Spatiotemporal &lt;u&gt;M&lt;/u&gt;eta-&lt;u&gt;Net&lt;/u&gt;work (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at &lt;u&gt;https://github.com/XDZhelheim/HimNet&lt;/u&gt;.","New York, NY, USA",,"Dong, Zheng and Jiang, Renhe and Gao, Haotian and Liu, Hangchen and Deng, Jinliang and Wen, Qingsong and Song, Xuan",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671961,9798400704901,,,,"heterogeneity, meta-parameter learning, spatiotemporal time series","Barcelona, Spain",,,11,631–641,Association for Computing Machinery,KDD '24,Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting,https://doi.org/10.1145/3637528.3671961,,2024
article,10.14778/3636218.3636230,"Multiple time series forecasting plays an essential role in many applications. Solutions based on graph neural network (GNN) that deliver state-of-the-art forecasting performance use the relation graph which can capture historical correlations among time series. However, in real world, it is common that correlations among time series evolve across time, resulting in dynamic relation graph, where the future correlations may be different from those in history. To address this problem, we propose multiple time series forecasting with dynamic graph modeling (MTSF-DG) that is able to learn historical relation graphs and predicting future relation graphs to capture the dynamic correlations. We also propose a causal GNN to extract features from both kinds of relation graphs efficiently. Then we propose a reasoning network to explicitly learn the variant influence from historical timestamps to future timestamps for final forecasting. Extensive experiments on six benchmark datasets show that MTSF-DG consistently outperforms state-of-the-art baselines, and justify our design with dynamic relation graph modeling.",,,"Zhao, Kai and Guo, Chenjuan and Cheng, Yunyao and Han, Peng and Zhang, Miao and Yang, Bin",,10.14778/3636218.3636230,,2150-8097,December 2023,Proc. VLDB Endow.,,,,4,13,753–765,VLDB Endowment,,Multiple Time Series Forecasting with Dynamic Graph Modeling,https://doi.org/10.14778/3636218.3636230,17,2023
article,10.14778/3636218.3636231,"Robust multivariate time series forecasting is crucial in many cyberphysical and Internet of Things applications. Existing state-of-the-art robust forecasting models decompose time series into independent functions covering trends and periodicities. However, these independent functions fail to capture correlations among multiple time series, thereby reducing prediction accuracy. Moreover, existing robust forecasting models treat certain abrupt but normal changes, e.g., caused by holidays, as outliers because they occur infrequently and have data distributions that resemble those of outliers. This exacerbates model bias and reduces prediction accuracy. This paper aims to capture correlations across multiple time series and abrupt but normal changes, thereby improving prediction accuracy. We employ weak labels to partition the dataset into source and target domains. Then, we propose the Domain Adversarial Robust Forecaster (DARF). This forecasting model is based on adversarial domain adaptation and includes two novel modules: Correlated Robust Forecaster (CORF) and Domain Critic. Specifically, CORF constitutes an encoder-decoder framework proficient at robust multivariate time series forecasting, and Domain Critic works to reduce data bias. Extensive experiments and discussions show that DARF is capable of state-of-the-art forecasting accuracy.",,,"Cheng, Yunyao and Chen, Peng and Guo, Chenjuan and Zhao, Kai and Wen, Qingsong and Yang, Bin and Jensen, Christian S.",,10.14778/3636218.3636231,,2150-8097,December 2023,Proc. VLDB Endow.,,,,4,14,766–779,VLDB Endowment,,Weakly Guided Adaptation for Robust Time Series Forecasting,https://doi.org/10.14778/3636218.3636231,17,2023
article,10.1145/3543511,"Effective time-series forecasting methods are of significant importance to solve a broad spectrum of research problems. Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, a relational global model learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, a relational local model not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5\% on average. Furthermore, we target addressing the common nature of many time-series forecasting applications where time-series are provided in a streaming version; however, most methods fail to leverage the newly incoming time-series values and result in worse performance over time. In this article, we propose an online incremental learning framework for probabilistic forecasting. The framework is theoretically proven to have lower time and space complexity. The framework can be universally applied to many other machine learning-based methods.","New York, NY, USA",26,"Chen, Hongjie and Rossi, Ryan A. and Mahadik, Kanak and Kim, Sungchul and Eldardiry, Hoda",,10.1145/3543511,,1556-4681,February 2023,ACM Trans. Knowl. Discov. Data,"Incremental online learning, Graph Neural Network, time-series forecasting",,,2,30,,Association for Computing Machinery,,Graph Deep Factors for Probabilistic Time-series Forecasting,https://doi.org/10.1145/3543511,17,2023
inproceedings,10.1145/3448016.3457564,"To capture higher-order structural features, most GNN-based algorithms learn node representations incorporating k-hop neighbors' information. Due to the high time complexity of querying k-hop neighbors, most graph algorithms cannot be deployed in a giant dense temporal network to execute millisecond-level inference. This problem dramatically limits the potential of applying graph algorithms in certain areas, especially financial fraud detection. Therefore, we propose Asynchronous Propagation Attention Network, an asynchronous continuous time dynamic graph algorithm for real-time temporal graph embedding. Traditional graph models usually execute two serial operations: first graph querying and then model inference. Different from previous graph algorithms, we decouple model inference and graph computation to alleviate the damage of the heavy graph query operation to the speed of model inference. Extensive experiments demonstrate that the proposed method can achieve competitive performance while greatly improving the inference speed. The source code is published at a Github repository.","New York, NY, USA",,"Wang, Xuhong and Lyu, Ding and Li, Mengjian and Xia, Yang and Yang, Qi and Wang, Xinwen and Wang, Xinguang and Cui, Ping and Yang, Yupu and Sun, Bowen and Guo, Zhenyu",Proceedings of the 2021 International Conference on Management of Data,10.1145/3448016.3457564,9781450383431,,,,"dynamic graph, graph neural networks, network embedding","Virtual Event, China",,,11,2628–2638,Association for Computing Machinery,SIGMOD '21,APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding,https://doi.org/10.1145/3448016.3457564,,2021
inproceedings,10.1145/3698587.3701364,"Motivation: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on healthcare time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale time series and ability to capture long-term temporal dependencies.Methods: In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies.Materials: We evaluated TimelyGPT on two large-scale healthcare time series datasets corresponding to continuous biosignals and irregularly-sampled time series, respectively: (1) the Sleep EDF dataset consisting of over 1.2 billion timesteps; (2) the longitudinal healthcare administrative database PopHR, comprising 489,000 patients randomly sampled from the Montreal population.Results: In forecasting continuous biosignals, TimelyGPT achieves accurate extrapolation up to 6,000 timesteps of body temperature during the sleep stage transition, given a short look-up window (i.e., prompt) containing only 2,000 timesteps. For irregularly-sampled time series, TimelyGPT with a proposed time-specific inference demonstrates high top recall scores in predicting future diagnoses using early diagnostic records, effectively handling irregular intervals between clinical records. Together, we envision TimelyGPT to be useful in various health domains, including long-term patient health state forecasting and patient risk trajectory prediction. Availability: The open-sourced code is available at Github.","New York, NY, USA",16,"Song, Ziyang and Lu, Qincheng and Xu, Hao and Zhu, He and Buckeridge, David and Li, Yue","Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics",10.1145/3698587.3701364,9798400713026,,,,"Time-series forecasting, Time-series pre-training, biosignals, clinical diagnosis, irregularly-sampled time series, transfer learning","Shenzhen, China",,,10,,Association for Computing Machinery,BCB '24,TimelyGPT: Extrapolatable Transformer Pre-training for Long-term Time-Series Forecasting in Healthcare,https://doi.org/10.1145/3698587.3701364,,2024
inproceedings,10.1145/3637528.3672055,"Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90\% of variables are missing, it can still accurately predict the future values of all variables.","New York, NY, USA",,"Yu, Chengqing and Wang, Fei and Shao, Zezhi and Qian, Tangwen and Zhang, Zhao and Wei, Wei and Xu, Yongjun",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3672055,9798400704901,,,,"adaptive graph convolution, graph interpolation attention recursive network, interpolation attention, multivariate time series forecasting, variable missing","Barcelona, Spain",,,12,3989–4000,Association for Computing Machinery,KDD '24,GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing,https://doi.org/10.1145/3637528.3672055,,2024
inproceedings,10.1145/3704558.3707079,"Tobacco pest is one of the main factors that harm tobacco quality in tobacco factories, leading to economic losses. Accurate prediction of pest distribution is crucial for the management and production of tobacco factories. Nevertheless, conventional time series forecasting techniques prove inadequate in capturing sufficient information for fine-grained forecasting tasks involving district-variable-level and day-sampling-level tobacco pest datasets. This limitation arises from the lack of consideration for the knowledge pertaining to the migration and reproduction patterns of tobacco pests. In this work, a dual track self-supervised and distribution graph guided network (DTSSDGN) is proposed to handle this problem. First, a distribution graph guided feature extraction module is designed to help capture the internal and external patterns of pest distribution. Subsequently, a two-stage training strategy is devised, comprising a dual-track self-supervised training phase to acquire features that possess a comprehensive understanding of the pest distribution trend, followed by a fine-grained fine-tuning phase that leverages this knowledge to achieve accurate forecasting outcomes. The effectiveness and superiority of the proposed method are illustrated on a real tobacco pest distribution dataset.","New York, NY, USA",,"Zhu, Liming and Kong, Xu and Li, Ming and Qin, Ting and Chen, Yangjun and Chang, Junyu and Chen, Xu",Proceedings of the 2024 2nd International Conference on Frontiers of Intelligent Manufacturing and Automation,10.1145/3704558.3707079,9798400710681,,,,"Distribution graph, deep learning, dual track self-supervised, graph neural network, time series forecasting","
",,,5,406–410,Association for Computing Machinery,CFIMA '24,A Distribution Graph Guided Network with Dual Track Self-Supervised Strategy for Tobacco Pest Time Series Forecasting,https://doi.org/10.1145/3704558.3707079,,2025
article,10.1145/3589270,"Correlated time series (CTS) forecasting plays an essential role in many practical applications, such as traffic management and server load control. Many deep learning models have been proposed to improve the accuracy of CTS forecasting. However, while models have become increasingly complex and computationally intensive, they struggle to improve accuracy. Pursuing a different direction, this study aims instead to enable much more efficient, lightweight models that preserve accuracy while being able to be deployed on resource-constrained devices. To achieve this goal, we characterize popular CTS forecasting models and yield two observations that indicate directions for lightweight CTS forecasting. On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking that is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operator modules, called L-TCN and GL-Former, that offer improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations. Experiments with single-step and multi-step forecasting benchmark datasets show that LightCTS is capable of nearly state-of-the-art accuracy at much reduced computational and storage overheads.","New York, NY, USA",125,"Lai, Zhichen and Zhang, Dalin and Li, Huan and Jensen, Christian S. and Lu, Hua and Zhao, Yan",,10.1145/3589270,,,June 2023,Proc. ACM Manag. Data,"correlated time series forecasting, lightweight neural networks",,,2,26,,Association for Computing Machinery,,LightCTS: A Lightweight Framework for Correlated Time Series Forecasting,https://doi.org/10.1145/3589270,1,2023
inbook,10.1145/3757749.3757774,"This paper proposes a stock price prediction model based on a combination method of recurrent neural networks (RNN) and attention mechanisms. The goal is to improve the forecasting accuracy of financial time series data. By applying the attention mechanism to the RNN architecture, the model can learn dynamic importance weights for different time steps. This allows better capture of dominant features in stock price volatility. The study utilizes real Apple Inc.'s stock prices between January 2024 and January 2025 to build models and validate the models. Normalizing and implementing train-test strategies were used to preprocess data in a bid to enhance stability within the models. Experiments utilize mean squared error (MSE), mean absolute error (MAE), and the coefficient of determination (R2) as evaluation metrics for the model. Comparative evaluation was performed against a set of mainstream popular models. Results show that the constructed RNN+Attention model outperforms traditional RNN, LSTM, MLP, and Transformer models both in prediction accuracy and fitting performance. In addition, visualizations demonstrate a strong correspondence between predicted values and actual values. The curve of training loss shows a clear downward trend, further supporting the effectiveness and convergence of the model. This method provides an efficient and realistic technical solution to stock price prediction using deep learning.","New York, NY, USA",,"Xu, Zhen and Liu, Xiqing and Xu, Qingqing and Su, Xin and Guo, Xiaojun and Wang, Yixian",Proceedings of the 2025 2nd International Conference on Computer and Multimedia Technology,,9798400713347,,,,,,,,5,155–159,Association for Computing Machinery,,Time Series Forecasting with Attention-Augmented Recurrent Networks: A Financial Market Application,https://doi.org/10.1145/3757749.3757774,,2025
inproceedings,10.1145/3534678.3539394,"We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast (VSF), where only a small subset of the variables is available during inference. Variables are absent during inference because of long-term data loss (eg. sensor failures) or high -&gt; low-resource domain shift between train / test. To the best of our knowledge, robustness of MTSF models in presence of such failures, has not been studied in the literature. Through extensive evaluation, we first show that the performance of state of the art methods degrade significantly in the VSF setting. We propose a non-parametric, wrapper technique that can be applied on top any existing forecast models. Through systematic experiments across 4 datasets and 5 forecast models, we show that our technique is able to recover close to 95\% performance of the models even when only 15\% of the original variables are present.","New York, NY, USA",,"Chauhan, Jatin and Raghuveer, Aravindan and Saket, Rishi and Nandy, Jay and Ravindran, Balaraman",Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3534678.3539394,9781450393850,,,,"multivariate time series forecasting, partial inference, retrieval model, variable subsets","Washington DC, USA",,,11,76–86,Association for Computing Machinery,KDD '22,Multi-Variate Time Series Forecasting on Variable Subsets,https://doi.org/10.1145/3534678.3539394,,2022
article,10.14778/3665844.3665863,"Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains : traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The results offer a deeper understanding of the forecasting methods, allowing us to better select the ones that are most suitable for particular datasets and settings. Overall, TFB and this evaluation provide researchers with improved means of designing new TSF methods.",,,"Qiu, Xiangfei and Hu, Jilin and Zhou, Lekui and Wu, Xingjian and Du, Junyang and Zhang, Buang and Guo, Chenjuan and Zhou, Aoying and Jensen, Christian S. and Sheng, Zhenli and Yang, Bin",,10.14778/3665844.3665863,,2150-8097,May 2024,Proc. VLDB Endow.,,,,9,15,2363–2377,VLDB Endowment,,TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods,https://doi.org/10.14778/3665844.3665863,17,2024
inproceedings,10.1145/3604237.3626868,"Data-driven approaches using deep neural networks have been successful in modeling complex financial time series and generating accurate predictions without requiring extensive domain knowledge. However, most of the existing models that assume independent and identically distributed (i.i.d.) data may not generalize well to novel situations or distributional shifts across or inside financial scenarios. To address this challenge, we introduce an invariant learning-based regularizer with relaxed bounds that expands the range of feasible solutions and mitigates over-convergence issues in Invariant Risk Minimization (IRM). In practice, the regularizer can be incorporated into both linear and nonlinear financial time series forecasting models. Experimental results on real-world large-scale financial datasets show that our proposed method enables more robust and adaptable financial forecasting models, enhancing the overall performance and generalizability of financial forecasting on both in-distribution and out-of-distribution (OOD) samples.","New York, NY, USA",,"Cao, Defu and Zheng, Yixiang and Hassanzadeh, Parisa and Lamba, Simran and Liu, Xiaomo and Liu, Yan",Proceedings of the Fourth ACM International Conference on AI in Finance,10.1145/3604237.3626868,9798400702402,,,,"Distributional shifts, Financial time series, Forecasting algorithm","Brooklyn, NY, USA",,,9,472–480,Association for Computing Machinery,ICAIF '23,Large Scale Financial Time Series Forecasting with Multi-faceted Model,https://doi.org/10.1145/3604237.3626868,,2023
inproceedings,10.1145/3637528.3671881,"Spatial-temporal forecasting systems play a crucial role in addressing numerous real-world challenges. In this paper, we investigate the potential of addressing spatial-temporal forecasting problems using general time series forecasting models, i.e., models that do not leverage the spatial relationships among the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series forecasting architecture called RPMixer. The all-MLP architecture was chosen due to its recent success in time series forecasting benchmarks. Furthermore, our method capitalizes on the ensemble-like behavior of deep neural networks, where each individual block within the network behaves like a base learner in an ensemble model, particularly when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby improving the overall performance of the network. Extensive experiments conducted on the largest spatial-temporal forecasting benchmark datasets demonstrate that the proposed method outperforms 14 alternative methods.","New York, NY, USA",,"Yeh, Chin-Chia Michael and Fan, Yujie and Dai, Xin and Saini, Uday Singh and Lai, Vivian and Aboagye, Prince Osei and Wang, Junpeng and Chen, Huiyuan and Zheng, Yan and Zhuang, Zhongfang and Wang, Liang and Zhang, Wei",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671881,9798400704901,,,,"forecasting, large spatial-temporal graph, time series","Barcelona, Spain",,,12,3919–3930,Association for Computing Machinery,KDD '24,RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data,https://doi.org/10.1145/3637528.3671881,,2024
inproceedings,10.1145/3637528.3671665,"Accurate traffic forecasting is crucial for the development of Intelligent Transportation Systems (ITS), playing a pivotal role in modern urban traffic management. Traditional forecasting methods, however, struggle with the irregular traffic time series resulting from adaptive traffic signal controls, presenting challenges in asynchronous spatial dependency, irregular temporal dependency, and predicting variable-length sequences. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored for irregular traffic time series forecasting. Specifically, we first propose an Asynchronous Graph Diffusion Network to capture the spatial dependency between asynchronously measured traffic states regulated by adaptive traffic signals. After that, to capture the temporal dependency within irregular traffic state sequences, a personalized time encoding is devised to embed the continuous time signals. Then, we propose a Transformable Time-aware Convolution Network, which adapts meta-filters for time-aware convolution on the sequences with inconsistent temporal flow. Additionally, a Semi-Autoregressive Prediction Network, comprising a state evolution unit and a semiautoregressive predictor, is designed to predict variable-length traffic sequences effectively and efficiently. Extensive experiments on a newly established benchmark demonstrate the superiority of ASeer compared with twelve competitive baselines across six metrics.","New York, NY, USA",,"Zhang, Weijia and Zhang, Le and Han, Jindong and Liu, Hao and Fu, Yanjie and Zhou, Jingbo and Mei, Yu and Xiong, Hui",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671665,9798400704901,,,,"convolutional networks, irregular time series analysis, spatio-temporal modeling, traffic forecasting","Barcelona, Spain",,,12,4302–4313,Association for Computing Machinery,KDD '24,Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Networks,https://doi.org/10.1145/3637528.3671665,,2024
inproceedings,10.1145/3580305.3599533,"Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their memory and compute-intensive requirements pose a critical bottleneck for long-term forecasting, despite numerous advancements in compute-aware self-attention modules. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60\%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2\%) with a significant reduction in memory and runtime (2-3X).","New York, NY, USA",,"Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam and Sinthong, Phanwadee and Kalagnanam, Jayant",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599533,9798400701030,,,,"forecasting, mlp-mixer, time series, transformer","Long Beach, CA, USA",,,11,459–469,Association for Computing Machinery,KDD '23,TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting,https://doi.org/10.1145/3580305.3599533,,2023
inproceedings,10.1145/3511808.3557282,"Time Series Forecasting (TSF) has been a topic of extensive research, which has many real world applications such as weather prediction, stock market value prediction, traffic control etc. Many machine learning models have been developed to address TSF, yet, predicting extreme values remains a challenge to be effectively addressed. Extreme events occur rarely, but tend to cause a huge impact, which makes extreme event prediction important. Assuming light tailed distributions, such as Gaussian distribution, on time series data does not do justice to the modeling of extreme points. To tackle this issue, we develop a novel approach towards improving attention to extreme event prediction. Within our work, we model time series data distribution, as a mixture of Gaussian distribution and Generalized Pareto distribution (GPD). In particular, we develop a novel Deep eXtreme Mixture Model (DXtreMM) for univariate time series forecasting, which addresses extreme events in time series. The model consists of two modules: 1) Variational Disentangled Auto-encoder (VD-AE) based classifier and 2) Multi Layer Perceptron (MLP) based forecaster units combined with Generalized Pareto Distribution (GPD) estimators for lower and upper extreme values separately. VD-AE Classifier model predicts the possibility of occurrence of an extreme event given a time segment, and forecaster module predicts the exact value. Through extensive set of experiments on real-world datasets we have shown that our model performs well for extreme events and is comparable with the existing baseline methods for normal time step forecasting.","New York, NY, USA",,"S, Abilasha and Bhadra, Sahely and Dadarkar, Ahmed Zaheer and P, Deepak",Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management,10.1145/3511808.3557282,9781450392365,,,,"extreme events, generalized pareto distribution, mixture models","Atlanta, GA, USA",,,10,1726–1735,Association for Computing Machinery,CIKM '22,Deep Extreme Mixture Model for Time Series Forecasting,https://doi.org/10.1145/3511808.3557282,,2022
article,10.1145/3675165,"How to capture dynamic spatial-temporal dependencies remains an open question in multivariate time series (MTS) forecasting. Although recent advanced spatial-temporal graph neural networks (STGNNs) achieve superior forecasting performance, they either consider pre-defined spatial correlations or simply learn static graphs. Some research has tried to learn many adjacent matrices to reveal time-varying spatial correlations, but they generate discrete graphs which cannot encode evolutionary information and also face computational complexity problem. In this article, we propose two significant plugins to help automatically learn enhanced dynamic spatial-temporal embedding of MTS data: (1) a novel neural conditional random field (CRF) layer. We find that the implicit time-varying spatial dependencies are reflected by the explicit changeable links between edges, and we propose the neural CRF to encode such pairwise changeable evolutionary inter-dependencies; (2) a structure adaptive graph convolution (SAGC) that does not require pre-defined graphs to capture semantically richer spatial correlations. Then, we integrate the neural CRF, SAGC with recurrent neural network to develop a new STGNN paradigm termed Adaptive Spatial-Temporal graph neural network with Conditional Random Field (ASTCRF), which can be trained in an end-to-end fashion. We validate the effectiveness, efficiency, and scalability of ASTCRF on five public benchmark MTS datasets.","New York, NY, USA",35,"Yi, Peiyu and Huang, Feihu and Peng, Jian and Bao, Zhifeng",,10.1145/3675165,,2374-0353,December 2024,ACM Trans. Spatial Algorithms Syst.,"Multivariate time series, spatial-temporal embedding, conditional random field, evolving inter-series correlations",,,4,23,,Association for Computing Machinery,,Dynamic Spatial-Temporal Embedding via Neural Conditional Random Field for Multivariate Time Series Forecasting,https://doi.org/10.1145/3675165,10,2024
article,10.14778/3611540.3611561,"Time series forecasting, that predicts events through a sequence of time, has received increasing attention in past decades. The diverse range of time series forecasting models presents a challenge for selecting the most suitable model for a given dataset. As such, the Alibaba Cloud database monitoring system must address the issue of selecting an optimal forecasting model for a single time series data. While several model selection frameworks, including AutoAI-TS, have been developed to predict a dataset, their effectiveness may be limited as they may not adapt well to all types of time series, resulting in reduced prediction accuracy. Alternatively, models such as AutoForecast, which train on individual data points, may offer better adaptability but are limited by longer training time required.In this paper, we introduce SimpleTS, a versatile framework for time series forecasting that exhibits high efficiency and accuracy across all types of time series data. When performing an online prediction task, SimpleTS first classifies input time series into one type, and then efficiently selects the most suitable prediction model for this type. To optimize performance, SimpleTS (i) clusters models with similar performance to improve the efficiency of classification; (ii) uses soft labeling and weighted representation learning to achieve higher classification accuracy for different time series types. Extensive experiments on 3 private datasets and 52 public datasets show that SimpleTS outperforms the state-of-the-art toolkits in terms of both training time and prediction accuracy.",,,"Yao, Yuanyuan and Li, Dimeng and Jie, Hailiang and Jie, Hailiang and Li, Tianyi and Chen, Jie and Wang, Jiaqi and Li, Feifei and Gao, Yunjun",,10.14778/3611540.3611561,,2150-8097,August 2023,Proc. VLDB Endow.,,,,12,13,3741–3753,VLDB Endowment,,SimpleTS: An Efficient and Universal Model Selection Framework for Time Series Forecasting,https://doi.org/10.14778/3611540.3611561,16,2023
inproceedings,10.1145/3711896.3736861,"Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLU rred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90\%. Our code is available at https://github.com/slzhou-xy/BLUE.","New York, NY, USA",,"Zhou, Silin and Chen, Yao and Shang, Shuo and Chen, Lisi and He, Bingsheng and Shibasaki, Ryosuke",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736861,9798400714542,,,,"hierarchical patches, multiple levels, pyramid structure, trajectory representation learning","Toronto ON, Canada",,,12,4132–4143,Association for Computing Machinery,KDD '25,Blurred Encoding for Trajectory Representation Learning,https://doi.org/10.1145/3711896.3736861,,2025
inproceedings,10.1145/3534678.3539274,"Recent studies have shown great promise in applying graph neural networks for multivariate time series forecasting, where the interactions of time series are described as a graph structure and the variables are represented as the graph nodes. Along this line, existing methods usually assume that the graph structure (or the adjacency matrix), which determines the aggregation manner of graph neural network, is fixed either by definition or self-learning. However, the interactions of variables can be dynamic and evolutionary in real-world scenarios. Furthermore, the interactions of time series are quite different if they are observed at different time scales. To equip the graph neural network with a flexible and practical graph structure, in this paper, we investigate how to model the evolutionary and multi-scale interactions of time series. In particular, we first provide a hierarchical graph structure cooperated with the dilated convolution to capture the scale-specific correlations among time series. Then, a series of adjacency matrices are constructed under a recurrent manner to represent the evolving correlations at each layer. Moreover, a unified neural network is provided to integrate the components above to get the final prediction. In this way, we can capture the pair-wise correlations and temporal dependency simultaneously. Finally, experiments on both single-step and multi-step forecasting tasks demonstrate the superiority of our method over the state-of-the-art approaches.","New York, NY, USA",,"Ye, Junchen and Liu, Zihan and Du, Bowen and Sun, Leilei and Li, Weimiao and Fu, Yanjie and Xiong, Hui",Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3534678.3539274,9781450393850,,,,"deep learning, graph neural network, time series forecasting","Washington DC, USA",,,11,2296–2306,Association for Computing Machinery,KDD '22,Learning the Evolutionary and Multi-scale Graph Structure for Multivariate Time Series Forecasting,https://doi.org/10.1145/3534678.3539274,,2022
article,10.1145/3453724,"Time-series forecasting is an important problem across a wide range of domains. Designing accurate and prompt forecasting algorithms is a non-trivial task, as temporal data that arise in real applications often involve both non-linear dynamics and linear dependencies, and always have some mixtures of sequential and periodic patterns, such as daily, weekly repetitions, and so on. At this point, however, most recent deep models often use Recurrent Neural Networks (RNNs) to capture these temporal patterns, which is hard to parallelize and not fast enough for real-world applications especially when a huge amount of user requests are coming. Recently, CNNs have demonstrated significant advantages for sequence modeling tasks over the de-facto RNNs, while providing high computational efficiency due to the inherent parallelism. In this work, we propose HyDCNN, a novel hybrid framework based on fully Dilated CNN for time-series forecasting tasks. The core component in HyDCNN is a proposed hybrid module, in which our proposed position-aware dilated CNNs are utilized to capture the sequential non-linear dynamics and an autoregressive model is leveraged to capture the sequential linear dependencies. To further capture the periodic temporal patterns, a novel hop scheme is introduced in the hybrid module. HyDCNN is then composed of multiple hybrid modules to capture the sequential and periodic patterns. Each of these hybrid modules targets on either the sequential pattern or one kind of periodic patterns. Extensive experiments on five real-world datasets have shown that the proposed HyDCNN is better compared with state-of-the-art baselines and is at least 200\% better than RNN baselines. The datasets and source code will be published in Github to facilitate more future work.","New York, NY, USA",14,"Li, Yangfan and Li, Kenli and Chen, Cen and Zhou, Xu and Zeng, Zeng and Li, Keqin",,10.1145/3453724,,1556-4681,February 2022,ACM Trans. Knowl. Discov. Data,"Convolutional neural networks, dilated convolutions, time-series forecasting",,,1,22,,Association for Computing Machinery,,Modeling Temporal Patterns with Dilated Convolutions for Time-Series Forecasting,https://doi.org/10.1145/3453724,16,2021
inproceedings,10.1145/3604237.3626864,"Spatio-temporal modeling is an essential lens to understand many real-world phenomena from traffic to finance. There has been exciting work that explores spatio-temporal modeling with temporal graph convolutional networks. Often these methods assume that the spatial structure is static. We propose a new model Dyn-GWN&nbsp;for spatio-temporal learning from time-varying graphs. Our model relies on a novel module called the Tensor Graph Convolutional Module&nbsp;(TGCM), which captures dynamic trends in graphs effectively in the time-varying graph representations. This module has two components: (i) it applies temporal dilated convolutions both on the time-varying graph adjacency space and the time-varying features. (ii) it aggregates the higher-level latent representations from both time-varying components through a proposed layer TGCL. Experiments demonstrate the efficacy of these model across time-series data from finance and traffic domains. Dyn-GWN&nbsp; can give up to better out-of-sample performance than prior methods that learn from time-varying graphs, e.g., EvolveGCN and TM-GCN. Interestingly, Dyn-GWN&nbsp; can be ∼ 300 \texttimes{","New York, NY, USA",,"Ibrahim, Shibal and Tell, Max and Mazumder, Rahul",Proceedings of the Fourth ACM International Conference on AI in Finance,10.1145/3604237.3626864,9798400702402,,,,"Graph neural networks, Spatio-temporal modeling, Time-series forecasting, Time-varying graphs.","Brooklyn, NY, USA",,,9,167–175,Association for Computing Machinery,ICAIF '23,Dyn-GWN: Time-Series Forecasting using Time-varying Graphs with Applications to Finance and Traffic Prediction,https://doi.org/10.1145/3604237.3626864,,2023
inproceedings,10.1145/3711896.3736952,"Recent advancements in time-series forecasting have highlighted the importance of frequency-domain modeling. However, deep learning models primarily operate in the time domain, limiting their ability to capture frequency-based patterns. Existing approaches normally introduce novel neural network architectures tailored to task-specific frequency properties, yet they often lack generalization and require extensive domain-specific adaptations. In this paper, we propose FAT, a novel pretraining framework that learns generalizable Frequency-Aware Time-series representations through self-supervised learning. The key idea of FAT is to pretrain any backbone model to directly extract generalizable frequency patterns from time-domain signals and encode them into robust representations-eliminating the need for architectural modifications or additional modules during inference. This is achieved via a frequency reformer that amplifies critical frequency components learned through self-supervision and enforces similarity between the original and frequency-reformed time-series representations produced by the encoder. In addition, recognizing that semantically equivalent time-series can exhibit different frequency expressions-analogous to how the same phrase is pronounced differently by different speakers-FAT introduces a Knowledge-Guided Frequency Reformer that unifies the expression of frequency patterns with the same underlying semantics and extends similarity constraints to frequency-invariant augmented samples to enhance robustness of learned representation. Experiments on 14 benchmark datasets across regression and classification tasks show that FAT consistently achieves state-of-the-art performance while maintaining robustness across diverse backbone models, significantly outperforming existing pretraining methods. Our code is available at https://github.com/JiaXiangfei/FAT.","New York, NY, USA",,"Cheng, Rui and Jia, Xiangfei and Li, Qing and Xing, Rong and Huang, Jiwen and Zheng, Yu and Xie, Zhilong",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736952,9798400714542,,,,"frequency-domain modeling, representation learning, self-supervised pretraining, time series","Toronto ON, Canada",,,12,310–321,Association for Computing Machinery,KDD '25,FAT: Frequency-Aware Pretraining for Enhanced Time-Series Representation Learning,https://doi.org/10.1145/3711896.3736952,,2025
inproceedings,10.1145/3534678.3539396,"Multivariate Time Series (MTS) forecasting plays a vital role in a wide range of applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have become increasingly popular MTS forecasting methods. STGNNs jointly model the spatial and temporal patterns of MTS through graph neural networks and sequential models, significantly improving the prediction accuracy. But limited by model complexity, most STGNNs only consider short-term historical MTS data, such as data over the past one hour. However, the patterns of time series and the dependencies between them (i.e., the temporal and spatial patterns) need to be analyzed based on long-term historical MTS data. To address this issue, we propose a novel framework, in which STGNN is Enhanced by a scalable time series Pre-training model (STEP). Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series. Experiments on three public real-world datasets demonstrate that our framework is capable of significantly enhancing downstream STGNNs, and our pre-training model aptly captures temporal patterns.","New York, NY, USA",,"Shao, Zezhi and Zhang, Zhao and Wang, Fei and Xu, Yongjun",Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3534678.3539396,9781450393850,,,,"multivariate time series forecasting, pre-training model, spatial-temporal graph neural network","Washington DC, USA",,,11,1567–1577,Association for Computing Machinery,KDD '22,Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting,https://doi.org/10.1145/3534678.3539396,,2022
inbook,10.1145/3765612.3767245,"Electronic health Records (EHRs) have become a cornerstone in modern-day healthcare. They are a crucial part for analyzing the progression of patient health; however, their complexity, characterized by long, multivariate sequences, sparsity, and missing values-poses significant challenges in traditional deep learning modeling. While Transformer-based models have demonstrated success in modeling EHR data and predicting clinical outcomes, their quadratic computational complexity and limited context length hinder their efficiency and practical applications. On the other hand, State Space Models (SSMs) like Mamba present a promising alternative offering linear-time sequence modeling and improved efficiency for handling long sequences, but focus mostly on mixing sequence-level information rather than channel-level data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and Transformer Model for EHR Representation Learning), a novel hybrid model tailored for representing longitudinal data, combining the strengths of SSMs with advanced attention mechanisms. By testing the model on predictive tasks on multiple clinical datasets, we demonstrate HyMaTE's ability to capture an effective, richer, and more nuanced unified representation of EHR data. Additionally, the interpretability of the outcomes achieved by self-attention illustrates the effectiveness of our model as a scalable and generalizable solution for real-world healthcare applications. Codes are available at: https://github.com/healthylaife/HyMaTE.","New York, NY, USA",26,"Mottalib, Md Mozaharul and Phan, Thao-Ly T and Beheshti, Rahmatollah","Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",,9798400722004,,,,,,,,9,,Association for Computing Machinery,,HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning,https://doi.org/10.1145/3765612.3767245,,2025
inproceedings,10.1145/3551349.3560414,"Many real-world online systems expect to forecast the future trend of software quality to better automate operational processes, optimize software resource cost and ensure software reliability. To achieve that, all kinds of time series metrics collected from online software systems are adopted to characterize and monitor the quality of software services. To meet relevant software engineers’ requirements, we focus on time series forecasting and aim to provide an event-driven and self-adaptive forecasting service. In this paper, we present TTSF-transformer, a transferable time series forecasting service using deep transformer model. TTSF-transformer normalizes multiple metric frequencies to ensure the model sharing across multi-source systems, employs a deep transformer model with Bayesian estimation to generate the predictive marginal distribution, and introduces transfer learning and incremental learning into the training process to ensure the performance of long-term prediction. We conduct experiments on real-world time series metrics from two different types of game business in Tencent®. The results show that TTSF-transformer significantly outperforms other state-of-the-art methods and is suitable for wide deployment in large online systems.","New York, NY, USA",4,"Huang, Tao and Chen, Pengfei and Zhang, Jingrun and Li, Ruipeng and Wang, Rui",Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,10.1145/3551349.3560414,9781450394758,,,,"deep neural networks, online systems, time series prediction, transfer learning","Rochester, MI, USA",,,12,,Association for Computing Machinery,ASE '22,A Transferable Time Series Forecasting Service Using Deep Transformer Model for Online Systems,https://doi.org/10.1145/3551349.3560414,,2023
inproceedings,10.1145/3660605.3660942,"Cloud providers face issues with cost efficiency due to the inefficient utilization of resources within their clusters. To improve efficiency, cloud resource management systems use techniques such as autoscaling and overcommiting resources across users, where it is necessary to predict future resource usage. However, it is very challenging to generate accurate predictions, due to the wide variety of observed and unseen patterns. Recent works show that representation learning is very effective in generic time series forecasting. Inspired from this, we propose a system prototype for predicting cloud resource usage using representation learning. Our approach transforms time series into embeddings and explores their spatial proximity to generate predictions about future resource usage. Our experimental analysis acts as a proof-of-concept and shows that the proposed approach delivers highly accurate forecasts, significantly outperforming current machine learning baselines. Future directions of this work will focus on refining certain aspects of our proposed system and address remaining challenges and limitations.","New York, NY, USA",,"Ghorab, Razine Moundir and Doudali, Thaleia Dimitra",Proceedings of the 2024 Workshop on AI For Systems,10.1145/3660605.3660942,9798400706523,,,,"cloud computing, representation learning, machine learning, forecasting, prediction, resource usage","Pisa, Italy",,,7,13–19,Association for Computing Machinery,AI4Sys '24,Toward Using Representation Learning for Cloud Resource Usage Forecasting,https://doi.org/10.1145/3660605.3660942,,2024
article,10.5555/3648699.3648941,"Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled--yet practical--probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning problem, we are able to achieve state-of-the-art performance while controlling the sparsity of the learned graph and the computational scalability. We empirically assess the effectiveness of the proposed method on synthetic and real-world benchmarks, showing that the proposed solution can be used as a stand-alone graph identification procedure as well as a graph learning component of an end-to-end forecasting architecture.",,242,"Cini, Andrea and Zambon, Daniele and Alippi, Cesare",,,,1532-4435,January 2023,J. Mach. Learn. Res.,"graph learning, spatiotemporal data, graph-based forecasting, time series forecasting, score-based learning, graph neural networks",,,1,36,,JMLR.org,,Sparse graph learning from spatiotemporal time series,,24,2023
inproceedings,10.1145/3711129.3711336,"Accurate wind power forecasting is crucial for grid stability and renewable energy integration, but existing methods struggle to capture complex temporal dependencies in wind data. This paper introduces Collaborative Temporal Representation Learning (CTRL), a novel deep learning model that leverages collaborative representation learning to enhance forecasting accuracy and robustness. CTRL integrates Reversible Instance Normalization (RevIN), RNN-based hidden state learning, and a specialized collaborative representation unit to capture multi-directional temporal dynamics across different time scales and variables. Experimental results on two real-world wind power datasets demonstrate that CTRL significantly outperforms 20 existing methods, including state-of-the-art deep learning approaches, achieving up to 9.67\% and 10.42\% improvement in forecasting accuracy, respectively. These findings highlight the potential of collaborative representation learning for advancing wind power forecasting and facilitating the effective integration of renewable energy resources.","New York, NY, USA",,"Hu, Yue and Wu, Senzhen and Chen, Yu and He, Xinhao and Xie, Zihao and Wang, Zhijin and Liu, Xiufeng and Fu, Yonggang",Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering,10.1145/3711129.3711336,9798400710094,,,,"Collaboration, Representation learning, Temporal dynamics, Time series, Wind power forecasting","
",,,9,1219–1227,Association for Computing Machinery,EITCE '24,CTRL: Collaborative Temporal Representation Learning for Wind Power Forecasting,https://doi.org/10.1145/3711129.3711336,,2025
inproceedings,10.1145/3701716.3715267,"This paper introduces TransForeCaster, a novel user representation learning approach to improving prediction accuracy in purchase and churn predictions via a two-stage feature integration process: In-Category Integration (ICI) and Cross-Category Integration (CCI). The ICI stage employs a Time-Series Feature Mixer (TSFM) to capture the temporal dynamics of features within the same categories, resulting in compact and continuous category representations. The CCI stage utilizes a Meta-Conditioned Transformer (MCT) to integrate the representations with multi-task learning, capturing complex relationships across categories and improving interpretability through attention mechanisms. Empirical evaluations of real-world datasets demonstrate significant improvements over conventional models, supported by qualitative analyses using feature importance assessments and UMAP visualizations. TransForeCaster's robustness is validated by its superior performance over other models in multiple in-house deployments across various games and applications. The source code is available at https://github.com/bagelcode-data-science-team/TransForeCaster.","New York, NY, USA",,"Myung, Hyunkee and Yun, Junwoo and Kwak, Wonryeol and Lee, Yoonbyung and Kim, Joongsu and Kim, Joohyun",Companion Proceedings of the ACM on Web Conference 2025,10.1145/3701716.3715267,9798400713316,,,,"churn prediction, feature engineering, feature integration, multi-task learning, purchase prediction, representation learning","Sydney NSW, Australia",,,10,403–412,Association for Computing Machinery,WWW '25,TransForeCaster: In-and-Cross Categorized Feature Integration in User Representation Learning,https://doi.org/10.1145/3701716.3715267,,2025
inproceedings,10.1145/3383455.3422550,"We present a representation learning framework for financial time series forecasting. One challenge of using deep learning models for finance forecasting is the shortage of available training data when using small datasets. Direct trend classification using deep neural networks trained on small datasets is susceptible to the overfitting problem. In this paper, we propose to first learn compact representations from time series data, then use the learned representations to train a simpler model for predicting time series movements. We consider a class-conditioned latent variable model. We train an encoder network to maximize the mutual information between the latent variables and the trend information conditioned on the encoded observed variables. We show that conditional mutual information maximization can be approximated by a contrastive loss. Then, the problem is transformed into a classification task of determining whether two encoded representations are sampled from the same class or not. This is equivalent to performing pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the encoder network. We use deep autoregressive models as our encoder to capture long-term dependencies of the sequence data. Empirical experiments indicate that our proposed method has the potential to advance state-of-the-art performance.","New York, NY, USA",9,"Wu, Hanwei and Gattami, Ather and Flierl, Markus",Proceedings of the First ACM International Conference on AI in Finance,10.1145/3383455.3422550,9781450375849,,,,,"New York, New York",,,7,,Association for Computing Machinery,ICAIF '20,Conditional mutual information-based contrastive loss for financial time series forecasting,https://doi.org/10.1145/3383455.3422550,,2021
inproceedings,10.1145/3695053.3731091,"Graph learning on dynamical systems has recently surfaced as an emerging research domain. By leveraging a novel electronic Dynamical System (DS), various graph learning challenges have been effectively tackled through a rapid, spontaneous natural annealing process. This method has attracted increasing attention due to its orders-of-magnitude improvements in speed and energy efficiency compared to traditional Graph Neural Network (GNN) approaches for inference tasks. However, (1) the current DS hardware only supports inference, missing its native solution for training; while relying on conventional hardware is likely more expensive than GNNs. (2) The current DS architecture only allows linear interactions among its nodes, limiting training accuracy.In this work, we present a Dynamical-System Training-Processing Unit (DS-TPU) developed through algorithm-architecture co-design to tackle the two major challenges: (1) An on-device lifelong learning mechanism that leverages feedback electric current as the loss function in response to the observed training data, allowing electron-speed refinement on the present model parameters. (2) A nonlinear DS node interaction mechanism constructed from Chebyshev polynomials to significantly improve the compatibility between the DS hardware and the embedded relation of graph data. Extensive evaluations using six real-world graph learning applications demonstrate that for accuracy, DS-TPU achieves 10.8\% MAE reduction over the best results of five widely used GNNs. In terms of training performance, the 5-Watt DS-TPU architecture achieves on-average 810 \texttimes{","New York, NY, USA",,"Wu, Chunshu and Song, Ruibing and Liu, Chuan and Haghi, Pouya and Li, Ang and Huang, Michael and Geng, Tong (Tony)",Proceedings of the 52nd Annual International Symposium on Computer Architecture,10.1145/3695053.3731091,9798400712616,,,,"Dynamical System, Energy-Based Model, Graph Learning","
",,,13,1867–1879,Association for Computing Machinery,ISCA '25,DS-TPU: Dynamical System for on-Device Lifelong Graph Learning with Nonlinear Node Interaction,https://doi.org/10.1145/3695053.3731091,,2025
inproceedings,10.1145/3664647.3681229,"The limited data availability and the low signal-to-noise ratio of fMRI signals lead to the challenging task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a large model, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer (ViT). However, significant individual variations exist among subjects, even under identical experimental setups, mandating the training of large subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices. To this end, we propose Lite-Mind, a lightweight, efficient, and robust brain representation learning paradigm based on Discrete Fourier Transform (DFT), which efficiently aligns fMRI voxels to fine-grained information of CLIP. We elaborately design a DFT backbone with Spectrum Compression and Frequency Projector modules to learn informative and robust voxel embeddings. Our experiments demonstrate that Lite-Mind achieves an impressive 94.6\% fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7\% fewer parameters than MindEye. Lite-Mind is also proven to be able to be migrated to smaller fMRI datasets and establishes a new state-of-the-art for zero-shot classification on the GOD dataset.","New York, NY, USA",,"Gong, Zixuan and Zhang, Qi and Bao, Guangyin and Zhu, Lei and Zhang, Yu and Liu, Ke and Hu, Liang and Miao, Duoqian",Proceedings of the 32nd ACM International Conference on Multimedia,10.1145/3664647.3681229,9798400706868,,,,"brain-computer interface (bci), cross-modal retrieval, fmri","Melbourne VIC, Australia",,,10,4014–4023,Association for Computing Machinery,MM '24,Lite-Mind: Towards Efficient and Robust Brain Representation Learning,https://doi.org/10.1145/3664647.3681229,,2024
article,10.1145/3719207,"Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across seven time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TS’s effectiveness with representation learning in forecasting tasks. Ablation studies further validate each component’s contribution to LLM4TS and underscore the essential role of utilizing LLM’s pre-trained weights for optimal performance. The code is available at .","New York, NY, USA",60,"Chang, Ching and Wang, Wei-Yao and Peng, Wen-Chih and Chen, Tien-Fu",,10.1145/3719207,,2157-6904,June 2025,ACM Trans. Intell. Syst. Technol.,"Time-Series Forecasting, Large Language Models, Representation Learning",,,3,20,,Association for Computing Machinery,,LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters,https://doi.org/10.1145/3719207,16,2025
article,10.1145/3655629,"Accurate urban flow prediction (UFP) is crucial for a range of smart city applications such as traffic management, urban planning, and risk assessment. To capture the intrinsic characteristics of urban flow, recent efforts have utilized spatial and temporal graph neural networks to deal with the complex dependence between the traffic in adjacent areas. However, existing graph neural network based approaches suffer from several critical drawbacks, including improper graph representation of urban traffic data, lack of semantic correlation modeling among graph nodes, and coarse-grained exploitation of external factors. To address these issues, we propose DiffUFP, a novel probabilistic graph-based framework for UFP. DiffUFP&nbsp;consists of two key designs: (1) a semantic region dynamic extraction method that effectively captures the underlying traffic network topology, and (2) a conditional denoising score-based adjacency matrix generator that takes spatial, temporal, and external factors into account when constructing the adjacency matrix rather than simply concatenation in existing studies. Extensive experiments conducted on real-world datasets demonstrate the superiority of DiffUFP&nbsp;over the state-of-the-art UFP&nbsp;models and the effect of the two specific modules.","New York, NY, USA",59,"Wang, Pengyu and Luo, Xuechen and Tai, Wenxin and Zhang, Kunpeng and Trajcevsky, Goce and Zhou, Fan",,10.1145/3655629,,2157-6904,June 2024,ACM Trans. Intell. Syst. Technol.,"Flow prediction, spatio-temporal learning, graph neural networks, score-based model, urban computing",,,3,25,,Association for Computing Machinery,,Score-based Graph Learning for Urban Flow Prediction,https://doi.org/10.1145/3655629,15,2024
inproceedings,10.1145/3580305.3599295,"Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior discrimination abilities. Extensive experiments show that DCdetector achieves state-of-the-art results on multiple time series anomaly detection benchmark datasets. Code is publicly available at https://github.com/DAMO-DI-ML/KDD2023-DCdetector.","New York, NY, USA",,"Yang, Yiyuan and Zhang, Chaoli and Zhou, Tian and Wen, Qingsong and Sun, Liang",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599295,9798400701030,,,,"contrastive learning, representation learning, self-supervised learning, time series anomaly detection","Long Beach, CA, USA",,,13,3033–3045,Association for Computing Machinery,KDD '23,DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection,https://doi.org/10.1145/3580305.3599295,,2023
inproceedings,10.1145/3447548.3467330,"Multi-variate time series (MTS) data is a ubiquitous class of data abstraction in the real world. Any instance of MTS is generated from a hybrid dynamical system with their specific dynamics normally unknown. The hybrid nature of such a dynamical system is a result of complex external impacts, which can be summarized as high-frequency and low-frequency from the temporal view, or global and local if we take the spatial view. These impacts also determine the forthcoming development of MTS making them paramount to capture in a time series forecasting task. However, conventional methods face intrinsic difficulties in disentangling the components yielded by each kind of impact from the raw data. To this end, we propose two kinds of normalization modules -- temporal and spatial normalization -- which separately refine the high-frequency component and the local component underlying the raw data. Moreover, both modules can be readily integrated into canonical deep learning architectures such as Wavenet and Transformer. Extensive experiments on three datasets are conducted to illustrate that, with additional normalization modules, the performance of the canonical architectures can be enhanced by a large margin in the application of MTS and achieves state-of-the-art results compared with existing MTS models.","New York, NY, USA",,"Deng, Jinliang and Chen, Xiusi and Jiang, Renhe and Song, Xuan and Tsang, Ivor W.",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467330,9781450383325,,,,"deep learning, normalization, time series forecasting","Virtual Event, Singapore",,,10,269–278,Association for Computing Machinery,KDD '21,ST-Norm: Spatial and Temporal Normalization for Multi-variate Time Series Forecasting,https://doi.org/10.1145/3447548.3467330,,2021
inproceedings,10.1145/3677052.3698680,"Current GNN-based asset price prediction models often focus on a fixed group of assets and their static relationships within the financial network. However, this approach overlooks the reality that the composition of asset pools and their interrelationships evolves over time, necessitating the development of a flexible framework capable of adapting to this dynamism. Accordingly, we propose DySTAGE, a framework with a universal formulation that transforms asset pricing time series into dynamic graphs, accommodating asset addition, deletion, and changes in correlations. Our framework includes a graph learning model specifically designed for this purpose. In our framework, assets at various historical time steps are structured as a sequence of dynamic graphs, where connections between assets reflect their long-term correlations. DySTAGE effectively captures both topological and temporal patterns. The Topological Module deploys Asset Influence Attention to learn global interrelationships among assets, further enhanced by Asset-wise Importance Encoding, Pair-wise Spatial Encoding, and Edge-wise Correlation Encoding. Meanwhile, the Temporal Module encapsulates node representations across the temporal dimension via the attention mechanism. We validate our approach through extensive experiments using three different real-world stock pricing data, demonstrating that DySTAGE surpasses popular benchmarks in return prediction, and offers profitable investment strategies. The code is publicly available under NJIT FinTech Lab’s GitHub1.","New York, NY, USA",,"Gu, Jingyi and Ye, Junyi and Uddin, Ajim and Wang, Guiling",Proceedings of the 5th ACM International Conference on AI in Finance,10.1145/3677052.3698680,9798400710810,,,,"Asset Pricing, Dynamic Graph, Graph Neural Networks, Stock Price Prediction","Brooklyn, NY, USA",,,9,388–396,Association for Computing Machinery,ICAIF '24,DySTAGE: Dynamic Graph Representation Learning for Asset Pricing via Spatio-Temporal Attention and Graph Encodings,https://doi.org/10.1145/3677052.3698680,,2024
inproceedings,10.1145/3627673.3679854,"Integrated Warehousing and Distribution Supply Networks (IWDSN) have shown their high efficiency in E-commerce. Efficient supply capacity prediction is crucial for logistics systems to maintain the delivery capacity to meet users' requirements. However, unforeseen events such as extreme weather and public health emergencies pose challenges in supply forecasting. Previous work mainly infers supply optimization based on the invariant topology of logistic networks, neglecting dynamic routing and distinct node effects reacting to emergencies. To address these challenges, the hierarchical relations among warehouses, sorting centers, and delivery stations in logistic networks are necessary to learn the diverse reactions. In this paper, we propose a hierarchical spatio-temporal graph learning model to predict the emergency supply capacity of IWDSN based on micro and macro graphs. The micro graph shows transportation connectivity while the macro graph shows the geographical correlation. Specifically, it consists of three components. (1) For micro graphs, a metapath aggregation strategy is designed to capture dynamic routing information on both route-view and event-view graphs. (2) For macro graphs, a bipartite graph learning approach to extract spatial representations. (3) For spatio-temporal feature fusion, the spatio-temporal joint forecasting module combines the temporal feature from the time-series encoder with hierarchical spatial features to predict the future supply capacity. The extensive experiments on two real-world datasets demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performance compared with advanced baselines.","New York, NY, USA",,"Lin, Li and Xia, Kaiwen and Zheng, Anqi and Hu, Shijie and Wang, Shuai",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679854,9798400704369,,,,"emergency supply network, heterogeneous information network, metapath aggregation, supply forecasting","Boise, ID, USA",,,10,1410–1419,Association for Computing Machinery,CIKM '24,Hierarchical Spatio-Temporal Graph Learning Based on Metapath Aggregation for Emergency Supply Forecasting,https://doi.org/10.1145/3627673.3679854,,2024
article,10.14778/3489496.3489503,"Multivariate time series forecasting has been drawing increasing attention due to its prevalent applications. It has been commonly assumed that leveraging latent dependencies between pairs of variables can enhance prediction accuracy. However, most existing methods suffer from static variable relevance modeling and ignorance of correlation between temporal scales, thereby failing to fully retain the dynamic and periodic interdependencies among variables, which are vital for long- and short-term forecasting. In this paper, we propose METRO, a generic framework with multi-scale temporal graphs neural networks, which models the dynamic and cross-scale variable correlations simultaneously. By representing the multivariate time series as a series of temporal graphs, both intra- and inter-step correlations can be well preserved via message-passing and node embedding update. To enable information propagation across temporal scales, we design a novel sampling strategy to align specific steps between higher and lower scales and fuse the cross-scale information efficiently. Moreover, we provide a modular interpretation of existing GNN-based time series forecasting works as specific instances under our framework. Extensive experiments conducted on four benchmark datasets demonstrate the effectiveness and efficiency of our approach. METRO has been successfully deployed onto the time series analytics platform of Huawei Cloud, where a one-month online test demonstrated that up to 20\% relative improvement over state-of-the-art models w.r.t. RSE can be achieved.",,,"Cui, Yue and Zheng, Kai and Cui, Dingshan and Xie, Jiandong and Deng, Liwei and Huang, Feiteng and Zhou, Xiaofang",,10.14778/3489496.3489503,,2150-8097,October 2021,Proc. VLDB Endow.,,,,2,13,224–236,VLDB Endowment,,METRO: a generic graph neural network framework for multivariate time series forecasting,https://doi.org/10.14778/3489496.3489503,15,2021
inproceedings,10.1145/3485447.3512085,"Users’ social connections have recently shown significant benefits to session-based recommendations, and graph neural networks have demonstrated great success in learning the pattern of information flow among users. However, the current paradigm presumes a given social network, which is not necessarily consistent with the fast-evolving shared interests and is expensive to collect. We propose a novel idea to learn the graph structure among users and make recommendations collectively in a coupled framework. This idea raises two challenges, i.e., scalability and effectiveness. We introduce a novel graph-structure learning framework for session-based recommendations&nbsp;(GSL4Rec) for solving both challenges simultaneously. Our framework has a two-stage strategy, i.e., the coarse neighbor screening and the self-adaptive graph structure learning, to enable the exploration of potential links among all users while maintaining a tractable amount of computation for scalability. We also propose a phased heuristic learning strategy to sequentially and synergistically train the graph learning part and recommendation part of GSL4Rec, thus improving the effectiveness by making the model easier to achieve good local optima. Experiments on five public datasets demonstrate that our proposed model significantly outperforms strong baselines, including state-of-the-art social network-based methods.","New York, NY, USA",,"Wei, Chunyu and Bai, Bing and Bai, Kun and Wang, Fei",Proceedings of the ACM Web Conference 2022,10.1145/3485447.3512085,9781450390965,,,,"graph neural networks, graph structure learning, session-based recommendations","Virtual Event, Lyon, France",,,11,2120–2130,Association for Computing Machinery,WWW '22,GSL4Rec: Session-based Recommendations with Collective Graph Structure Learning and Next Interaction Prediction,https://doi.org/10.1145/3485447.3512085,,2022
inproceedings,10.1145/3651671.3651693,"The early detection of potential failures in industrial machinery components is paramount for ensuring the reliability and safety of operations, thereby preserving Machine Condition Monitoring (MCM). This research addresses this imperative by introducing an innovative approach to Real-Time Acoustic Anomaly Detection. Our method combines semi-supervised temporal convolution with representation learning and a hybrid model strategy with Temporal Convolutional Networks (TCN) to handle various intricate anomaly patterns found in acoustic data effectively. The proposed model demonstrates superior performance compared to established research in the field, underscoring the effectiveness of this approach. Not only do we present quantitative evidence of its superiority, but we also employ visual representations, such as t-SNE plots, to further substantiate the model’s efficacy.","New York, NY, USA",,"Dissanayaka, Sahan and Wickramasinghe, Manjusri and Marasinghe, Pasindu",Proceedings of the 2024 16th International Conference on Machine Learning and Computing,10.1145/3651671.3651693,9798400709234,,,,"Anomaly Detection, Deep Learning, Hybrid Modelling, Signal Processing, Time Series","Shenzhen, China",,,10,218–227,Association for Computing Machinery,ICMLC '24,Temporal Convolution-based Hybrid Model Approach with Representation Learning for Real-Time Acoustic Anomaly Detection,https://doi.org/10.1145/3651671.3651693,,2024
inproceedings,10.1145/3514094.3534183,"Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.","New York, NY, USA",,"Garcia de Macedo, Maysa Malfiza and Clarke, Wyatt and Lucherini, Eli and Baldwin, Tyler and Queiroz Neto, Dilermando and de Paula, Rogerio Abreu and Das, Subhro","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",10.1145/3514094.3534183,9781450392471,,,,"demand forecasting, labor economics, neural networks, skill representation","Oxford, United Kingdom",,,10,285–294,Association for Computing Machinery,AIES '22,Practical Skills Demand Forecasting via Representation Learning of Temporal Dynamics,https://doi.org/10.1145/3514094.3534183,,2022
inproceedings,10.1145/3447548.3467401,"We present a novel framework for multivariate time series representation learning based on the transformer encoder architecture. The framework includes an unsupervised pre-training scheme, which can offer substantial performance benefits over fully supervised learning on downstream tasks, both with but even without leveraging additional unlabeled data, i.e., by reusing the existing data samples. Evaluating our framework on several public multivariate time series datasets from various domains and with diverse characteristics, we demonstrate that it performs significantly better than the best currently available methods for regression and classification, even for datasets which consist of only a few hundred training samples. Given the pronounced interest in unsupervised learning for nearly all domains in the sciences and in industry, these findings represent an important landmark, presenting the first unsupervised method shown to push the limits of state-of-the-art performance for multivariate time series regression and classification.","New York, NY, USA",,"Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467401,9781450383325,,,,"classification, deep learning, framework, imputation, multivariate time series, regression, self-supervised learning, transformer, unsupervised learning","Virtual Event, Singapore",,,11,2114–2124,Association for Computing Machinery,KDD '21,A Transformer-based Framework for Multivariate Time Series Representation Learning,https://doi.org/10.1145/3447548.3467401,,2021
inproceedings,10.1145/3605098.3635976,"This paper develops a graph reinforcement learning approach to online planning of the schedule and destinations of electric aircraft that comprise an urban air mobility (UAM) fleet operating across multiple vertiports. This fleet scheduling problem is formulated to consider time-varying demand, constraints related to vertiport capacity, aircraft capacity and airspace safety guidelines, uncertainties related to take-off delay, weather-induced route closures, and unanticipated aircraft downtime. Collectively, such a formulation presents greater complexity, and potentially increased realism, than in existing UAM fleet planning implementations. To address these complexities, a new policy architecture is constructed, primary components of which include: graph capsule conv-nets for encoding vertiport and aircraft-fleet states both abstracted as graphs; transformer layers encoding time series information on demand and passenger fare; and a Multi-head Attention-based decoder that uses the encoded information to compute the probability of selecting each available destination for an aircraft. Trained with Proximal Policy Optimization, this policy architecture shows significantly better performance in terms of daily averaged profits on unseen test scenarios involving 8 vertiports and 40 aircraft, when compared to a random baseline and genetic algorithm-derived optimal solutions, while being nearly 1000 times faster in execution than the latter.","New York, NY, USA",,"Paul, Steve and Witter, Jhoel and Chowdhury, Souma",Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing,10.1145/3605098.3635976,9798400702433,,,,"multi-agent systems, urban air mobility, reinforcement learning","Avila, Spain",,,8,638–645,Association for Computing Machinery,SAC '24,"Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand \&amp; Uncertainties",https://doi.org/10.1145/3605098.3635976,,2024
inproceedings,10.1145/3459637.3482413,"Recent studies suggest that financial networks play an essential role in asset valuation and investment decisions. Unlike road networks, financial networks are neither given nor static, posing significant challenges in learning meaningful networks and promoting their applications in price prediction. In this paper, we first apply the attention mechanism to connect the ","New York, NY, USA",,"Uddin, Ajim and Tao, Xinyuan and Yu, Dantong",Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management,10.1145/3459637.3482413,9781450384469,,,,"asset pricing, diffusion recurrent convolution, fintech, graph attention, graph neural networks, stock price prediction","Virtual Event, Queensland, Australia",,,10,1844–1853,Association for Computing Machinery,CIKM '21,Attention Based Dynamic Graph Learning Framework for Asset Pricing,https://doi.org/10.1145/3459637.3482413,,2021
inproceedings,10.1145/3664647.3680977,"Current research in food analysis primarily concentrates on tasks such as food recognition, recipe retrieval and nutrition estimation from a single image. Nevertheless, there is a significant gap in exploring the impact of food intake on physiological indicators (e.g., weight) over time. This paper addresses this gap by introducing the DietDiary dataset, which encompasses daily dietary diaries and corresponding weight measurements of real users. Furthermore, we propose a novel task of weight prediction with a dietary diary that aims to leverage historical food intake and weight to predict future weights. To tackle this task, we propose a model-agnostic time series forecasting framework. Specifically, we introduce a Unified Meal Representation Learning (UMRL) module to extract representations for each meal. Additionally, we design a diet-aware loss function to associate food intake with weight variations. By conducting experiments on the DietDiary dataset with two state-of-the-art time series forecasting models, NLinear and iTransformer, we demonstrate that our proposed framework achieves superior performance compared to the original models. We make our dataset, code, and models publicly available at: https://yxg1005.github.io/weight-prediction.","New York, NY, USA",,"Gui, Yinxuan and Zhu, Bin and Chen, Jingjing and Ngo, Chong Wah and Jiang, Yu-Gang",Proceedings of the 32nd ACM International Conference on Multimedia,10.1145/3664647.3680977,9798400706868,,,,"food analysis, time series forecasting models, weight prediction","Melbourne VIC, Australia",,,10,127–136,Association for Computing Machinery,MM '24,Navigating Weight Prediction with Diet Diary,https://doi.org/10.1145/3664647.3680977,,2024
article,10.14778/3746405.3746436,"Graph Neural Networks (GNNs) have become a cornerstone in multivariate time series forecasting by addressing the challenge of modeling inter-series dependencies often overlooked by traditional temporal approaches. However, real-world temporal dependencies (inter- and intra-dependencies) are inherently intertwined, making it difficult to treat them as separate processes. Recent pure graph paradigms attempt to capture these dependencies holistically by transforming time series into fully connected graphs. While effective, these methods suffer from prohibitive computational complexity O((NT)2), limiting their scalability for large-scale data and long-term forecasting. To address these challenges, we propose UFGTime, a novel framework that leverages spectral signals to construct a ",,,"Li, Ruikun and Shi, Dai and Xiao, Ye and Gao, Junbin",,10.14778/3746405.3746436,,2150-8097,May 2025,Proc. VLDB Endow.,,,,9,14,3175–3188,VLDB Endowment,,UFGTime: Mining Intertwined Dependencies in Multivariate Time Series via an Efficient Pure Graph Approach,https://doi.org/10.14778/3746405.3746436,18,2025
article,10.1145/3605894,"Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world applications. Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. However, properly coping with the TTS is a much more challenging task, due to its high-dimensional and complex inner structure. In this article, we start by revealing the structure of TTS data from afn statistical view of point. Then, in line with this analysis, we perform Tensor Time Series forecasting via a proposed Multi-way Normalization (TTS-Norm), which effectively disentangles multiple heterogeneous low-dimensional substructures from the original high-dimensional structure. Finally, we design a novel objective function for TTS forecasting, accounting for the numerical heterogeneity among different low-dimensional subspaces of TTS. Extensive experiments on two real-world datasets verify the superior performance of our proposed model.1","New York, NY, USA",3,"Deng, Jiewen and Deng, Jinliang and Yin, Du and Jiang, Renhe and Song, Xuan",,10.1145/3605894,,1556-4681,January 2024,ACM Trans. Knowl. Discov. Data,"Tensor time series forecasting, representation learning, normalization, neural network",,,1,25,,Association for Computing Machinery,,TTS-Norm: Forecasting Tensor Time Series via Multi-Way Normalization,https://doi.org/10.1145/3605894,18,2023
inproceedings,10.1145/3746027.3755775,"The rapid urbanization process has significantly increased building energy consumption and carbon emissions, making reliable electricity load forecasting crucial for energy management. However, accurate load forecasting faces three key challenges: (1) complex impact of multimodal data, (2) inter-building semantical relationships, and (3) uncertainty modeling of load patterns. To address these, we propose MMLoad, a novel diffusion-based multimodal framework for multi-scenario building load forecasting with three innovations: (i) a Multimodal Data Enhancement Pipeline generating rich building descriptions using LLMs and integrating temporal factors to analyze multimodal impacts; (ii) a Cross-modal Relation Encoder discovering latent interdependencies through hierarchical fusion, projecting buildings into a unified spatio-temporal (ST) embedding space; and (iii) a Scenario-Conditioned Diffusion Generator employing transformer-based denoising with Scenario-Adaptive Normalization (SAN) for diverse trajectory generation with uncertainty quantification. Experiments show MMLoad outperforms state-of-the-art baselines in accuracy while generating plausible future scenarios, establishing a new paradigm for multimodal learning in smart energy systems.","New York, NY, USA",,"Liu, Yongzheng and Zhong, Siru and Luo, Gefeng and Ruan, Weilin and Liang, Yuxuan",Proceedings of the 33rd ACM International Conference on Multimedia,10.1145/3746027.3755775,9798400720352,,,,"diffusion model, multimodal, time series forecasting","Dublin, Ireland",,,9,2188–2196,Association for Computing Machinery,MM '25,Towards Multi-Scenario Forecasting of Building Electricity Loads with Multimodal Data,https://doi.org/10.1145/3746027.3755775,,2025
inproceedings,10.1145/3539597.3570440,"Accurate telecommunication time series forecasting is critical for smart management systems of cellular networks, and has a special challenge in predicting different types of time series simultaneously at one base station (BS), e.g., the SMS, Calls, and Internet. Unlike the well-studied single target forecasting problem for one BS, this distributed multi-target forecasting problem should take advantage of both the intra-BS dependence of different types of time series at the same BS and the inter-BS dependence of time series at different BS. To this end, we first propose a model to learn the inter-BS dependence by aggregating the multi-view dependence, e.g., from the viewpoint of SMS, Calls, and Internet. To incorporate the interBS dependence in time series forecasting, we then propose a Graph Gate LSTM (GGLSTM) model that includes a graph-based gate mechanism to unite those base stations with a strong dependence on learning a collaboratively strengthened prediction model. We also extract the intra-BS dependence by an attention network and use it in the final prediction. Our proposed approach is evaluated on two real-world datasets. Experiment results demonstrate the effectiveness of our model in predicting multiple types of telecom traffic at the distributed base stations.","New York, NY, USA",,"Gou, Xiaochuan and Zhang, Xiangliang",Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,10.1145/3539597.3570440,9781450394079,,,,"multi-source time series forecasting, multi-task learning, telecommunication traffic forecasting","Singapore, Singapore",,,9,859–867,Association for Computing Machinery,WSDM '23,Telecommunication Traffic Forecasting via Multi-task Learning,https://doi.org/10.1145/3539597.3570440,,2023
inproceedings,10.1145/3589132.3625567,"Forecasting the number of visits to Points-of-Interest (POI) in an urban area is critical for planning and decision making in various application domains, from urban planning and transportation management to public health and social studies. Although this forecasting problem can be formulated as a multivariate time-series forecasting task, current approaches cannot fully exploit the ever-changing multi-context correlations among POIs. Therefore, we propose Busyness Graph Neural Network (BysGNN), a temporal graph neural network designed to learn and uncover the underlying multi-context correlations between POIs for accurate visit forecasting. Unlike other approaches where only time-series data is used to learn a dynamic graph, BysGNN utilizes all contextual information and time-series data to learn an accurate dynamic graph representation. By incorporating all contextual, temporal, and spatial signals, we observe a significant improvement in our forecasting accuracy over state-of-the-art forecasting models in our experiments with real-world datasets across the United States.","New York, NY, USA",22,"Hajisafi, Arash and Lin, Haowen and Shaham, Sina and Hu, Haoji and Siampou, Maria Despoina and Chiang, Yao-Yi and Shahabi, Cyrus",Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems,10.1145/3589132.3625567,9798400701689,,,,"graph neural networks, time-series forecasting, POI visiting patterns, multi-context correlations","Hamburg, Germany",,,12,,Association for Computing Machinery,SIGSPATIAL '23,Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting,https://doi.org/10.1145/3589132.3625567,,2023
inproceedings,10.1145/3767052.3767086,"This paper proposes a stock price prediction method based on a graph neural network architecture. The method is designed to address key characteristics of the stock market, including high nonlinearity, multivariable dependencies, and dynamically changing structural relationships. It constructs a dynamic stock graph to represent the evolving relationship network among individual stocks over time. A temporal-aware graph neural network module is designed to jointly model node features through structural propagation and temporal dependence. Specifically, the model incorporates multi-source heterogeneous information to build the dynamic graph structure. This enables explicit representation of the time-varying linkages between stocks within the graph. Graph convolution is then applied to extract structural features at each time step. A temporal module is used to model the evolution of these features over time. To validate the effectiveness of the method, the model is compared with existing graph-based and time-series models across multiple evaluation metrics. Ablation studies, robustness tests, and performance assessments under different market conditions are conducted to comprehensively analyze the model's behavior in various scenarios. Experimental results show that the proposed method achieves low prediction error while maintaining strong stability and generalization ability. It significantly improves the accuracy of modeling asset price trends in financial markets. This study provides a unified solution for structural and dynamic aspects of the stock prediction problem and extends the application scope of graph neural networks in financial time series analysis.","New York, NY, USA",,"Xu, Qingqing","Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy",10.1145/3767052.3767086,9798400716010,,,,"Structural modeling, graph neural networks, stock price forecasting, time series forecasting","
",,,8,219–226,Association for Computing Machinery,BDAIE '25,Capturing Structural Evolution in Financial Markets with Graph Neural Time Series Models,https://doi.org/10.1145/3767052.3767086,,2025
article,10.1145/3663760,"A Session-Based Recommendation (SBR) seeks to predict users’ future item preferences by analyzing their interactions with previously clicked items. In recent approaches, Graph Neural Networks (GNNs) have been commonly applied to capture item relations within a session to infer user intentions. However, these GNN-based methods typically struggle with feature ambiguity between the sequential session information and the item conversion within an item graph, which may impede the model’s ability to accurately infer user intentions. In this article, we propose a novel Multi-hop Multi-view Memory Transformer (M3T) to effectively integrate the sequence-view information and relation conversion (graph-view information) of items in a session. First, we propose a Multi-view Memory Transformer (M2T) module to concurrently obtain multi-view information of items. Then, a set of trainable memory matrices are employed to store sharable item features, which mitigates cross-view item feature ambiguity. To comprehensively capture latent user intentions, an M3T framework is designed to integrate user intentions across different hops of an item graph. Specifically, a k-order power method is proposed to manage the item graph to alleviate the over-smoothing problem when obtaining high-order relations of items. Extensive experiments conducted on three real-world datasets demonstrate the superiority of our method.","New York, NY, USA",144,"Zhuo, Xingrui and Qian, Shengsheng and Hu, Jun and Dai, Fuxin and Lin, Kangyi and Wu, Gongqing",,10.1145/3663760,,1046-8188,November 2024,ACM Trans. Inf. Syst.,"Session-based recommendation, multi-view intention fusion, memory Transformer, multi-hop graph embedding",,,6,28,,Association for Computing Machinery,,Multi-Hop Multi-View Memory Transformer for Session-Based Recommendation,https://doi.org/10.1145/3663760,42,2024
inproceedings,10.1145/3711896.3736962,"Climate change and rapid urbanization have led to the Urban Heat Island (UHI) effect, resulting in higher temperatures in metropolitan areas and negatively impacting urban communities. Accurate UHI forecasting is crucial for identifying high-risk periods and locations, especially in cities with vulnerable populations. Current methods are limited by data granularity and inadequate modeling of regional thermodynamics, which affects both accuracy and spatio-temporal granularity. In this paper, we propose DeepUHI, a data-driven context-aware framework for modeling local thermodynamics based on the heat equation, alongside the SeoulTemp dataset, the first multi-modal dataset for UHI effect predictions at the street level. Our framework utilizes a heat decomposition method to represent urban thermodynamics through thermodynamic cycles and thermal flows, effectively integrating urban environmental data. Extensive experiments show that our framework improves accuracy in UHI effect prediction and warning tasks, outperforming leading models. We have integrated DeepUHI into our SeoUHI platform to provide hourly street-level UHI forecasting for Seoul. The code, platform, and dataset are accessible at https://github.com/CityMind-Lab/DeepUHI.","New York, NY, USA",,"Zou, Xingchen and Ruan, Weilin and Zhong, Siru and Hu, Yuehong and Liang, Yuxuan",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736962,9798400714542,,,,"spatio-temporal data mining, thermodynamic modeling, time series forecasting, urban computing, urban heat island effect","Toronto ON, Canada",,,12,4226–4237,Association for Computing Machinery,KDD '25,Fine-grained Urban Heat Island Effect Forecasting: A Context-aware Thermodynamic Modeling Framework,https://doi.org/10.1145/3711896.3736962,,2025
inproceedings,10.1145/3488560.3498428,"Temporal graph embedding has been widely studied thanks to its superiority in tasks such as prediction and recommendation. Despite the advances in algorithms and novel frameworks such as deep learning, there has been relatively little work on systematically studying the properties of temporal network models and their cornerstones, the graph time-series representations that are used in these approaches. This paper aims to fill this gap by introducing a general framework that extends an arbitrary existing static embedding approach to handle dynamic tasks, and conducting a systematic study of seven base static embedding methods and six temporal network models. Our framework generalizes static node embeddings derived from the time-series representation of stream data to the dynamic setting by modeling the temporal dependencies with classic models such as the reachability graph. While previous works on dynamic modeling and embedding have focused on representing a stream of timestamped edges using a time-series of graphs based on a specific time-scale (eg, 1 month), we introduce the notion of an ε-graph time-series that uses a fixed number of edges for each graph, and show its superiority in practical settings over the standard solution. From the 42 methods that our framework subsumes, we find that leveraging the new ε-graph time-series representation and capturing temporal dependencies with the proposed reachability or summary graph tend to perform well. Furthermore, the new dynamic embedding methods based on our framework perform comparably and on average better than the state-of-the-art embedding methods designed specifically for temporal graphs in link prediction tasks.","New York, NY, USA",,"Jin, Di and Kim, Sungchul and Rossi, Ryan A. and Koutra, Danai",Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,10.1145/3488560.3498428,9781450391320,,,,"dynamic networks, graph time-series, representation learning","Virtual Event, AZ, USA",,,11,410–420,Association for Computing Machinery,WSDM '22,On Generalizing Static Node Embedding to Dynamic Settings,https://doi.org/10.1145/3488560.3498428,,2022
inproceedings,10.1145/3589334.3645441,"Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.","New York, NY, USA",,"Yang, Ruiyi and Salim, Flora D. and Xue, Hao",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645441,9798400701719,,,,"knowledge graph, spatio-temporal data, time series forecasting","Singapore, Singapore",,,9,551–559,Association for Computing Machinery,WWW '24,SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding,https://doi.org/10.1145/3589334.3645441,,2024
inproceedings,10.1145/3736425.3770109,"Long-term forecasting of multivariate urban data poses a significant challenge due to the complex spatiotemporal dependencies inherent in such datasets. This paper presents DST, a novel multivariate time-series forecasting model that integrates graph attention and temporal convolution within a Graph Neural Network (GNN) to effectively capture spatial and temporal dependencies, respectively. To enhance model performance, we apply a decomposition-based preprocessing step that isolates trend, seasonal, and residual components of the time series, enabling the learning of distinct graph structures for different time-series components. Extensive experiments on real-world urban datasets—including electricity demand, weather metrics, carbon intensity, and air pollution—demonstrate the effectiveness of DST across a range of forecast horizons, from several days to one month. Specifically, our approach achieves an average improvement of 2.89\% to 9.10\% in long-term forecasting accuracy over state-of-the-art time-series forecasting models.","New York, NY, USA",,"Sohrabbeig, Amirhossein and Ardakanian, Omid and Musilek, Petr","Proceedings of the 12th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",10.1145/3736425.3770109,9798400719455,,,,"multivariate time-series, long-term forecasting, spatio-temporal graph neural networks, time-series decomposition","Colorado School of Mines, Golden, CO, USA",,,10,161–170,Association for Computing Machinery,BuildSys '25,Long-Term Forecasting of Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis,https://doi.org/10.1145/3736425.3770109,,2025
inproceedings,10.1145/3501409.3501591,"Multivariate time series forecasting problems are an important part of research in various fields at all times, such as financial and stock markets, natural disasters, disease prevention. However, forecasting has always been difficult due to its own reasons or external factors. In this paper, we propose a brand-new Conv-Attention network (CANet) for harmful algal blooms prediction. To capture more spatial dimension feature information, the network extracts the context dependency from each time series, and at the same time obtains the impact score between the interacting time series. In the previous stage of training, the feature factors are acquired through different convolution kernels. Then attention mechanism is adopted to model the processes that depend on mutual influence. To further enhance the robustness of the network, the CANet incorporates simple MLP layer-assisted training. The experimental results show that our proposed network performs well under the evaluation of the performance index.","New York, NY, USA",,"Chen, Xiaoqian and Fu, Yonggang and Zhou, Honghua",Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering,10.1145/3501409.3501591,9781450384322,,,,"Multivariate time series forecasting, attention mechanism, convolution, harmful algal blooms","Xiamen, China",,,6,1018–1023,Association for Computing Machinery,EITCE '21,Conv-Attention Model Based on Multivariate Time Series Prediction: The Cyanobacteria Bloom Case,https://doi.org/10.1145/3501409.3501591,,2022
article,10.1145/3653070,"Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have not yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial domains, including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality, such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, the task-agnostic large learning models (LLMs) can outperform task-specific fully supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image–based urban noise intensity classification, and remote sensing image scene classification), existing FMs still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing an FM for GeoAI is to address the multimodal nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal FM that can reason over various types of geospatial data through geospatial alignments. We conclude this article by discussing the unique risks and challenges to developing such a model for GeoAI.","New York, NY, USA",11,"Mai, Gengchen and Huang, Weiming and Sun, Jin and Song, Suhang and Mishra, Deepak and Liu, Ninghao and Gao, Song and Liu, Tianming and Cong, Gao and Hu, Yingjie and Cundy, Chris and Li, Ziyuan and Zhu, Rui and Lao, Ni",,10.1145/3653070,,2374-0353,June 2024,ACM Trans. Spatial Algorithms Syst.,"Foundation models, geospatial artificial intelligence, multimodal learning",,,2,46,,Association for Computing Machinery,,On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper),https://doi.org/10.1145/3653070,10,2024
inproceedings,10.1145/3637528.3671657,"Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a new benchmark via: (i) achieving 25\% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18\% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23\% (100\% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.","New York, NY, USA",,"Jagirdar, Hussain and Talwadker, Rukma and Pareek, Aditya and Agrawal, Pulkit and Mukherjee, Tridib",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671657,9798400704901,,,,"explainable ai, gaming, interpretability, multivariate time series forecasting, responsible ai","Barcelona, Spain",,,12,5126–5137,Association for Computing Machinery,KDD '24,Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay,https://doi.org/10.1145/3637528.3671657,,2024
inproceedings,10.1145/3573428.3573753,"With the aggravation of global warming, drought has caused more and more serious impact on society and national economy, so the accurate prediction of drought is also the most important thing to deal with the frequent drought problem. On the other hand, artificial intelligence technology has shown its advantages in various fields and has a broad application prospects, so it is gradually being used in drought prediction. In this paper, drought prediction algorithms in recent years are reviewed, and existing cases of drought prediction using different arithmetic models for different drought indices are studied. The advantages and disadvantages of different algorithms are compared, the challenges of drought prediction are summarized, and the future of drought prediction is prospected.","New York, NY, USA",,"Liu, Mengxi and Zai, Guangjun and Wang, Guofu and Shi, Weiwei and Zhang, Guanqun",Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering,10.1145/3573428.3573753,9781450397148,,,,"Drought forecasting, Machine learning, Time series forecasting","Xiamen, China",,,6,1845–1850,Association for Computing Machinery,EITCE '22,Research on the application of artificial intelligence algorithms in drought prediction,https://doi.org/10.1145/3573428.3573753,,2023
article,10.1145/3532611,"Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph. As far as we know, we are first to employ a generation method to model fine topology of dynamic graph at each time step. Furthermore, to enhance efficiency and performance, we employ a training strategy for DGCRN by restricting the iteration number of decoder during forward and backward propagation. Finally, a reproducible standardized benchmark and a brand new representative traffic dataset are opened for fair comparison and further research. Extensive experiments on three datasets demonstrate that our model outperforms 15 baselines consistently. Source codes are available at .","New York, NY, USA",9,"Li, Fuxian and Feng, Jie and Yan, Huan and Jin, Guangyin and Yang, Fan and Sun, Funing and Jin, Depeng and Li, Yong",,10.1145/3532611,,1556-4681,January 2023,ACM Trans. Knowl. Discov. Data,"Traffic prediction, dynamic graph construction, traffic benchmark",,,1,21,,Association for Computing Machinery,,Dynamic Graph Convolutional Recurrent Network for Traffic Prediction: Benchmark and Solution,https://doi.org/10.1145/3532611,17,2023
inproceedings,10.1145/3711896.3737007,"Multivariate time series forecasting in practical deployment faces a critical challenge termed Variable Subset Forecasting (VSF), where certain variables accessible during training are entirely missing during inference. This creates a stark discrepancy between the training (source domain with full variables) and inference (target domain with partial variables) environments, disrupting cross-variable dependencies and fragmenting global temporal patterns. Existing imputation methods, limited to transferring local knowledge (e.g., temporal neighbors or pairwise correlations), fail to capture essential global dynamics, leading to severe performance degradation under distribution shifts. To address these challenges, we redefine VSF as a cross-domain knowledge transfer problem and propose VIDA, a framework that systematically transfers Variable Invariant knowledge from complete to partial observations through Domain Adaptation. Key to our approach is (1) Global time-frequency joint representation learning, which encodes temporal dynamics via dilated convolutions and captures low-frequency spectral consistency using Fourier neural operators, and (2) Sinkhorn-regularized distribution alignment to bridge non-overlapping feature supports across domains via optimal transport. Unlike imputation-first methods, VIDA enforces task-driven consistency by jointly optimizing predictions on reconstructed and original data, ensuring the transferred knowledge directly enhances forecasting robustness. Extensive experiments across four real-world datasets show that VIDA outperforms state-of-the-art imputation methods by 25\% on average with partially observed variables. This work establishes a new paradigm for variable-missing scenarios by unifying imputation and forecasting through principled knowledge transfer.","New York, NY, USA",,"Liang, Runchang and Hao, Qi and Gao, Yue and Liu, Kunpeng and Jiang, Lu and Wang, Pengyang and Yin, Minghao",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737007,9798400714542,,,,"domain adaptation, knowledge transfer, variable subset forecasting","Toronto ON, Canada",,,12,1683–1694,Association for Computing Machinery,KDD '25,Imputation via Domain Adaptation: Rethinking Variable Subset Forecasting from Knowledge Transfer,https://doi.org/10.1145/3711896.3737007,,2025
inproceedings,10.1145/3711896.3737248,"The LifeTime Value (LTV) prediction, which endeavors to forecast the cumulative purchase contribution of a user to a particular item, remains a vital challenge that advertisers are keen to resolve. A precise LTV prediction system enhances the alignment of user interests with meticulously designed advertisements, thereby generating substantial profits for advertisers. Nonetheless, this issue is complicated by the paucity of data typically observed in real-world advertising scenarios. The purchase rate among registered users is often as critically low as 0.1\%, resulting in a dataset where the majority of users make only several purchases. Consequently, there is insufficient supervisory signal for effectively training the LTV prediction model. An additional challenge emerges from the interdependencies among tasks with high correlation. It is a common practice to estimate a user's contribution to a game over a specified temporal interval. Varying the lengths of these intervals corresponds to distinct predictive tasks, which are highly correlated. For instance, predictions over a 7-day period are heavily reliant on forecasts made over a 3-day period, where exceptional cases can adversely affect the accuracy of both tasks. In order to comprehensively address the aforementioned challenges, we introduce an innovative framework denoted as Graph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph representation learning is initially employed to address the issue of data scarcity. Subsequently, Pareto-Optimization is utilized to manage the interdependence of prediction tasks. Our method is evaluated using a proprietary offline mini-game recommendation dataset in conjunction with an online A/B test. The implementation of our method results in a significant enhancement within the offline dataset. Moreover, the A/B test demonstrates encouraging outcomes, increasing average Gross Merchandise Value (GMV) by 8.4\%.","New York, NY, USA",,"Chen, Aochuan and Niu, Yifan and Gao, Ziqi and Sun, Yujie and Liu, Shoujun and Chen, Gong and Liu, Yang and Li, Jia",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737248,9798400714542,,,,"computational advertising, graph representation learning, lifetime value prediction, pareto optimization","Toronto ON, Canada",,,10,4320–4329,Association for Computing Machinery,KDD '25,Mini-Game Lifetime Value Prediction in WeChat,https://doi.org/10.1145/3711896.3737248,,2025
inproceedings,10.1145/3637528.3671621,"As the non-fungible token (NFT) market flourishes, price prediction emerges as a pivotal direction for investors gaining valuable insight to maximize returns. However, existing works suffer from a lack of practical definitions and standardized evaluations, limiting their practical application. Moreover, the influence of users' multi-behaviour transactions that are publicly accessible on NFT price is still not explored and exhibits challenges. In this paper, we address these gaps by presenting a practical and hierarchical problem definition. This approach unifies both collection-level and token-level task and evaluation methods, which cater to varied practical requirements of investors. To further understand the impact of user behaviours on the variation of NFT price, we propose a general wallet profiling framework and develop a COmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMET profiles wallets with a comprehensive view and considers the impact of diverse relations and interactions within the NFT ecosystem on NFT price variations, thereby improving prediction performance. Extensive experiments conducted in our deployed system demonstrate the superiority of COMET, underscoring its potential in the insight toolkit for NFT investors.","New York, NY, USA",,"Wang, Tianfu and Deng, Liwei and Wang, Chao and Lian, Jianxun and Yan, Yue and Yuan, Nicholas Jing and Zhang, Qi and Xiong, Hui",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671621,9798400704901,,,,"fintech, graph learning, non-fungible token, web3","Barcelona, Spain",,,12,5893–5904,Association for Computing Machinery,KDD '24,COMET: NFT Price Prediction with Wallet Profiling,https://doi.org/10.1145/3637528.3671621,,2024
inproceedings,10.1145/3589334.3645339,"Active learning (AL), that aims to label limited data samples to effectively train the model, stands as a very cost-effective data labelling strategy in machine learning. Given the state-of-the-art performance GNNs have achieved in graph-based tasks, it is critical to design proper AL methods for graph neural networks (GNNs). However, existing GNN-based AL methods require considerable supervised information to guide the AL process, such as the GNN model to use, and initially labelled nodes and labels of newly selected nodes. Such dependency on supervised information limits both flexibility and scalabilty. In this paper, we propose an unsupervised, scalable and flexible AL method - it incurs low memory footprints and time cost, is flexible to the choice of underlying GNNs, and operates without requiring GNN-model-specific knowledge or labels of selected nodes. Specifically, we leverage the commonality of existing GNNs to reformulate the unsupervised AL problem as the Aggregation Involvement Maximization (AIM) problem. The objective of AIM is to maximize the involvement or participation of all nodes during the feature aggregation process of GNNs for nodes to be labelled. In this way, the aggregated features of labelled nodes can be diversified to a large extent, thereby benefiting the training of feature transformation matrices which are major trainable components in GNNs. We prove that the AIM problem is NP-hard and propose an efficient solution with theoretical guarantees. Extensive experiments on public datasets demonstrate the effectiveness, scalability and flexibility of our method. Our study is highly relevant to the track ","New York, NY, USA",,"Huang, Shixun and Lee, Ge and Bao, Zhifeng and Pan, Shirui",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645339,9798400701719,,,,"active learning, graph neural networks","Singapore, Singapore",,,12,353–364,Association for Computing Machinery,WWW '24,Cost-effective Data Labelling for Graph Neural Networks,https://doi.org/10.1145/3589334.3645339,,2024
article,10.1145/3757922,"Many multivariate time series anomaly detection frameworks have been proposed and widely applied. However, most of these frameworks do not consider intrinsic relationships between variables in multivariate time series data, thus ignoring the causal relationship among variables and degrading anomaly detection performance. This work proposes a novel framework called CGAD, an entropy causal graph for multivariate time series Anomaly Detection. CGAD utilizes transfer entropy to construct graph structures that unveil the underlying causal relationships among time series data. Weighted graph convolutional networks combined with causal convolutions are employed to model both the causal graph structures and the temporal patterns within multivariate time series data. Furthermore, CGAD applies anomaly scoring, leveraging median absolute deviation-based normalization to improve the robustness of the anomaly identification process. Extensive experiments demonstrate that CGAD outperforms state-of-the-art methods on real-world datasets with a 9\% average improvement in terms of three different multivariate time series anomaly detection metrics.","New York, NY, USA",125,"Febrinanto, Falih Gozi and Moore, Kristen and Thapa, Chandra and Liu, Mujie and Saikrishna, Vidya and Ma, Jiangang and Xia, Feng",,10.1145/3757922,,2157-6904,December 2025,ACM Trans. Intell. Syst. Technol.,"Anomaly Detection, Multivariate Time Series, Causal Graph, Transfer Entropy, Graph Learning",,,6,25,,Association for Computing Machinery,,Entropy Causal Graphs for Multivariate Time Series Anomaly&nbsp;Detection,https://doi.org/10.1145/3757922,16,2025
inproceedings,10.1145/3696410.3714943,"Ranking is an essential and practical task on dynamic graphs, which aims to prioritize future interaction candidates for given queries. While existing solutions achieve promising ranking performance, they leverage a single listwise loss to jointly optimize candidate sets, which leads to the gradient vanishing issue; and they employ neural networks to model complex temporal structures within a shared latent space, which fails to accurately capture multi-scale temporal patterns due to the frequency aliasing issue. To address these issues, we propose BandRank, a novel and robust band-pass disentangled ranking approach for dynamic graphs in the frequency domain. Concretely, we propose a band-pass disentangled representation (BPDR) approach, which disentangles complex temporal structures into multiple frequency bands and employs non-shared frequency-enhanced multilayer perceptrons (MLPs) to model each band independently. We prove that our BPDR approach ensures effective multi-scale learning for temporal structures by demonstrating its multi-scale global convolution property. Besides, we design a robust Harmonic Ranking (HR) loss to jointly optimize candidate sets and continuously track comparisons between real and virtual candidates, where we theoretically guarantee its ability to alleviate the gradient vanishing issue. Extensive experimental results show that our BandRank achieves an average improvement of 21.31\% against eight baselines while demonstrating superior robustness across different learning scenarios.","New York, NY, USA",,"Li, Yingxuan and Xu, Yuanyuan and Lin, Xuemin and Zhang, Wenjie and Zhang, Ying",Proceedings of the ACM on Web Conference 2025,10.1145/3696410.3714943,9798400712746,,,,"disentangled representation learning, dynamic graphs, ranking","Sydney NSW, Australia",,,12,3918–3929,Association for Computing Machinery,WWW '25,Ranking on Dynamic Graphs: An Effective and Robust Band-Pass Disentangled Approach,https://doi.org/10.1145/3696410.3714943,,2025
inproceedings,10.1145/3627673.3679741,"Time series forecasting is a critical and challenging task in practical application. Recent advancements in pre-trained foundation models for time series forecasting have gained significant interest. However, current methods often overlook the multi-scale nature of time series, which is essential for accurate forecasting. To address this, we propose HiMTM, a hierarchical multi-scale masked time series modeling with self-distillation for long-term forecasting. HiMTM integrates four key components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) that directs the encoder towards feature extraction while the decoder focuses on pretext tasks; (3) hierarchical self-distillation (HSD) for multi-stage feature-level supervision signals during pre-training; and (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies between different scales for downstream tasks. These components collectively enhance multi-scale feature extraction in masked time series modeling, improving forecasting accuracy. Extensive experiments on seven mainstream datasets show that HiMTM surpasses state-of-the-art self-supervised and end-to-end learning methods by a considerable margin of 3.16-68.54\%. Additionally, HiMTM outperforms the latest robust self-supervised learning method, PatchTST, in cross-domain forecasting by a significant margin of 2.3\%. The effectiveness of HiMTM is further demonstrated through its application in natural gas demand forecasting.","New York, NY, USA",,"Zhao, Shubao and Jin, Ming and Hou, Zhaoxiang and Yang, Chengyi and Li, Zengxiang and Wen, Qingsong and Wang, Yi",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679741,9798400704369,,,,"long-term forecasting, multi-scale, self-supervised learning, time series modeling","Boise, ID, USA",,,11,3352–3362,Association for Computing Machinery,CIKM '24,HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling with Self-Distillation for Long-Term Forecasting,https://doi.org/10.1145/3627673.3679741,,2024
inproceedings,10.1145/3768292.3771256,"We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.","New York, NY, USA",,"Kong, Yaxuan and Hwang, Yoontae and Kaiser, Marcus and Vryonides, Chris and Oomen, Roel and Zohren, Stefan",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3771256,9798400722202,,,,"Multimodal fusion, Temporal alignment, Look-ahead bias mitigation, Heterogeneous data integration, Representation learning","
",,,9,683–691,Association for Computing Machinery,ICAIF '25,Fusing Narrative Semantics for Financial Volatility Forecasting,https://doi.org/10.1145/3768292.3771256,,2025
inproceedings,10.1145/3637528.3671673,"Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.","New York, NY, USA",,"Senane, Zineb and Cao, Lele and Buchner, Valentin Leonhard and Tashiro, Yusuke and You, Lei and Herman, Pawel Andrzej and Nordahl, Mats and Tu, Ruibo and von Ehrenheim, Vilhelm",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671673,9798400704901,,,,"anomaly detection, classification, clustering, diffusion model, forecasting, imputation, interpolation, multivariate time series, representation learning, self-supervised learning, time series modeling","Barcelona, Spain",,,12,2560–2571,Association for Computing Machinery,KDD '24,Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask,https://doi.org/10.1145/3637528.3671673,,2024
article,10.1145/3643855,"Many deep learning works on financial time-series forecasting focus on predicting future prices/returns of individual assets with numerical price-related information for trading, and hence propose models designed for univariate, single-task, and/or unimodal settings. Forecasting for investment and risk management involves multiple tasks in multivariate settings: forecasts of expected returns and risks of assets in portfolios, and correlations between these assets. As different sources/types of time-series influence future returns, risks, and correlations of assets in different ways, it is also important to capture time-series from different modalities. Hence, this article addresses financial time-series forecasting for investment and risk management in a multivariate, multitask, and multimodal setting. Financial time-series forecasting, however, is challenging due to the low signal-to-noise ratios typical in financial time-series, and as intra-series and inter-series relationships of assets evolve across time. To address these challenges, our proposed Temporal Implicit Multimodal Network (TIME) model learns implicit inter-series relationship networks between assets from multimodal financial time-series at multiple time-steps adaptively. TIME then uses dynamic network and temporal encoding modules to jointly capture such evolving relationships, multimodal financial time-series, and temporal representations. Our experiments show that TIME outperforms other state-of-the-art models on multiple forecasting tasks and investment and risk management applications.","New York, NY, USA",38,"Ang, Gary and Lim, Ee-Peng",,10.1145/3643855,,2157-6904,April 2024,ACM Trans. Intell. Syst. Technol.,"Time-series, forecasting, graphs, graph neural networks, finance, multi-modality",,,2,25,,Association for Computing Machinery,,Temporal Implicit Multimodal Networks for Investment and Risk Management,https://doi.org/10.1145/3643855,15,2024
inproceedings,10.1145/3533271.3561678,"We propose an architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a filtering module which filters the inverse correlation matrix into a sparse network structure. In contrast with existing sparsification methods adopted in graph neural networks, our model explicitly leverages time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales volume from a synthetic time-series sales volume dataset. The proposed spatial-temporal graph neural network displays superior performances to baseline approaches with no graphical information, fully connected, disconnected graphs, and unfiltered graphs, as well as the state-of-the-art spatial-temporal GNN. Comparison of the results with Diffusion Convolutional Recurrent Neural Network (DCRNN) suggests that, by combining a (inferior) GNN with graph sparsification and filtering, one can achieve comparable or better efficacy than the state-of-the-art in multivariate time-series regression.","New York, NY, USA",,"Wang, Yuanrong and Aste, Tomaso",Proceedings of the Third ACM International Conference on AI in Finance,10.1145/3533271.3561678,9781450393768,,,,"Attention, Complex Network, Correlation Matrix, Information Filtering Network, LSTM, Multivariate Time-series Forecasting, Sparse Graph, Spatial-temporal GNN","New York, NY, USA",,,8,463–470,Association for Computing Machinery,ICAIF '22,Network Filtering of Spatial-temporal GNN for Multivariate Time-series Prediction,https://doi.org/10.1145/3533271.3561678,,2022
inproceedings,10.1145/3746027.3755238,"Time series analysis is crucial in diverse scenarios. Beyond forecasting, considerable real-world tasks are categorized into classification, imputation, and anomaly detection, underscoring different capabilities termed time series understanding in this paper. While GPT-style models have been positioned as foundation models for time series forecasting, the BERT-style architecture, which has made significant advances in natural language understanding, has not been fully unlocked for time series understanding, possibly attributed to the undesirable dropout of essential elements of BERT. In this paper, inspired by the shared multi-granularity structure between multivariate time series and multisentence documents, we design TimesBERT to learn generic representations of time series including temporal patterns and variate-centric characteristics. In addition to a natural adaptation of masked modeling, we propose a parallel task of functional token prediction to embody vital multi-granularity structures. Our model is pre-trained on 260 billion time points across diverse domains. Leveraging multi-granularity representations, TimesBERT achieves state-of-the-art performance across four typical downstream understanding tasks, outperforming task-specific models and language pre-trained backbones, positioning it as a versatile foundation model for time series understanding.","New York, NY, USA",,"Zhang, Haoran and Liu, Yong and Qiu, Yunzhong and Liu, Haixuan and Pei, Zhongyi and Wang, Jianmin and Long, Mingsheng",Proceedings of the 33rd ACM International Conference on Multimedia,10.1145/3746027.3755238,9798400720352,,,,"foundation model, pre-training, time series analysis, understanding","Dublin, Ireland",,,9,10975–10983,Association for Computing Machinery,MM '25,TimesBERT: A BERT-Style Foundation Model for Time Series Understanding,https://doi.org/10.1145/3746027.3755238,,2025
inproceedings,10.1145/3589334.3645615,"Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40\% improvement on profit. Code is available in PyTorch1.","New York, NY, USA",,"Zhang, Wentao and Zhao, Yilei and Sun, Shuo and Ying, Jie and Xie, Yonggang and Song, Zitao and Wang, Xinrun and An, Bo",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645615,9798400701719,,,,"portfolio management, reinforcement learning, representation learning","Singapore, Singapore",,,12,187–198,Association for Computing Machinery,WWW '24,Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools,https://doi.org/10.1145/3589334.3645615,,2024
inproceedings,10.1145/3762249.3762299,"Accurate prediction of customer transaction trend can provide effective technical support for banks in managing their credit card business. Customer transaction behaviors often involve multiple types of consumption, and existing forecasting methods typically concatenate multi-type features into a unified input, which inadequately accounts for structural differences and interactions among different transaction types. To address this issue, this paper proposes TTF-GNN, a credit card transaction trend forecasting model based on graph neural networks. The proposed method effectively explores customer behavioral characteristics in multi-type transaction scenarios by integrating type-aware embedding with time-varying sensitive graph construction. It jointly captures temporal dynamics and interdependencies among multiple transaction types through spatio-temporal graph convolution, thereby enabling accurate prediction of future trends. Experimental results on three public datasets demonstrate that the proposed method significantly outperforms several exiting forecasting models across multiple evaluation metrics, confirming its effectiveness and adaptability.","New York, NY, USA",,"Wang, Xiaoguo and Zheng, Yuheng and Zhu, Hongming and Chen, Chao and Li, Huanzhang","Proceedings of the 2025 2nd International Conference on Digital Economy, Blockchain and Artificial Intelligence",10.1145/3762249.3762299,9798400713491,,,,"Credit Card Transaction Trend Forecasting, Customer Behavior Analysis, Graph Neural Network","
",,,7,326–332,Association for Computing Machinery,DEBAI '25,Credit Card Transaction Trend Forecasting Based on Graph Neural Network,https://doi.org/10.1145/3762249.3762299,,2025
inproceedings,10.1145/3604915.3608810,"Modern recommender systems usually include separate recommendation carousels such as ‘trending now’ to list trending items and further boost their popularity, thereby attracting active users. Though widely useful, such ‘trending now’ carousels typically generate item lists based on simple heuristics, e.g., the number of interactions within a time interval, and therefore still leave much room for improvement. This paper aims to systematically study this under-explored but important problem from the new perspective of time series forecasting. We first provide a set of rigorous definitions related to item trendiness and formulate the trend recommendation task as a one-step time series forecasting problem. We then propose a deep latent variable model, dubbed Trend Recommender (TrendRec), to forecast items’ future trends and generate trending item lists. Furthermore, we design associated evaluation protocols for trend recommendation. Experiments on real-world datasets from various domains show that our TrendRec significantly outperforms the baselines, verifying our model’s effectiveness.","New York, NY, USA",,"Ding, Hao and Kveton, Branislav and Ma, Yifei and Park, Youngsuk and Kini, Venkataramana and Gu, Yupeng and Divvela, Ravi and Wang, Fei and Deoras, Anoop and Wang, Hao",Proceedings of the 17th ACM Conference on Recommender Systems,10.1145/3604915.3608810,9798400702419,,,,"Bayesian Deep Learning, Recommender Systems, Trend Recommendation","Singapore, Singapore",,,12,294–305,Association for Computing Machinery,RecSys '23,Trending Now: Modeling Trend Recommendations,https://doi.org/10.1145/3604915.3608810,,2023
article,10.1145/3699767,"Self-supervised learning (SSL) has emerged as a promising alternative to purely supervised learning, since it can learn from labeled and unlabeled data using a pre-train-then-fine-tune strategy, achieving state-of-the-art performances across many research areas. The field of accelerometer-based human activity recognition (HAR) can benefit from SSL since unlabeled data can be collected cost-efficiently due to the ubiquitous nature of sensors embedded in smart devices, which is in contrast to labeled data, that require a costly annotation process. Motivated by the success of SSL and the lack of surveys on SSL for HAR, this survey comprehensively examines 52 SSL methods applied to HAR, and categorizes them into four SSL paradigms based on pre-training objectives. We discuss SSL strategies, evaluation protocols, and utilized datasets. We highlight limitations in current methodologies, including little large-scale pre-training, the absence of foundation models, as well as the scarcity of systematic domain shift experiments and domain knowledge utilization. Notably, the diversity in evaluation protocols across papers poses a considerable challenge when comparing methods. Future directions outlined in this survey include the development of an SSL framework for HAR to enable standardized benchmarking and large-scale pre-training, along with integrating domain knowledge to enhance model performance.","New York, NY, USA",149,"Logacjov, Aleksej",,10.1145/3699767,,,December 2024,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,"accelerometers, human activity recognition, representation learning, self-supervised learning",,,4,42,,Association for Computing Machinery,,Self-supervised Learning for Accelerometer-based Human Activity Recognition: A Survey,https://doi.org/10.1145/3699767,8,2024
inproceedings,10.1145/3768292.3770414,"We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel architecture for interpretable financial time-series forecasting that unifies Granger-causal hypergraph structure, Riemannian geometry, and causally masked Transformer attention. CSHT models the directional influence of financial news and sentiment on asset returns by extracting multivariate Granger-causal dependencies, which are encoded as directional hyperedges on the surface of a hypersphere. Attention is constrained via angular masks that preserve both temporal directionality and geometric consistency. Evaluated on S&amp;P 500 data from 2018 to 2023, including the 2020 COVID-19 shock, CSHT consistently outperforms baselines across return prediction, regime classification, and top-asset ranking tasks. By enforcing predictive causal structure and embedding variables in a Riemannian manifold, CSHT delivers both robust generalisation across market regimes and transparent attribution pathways from macroeconomic events to stock-level responses. These results suggest that CSHT is a principled and practical solution for trustworthy financial forecasting under uncertainty.","New York, NY, USA",,"Harit, Anoushka and Sun, Zhongtian and Yu, Jongmin",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3770414,9798400722202,,,,"Causal hypergraphs, Spherical embeddings, Transformer models, Financial time series, Interpretable machine learning, Market forecasting","
",,,9,674–682,Association for Computing Machinery,ICAIF '25,From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere,https://doi.org/10.1145/3768292.3770414,,2025
inproceedings,10.1145/3745133.3745179,"In recent years, financial time series forecasting methods based on deep neural network architectures have been proposed one after another, but there is significant heterogeneity in the performance of each model. This research systematically integrates the feature representation advantages of autoencoders (AE) and the time series modeling capabilities of bidirectional long short-term memory networks (BiLSTM) to construct a new framework for predicting stock market price trends. In terms of technical implementation, a data preprocessing mechanism based on stacked denoising autoencoders is first designed to effectively capture the non-stationary characteristics and implicit patterns of financial time series. Secondly, the bidirectional gated recurrent unit is innovatively embedded in the encoding-decoding architecture to achieve bidirectional time series dependency modeling of multi-dimensional market information, and then a parameter adaptive weighting mechanism is introduced to improve the contribution of prediction-related features through dynamic feature importance evaluation. The final prediction result is obtained through nonlinear mapping of the fully connected layer. Experimental results show that the hybrid model significantly outperforms other baseline models on two stock trading datasets.","New York, NY, USA",,"Yu, Yao and Guo, Bohan and Wang, Yuqing",Proceedings of the 2025 International Conference on Digital Economy and Information Systems,10.1145/3745133.3745179,9798400714375,,,,"Attention Mechanism, Auto-Encoder, BiLSTM, Stock Price Prediction","
",,,6,267–272,Association for Computing Machinery,DEIS '25,Stock price prediction model integrating autoencoder and bidirectional LSTM——Optimization based on attention mechanism,https://doi.org/10.1145/3745133.3745179,,2025
article,10.1145/3632403,"Sequential prediction has great value for resource allocation due to its capability in analyzing intents for next prediction. A fundamental challenge arises from real-world interaction dynamics where similar sequences involving multiple intents may exhibit different next items. More importantly, the character of volume candidate items in sequential prediction may amplify such dynamics, making deep networks hard to capture comprehensive intents. This article presents a sequential prediction framework with Decoupled Progressive Distillation (DePoD), drawing on the progressive nature of human cognition. We redefine target and non-target item distillation according to their different effects in the decoupled formulation. This can be achieved through two aspects: (1) Regarding how to learn, our target item distillation with progressive difficulty increases the contribution of low-confidence samples in the later training phase while keeping high-confidence samples in the earlier phase. And, the non-target item distillation starts from a small subset of non-target items from which size increases according to the item frequency. (2) Regarding whom to learn from, a difference evaluator is utilized to progressively select an expert that provides informative knowledge among items from the cohort of peers. Extensive experiments on four public datasets show DePoD outperforms state-of-the-art methods in terms of accuracy-based metrics.","New York, NY, USA",72,"Hu, Kaixi and Li, Lin and Xie, Qing and Liu, Jianquan and Tao, Xiaohui and Xu, Guandong",,10.1145/3632403,,1046-8188,May 2024,ACM Trans. Inf. Syst.,"Sequential prediction, representation learning, interaction dynamics, knowledge distillation",,,3,35,,Association for Computing Machinery,,Decoupled Progressive Distillation for Sequential Prediction with Interaction Dynamics,https://doi.org/10.1145/3632403,42,2023
inproceedings,10.1145/3447548.3467357,"Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, we propose a relational global model that learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, we learn a relational local model that not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5\% on average.","New York, NY, USA",,"Chen, Hongjie and Rossi, Ryan A. and Mahadik, Kanak and Kim, Sungchul and Eldardiry, Hoda",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467357,9781450383325,,,,"deep learning, graph neural networks, probabilistic model, relational time-series, time-series forecasting","Virtual Event, Singapore",,,11,106–116,Association for Computing Machinery,KDD '21,Graph Deep Factors for Forecasting with Applications to Cloud Resource Allocation,https://doi.org/10.1145/3447548.3467357,,2021
inproceedings,10.1145/3589335.3651918,"Time series forecasting holds significant value in various application scenarios. However, existing forecasting methods primarily focus on optimizing model architecture while neglecting the substantial impact of data quality on model learning. In this study, we aim to enhance model performance by optimizing data utilization based on data quality and propose a Data Quality-based Gradient Optimization (DQGO) method to facilitate training of recurrent neural networks. Firstly, we define sample quality as the matching degree between samples and model, and suggest using the attention entropy to calculate the sample quality through an attention mechanism. Secondly, we optimize the model's gradient vector by giving different weights to samples with different quality. Through experiments conducted on six datasets, the results demonstrate that DQGO significantly improves LSTM's performance. In certain cases, it even surpasses the state-of-the-art models.","New York, NY, USA",,"Huang, Feihu and Yi, Peiyu and Li, Shan and Xu, Haiwen",Companion Proceedings of the ACM Web Conference 2024,10.1145/3589335.3651918,9798400701726,,,,"data quality, gradient optimization, recurrent neural network, time series","Singapore, Singapore",,,6,1496–1501,Association for Computing Machinery,WWW '24,Data Quality-based Gradient Optimization for Recurrent Neural Networks,https://doi.org/10.1145/3589335.3651918,,2024
inproceedings,10.1145/3583780.3614702,"Accurate short-term precipitation forecast is of social and economic significance for preventing severe weather damage. Deep learning has been rapidly adopted in nowcasting based on weather radar, which plays a key role in preventing dangerous weather conditions such as torrential rainfall. However, the limited observation range of the radar imposes constraints on shorter forecast lead times. Securing a sufficient lead time for timely flood warnings and emergency responses is crucial. Here, we propose a novel GAN-based framework that combines radar and satellite data to extend forecast lead time. First, we tokenize the satellite image to align with radar dimensions and combine the satellite and radar data. We then apply positional encoding to add positional information. Second, we design the self-conditioned generator to estimate distributions of various rainfall intensities. Finally, we employ Gaussian Fourier features to map the input noise into a continuous representation. The proposed framework realistically and accurately produces time series images of various precipitation types. Furthermore, our multisource data-driven system outperforms numerical weather prediction at forecasts of up to 6 hours in South Korea.","New York, NY, USA",,"An, Sojung",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3614702,9798400701245,,,,"data fusion, generative adversarial network, precipitation forecast, remote sensing, representation learning","Birmingham, United Kingdom",,,7,4495–4501,Association for Computing Machinery,CIKM '23,Nowcast-to-Forecast: Token-Based Multiple Remote Sensing Data Fusion for Precipitation Forecast,https://doi.org/10.1145/3583780.3614702,,2023
article,10.1145/3694784,"The expansion of the Internet has resulted in a change in the flow of information. With the vast amount of digital information generated online, it is easy for users to feel overwhelmed. Finding the specific information can be a challenge, and it can be difficult to distinguish credible sources from unreliable ones. This has made recommender system (RS) an integral part of the information services framework. These systems alleviate users from information overload by analyzing users’ past preferences and directing only desirable information toward users. Traditional RSs use approaches like collaborative and content-based filtering to generate recommendations. Recently, these systems have evolved to a whole new level, intuitively optimizing recommendations using deep network models. graph neural networks (GNNs) have become one of the most widely used approaches in RSs, capturing complex relationships between users and items using graphs. In this survey, we provide a literature review of the latest research efforts done on GNN-based RSs. We present an overview of RS, discuss its generalized pipeline and evolution with changing learning approaches. Furthermore, we explore basic GNN architecture and its variants used in RSs, their applications, and some critical challenges for future research.","New York, NY, USA",9,"Anand, Vineeta and Maurya, Ashish Kumar",,10.1145/3694784,,1046-8188,January 2025,ACM Trans. Inf. Syst.,"Recommender Systems, Graph Neural Network, Collaborative Filtering, Attention Mechanisms, GNN-based Recommender Models",,,1,49,,Association for Computing Machinery,,A Survey on Recommender Systems Using Graph Neural Network,https://doi.org/10.1145/3694784,43,2024
article,10.1145/3690650,"Understanding and accurately predicting cellular traffic data is vital for communication operators and device users, as it facilitates efficient resource allocation and ensures superior service quality. However, large-scale cellular traffic data forecasting remains challenging due to intricate temporal variations and complex spatial relationships. This article proposes a Knowledge Graph Driven Decomposition Approach (KGDA) for precise cellular traffic prediction. The KGDA breaks down the impact of static environmental factors and dynamic autocorrelations of cellular traffic time series, enabling the capture of overall traffic changes and understanding of traffic dependence on past values. Specifically, we propose an urban knowledge graph to capture the static environmental context of base stations, mapping these entities into the same latent space while retaining static environmental knowledge. The cellular traffic is divided into a regular pattern and fluctuating residual components, with the KGDA comprising four modules: a Knowledge Graph Representation Learning model, a traffic regular pattern prediction module, a traffic residual dynamic prediction module, and an attentional fusion module. The first leverages graph neural networks to extract spatial contexts and predict regular patterns, the second utilizes the Bi-directional Long Short-Term Memory (Bi-LSTM) model to capture autocorrelations of traffic time series, and the final module integrates the patterns and residuals to produce the final prediction result. Comprehensive experiments demonstrate that our proposed model outperforms state-of-the-art models by more than 10\% in forecasting cellular traffic.","New York, NY, USA",123,"Gong, Jiahui and Li, Tong and Wang, Huandong and Liu, Yu and Wang, Xing and Wang, Zhendong and Deng, Chao and Feng, Junlan and Jin, Depeng and Li, Yong",,10.1145/3690650,,2157-6904,December 2024,ACM Trans. Intell. Syst. Technol.,"knowledge graph, mobile traffic prediction, graph neural networks",,,6,22,,Association for Computing Machinery,,KGDA: A Knowledge Graph Driven Decomposition Approach for Cellular Traffic Prediction,https://doi.org/10.1145/3690650,15,2024
inproceedings,10.1145/3746252.3761512,"Geocoding in India presents unique challenges due to the unstructured, multilingual and diverse nature of its address systems. While recent advances in geospatial AI have explored the combination of spatial and semantic cues, existing methods often fall short in effectively integrating both dimensions for robust address resolution. In this work, we propose GeoIndia-V2, an enhanced version of GeoIndia [21], that unifies geospatial and semantic modeling through a novel fusion framework. Our unified model combines the Graphormer architecture [27] and a Pre-trained Transformer based Language Model (PTLM) that is trained from scratch on proprietary Indian address data, using our proposed Key Modulated Cross-Attention (KMCA) mechanism. KMCA enables deep cross-modal interaction between geospatial topology and linguistic structure and allows the model to reason contextually across both geographic and textual dimention, effectively handling the semantic intricacies of Indian addresses-including colloquial usage, inconsistent formatting, and multilinguality. We leverage last-mile e-commerce delivery data to construct a fine-grained graph of neighbourhood connectivity, enabling Graphormer to capture rich spatial relationships. Unlike prior methods that rely on self-loops, we generate graphs dynamically at inference time to exploit Graphormer's topological strength. Additionally, we introduce a generative decoding strategy for predicting hierarchical H3 cells. https://www.uber.com/en-IN/blog/h3/, moving beyond conventional bit-wise classification approaches. To the best of our knowledge, this is the first method to explicitly fuse graph-based geospatial learning with language-driven semantic modeling via cross-attention in the Indian geocoding context. Our approach significantly outperforms existing solutions and marks a substantial advancement toward building scalable real-world geocoding systems for complex address ecosystems like India.","New York, NY, USA",,"Tiwari, Arpit and Singhal, Bhavuk and Aditya, Anshu and Jain, Shubham and Mukherjee, Debashis and Mukherjee, Debdoot",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761512,9798400720406,,,,"H3-cell, geocoding, graph","Seoul, Republic of Korea",,,8,6067–6074,Association for Computing Machinery,CIKM '25,GeoIndia V2: A Unified Graph and Language Model for Context-Aware Geocoding,https://doi.org/10.1145/3746252.3761512,,2025
inproceedings,10.1145/3437963.3441719,"There are many scenarios where short- and long-term causal effects of an intervention are different. For example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. This work, therefore, studies the the problem of long-term effect where the outcome of primary interest, orprimary outcome, takes months or even years to accumulate. The observational study of long-term effect presents unique challenges. First, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. Second, short-term outcomes are often directly used as the proxy of the primary outcome, i.e., thesurrogate. Nevertheless, this method entails the strong surrogacy assumption that is often impractical. To tackle these challenges, we propose to build connections between long-term causal inference and sequential models in machine learning. This enables us to learnsurrogate representations that account for thetemporal unconfoundedness and circumvent the stringent surrogacy assumption by conditioning on the inferred time-varying confounders. Experimental results show that the proposed framework outperforms the state-of-the-art.","New York, NY, USA",,"Cheng, Lu and Guo, Ruocheng and Liu, Huan",Proceedings of the 14th ACM International Conference on Web Search and Data Mining,10.1145/3437963.3441719,9781450382977,,,,"long-term effect, representation learning, sequential models, surrogates","Virtual Event, Israel",,,9,274–282,Association for Computing Machinery,WSDM '21,Long-Term Effect Estimation with Surrogate Representation,https://doi.org/10.1145/3437963.3441719,,2021
article,10.1145/3718092,"In the current ridesharing scenario, finding a compatible passenger is highly challenging and largely dependent on chance. Existing algorithms prioritize the shortest route without considering future requests or traffic conditions, which reduces the likelihood of matching with another compatible passenger. This uncertainty leads to increased congestion along shortest routes and fewer ridesharing trips overall. This article proposes a route recommendation strategy that goes beyond the shortest route, aiming to address these issues. The proposed strategy results in higher demand, reduced congestion, broader coverage of points of interest, and an increased probability of finding compatible passengers during a trip. To achieve this, we introduce a time-series forecasting method leveraging a multi-task long short-term memory model to predict demand and traffic patterns in city-zone neighborhoods. These predictions are then used to recommend optimized routes. To evaluate our approach, we tested it on three datasets containing trip and traffic details from New York City, Los Angeles, and Shenzhen. Our model demonstrated 96\% accuracy and a 2\% RMSE loss in predicting the expected number of passengers. Furthermore, during route recommendations, we observed a 23\% increase in passenger count for 97\% of trips and a reduction in travel time for the shortest path for 60\% of trips. In light of the above experimentation, we believe that while our approach recommends a longer route than the shortest one (for 40\% of cases), it helps taxi drivers find compatible passengers on most trips which increases the profit of ridesharing services, and reduces the waiting time for passengers. The source code and dataset used in the paper is available at:","New York, NY, USA",57,"Vyas, Jayant and Budhwani, Jayesh and Das, Debasis",,10.1145/3718092,,2157-6904,June 2025,ACM Trans. Intell. Syst. Technol.,"route recommendation, road-side demand, road traffic, ridesharing, intelligent transportation, neural networks",,,3,23,,Association for Computing Machinery,,PRO-MTL: Parameterized Route Optimization Using Multi-Task Learning,https://doi.org/10.1145/3718092,16,2025
article,10.1145/3690649,"In logistics service, the delivery timely rate is a key experience indicator, which is highly essential to the competitive advantage of express companies. Prediction on it enables intervention on couriers with low predicted results in advance, thus ensuring employee productivity and customer satisfaction. Currently, few related works focus on couriers’ level delivery timely rate prediction, and there are complex spatial correlations between couriers and road districts in the express scenario, which makes traditional real-time prediction approaches hard to utilize. To deal with this, we propose a deep spatial-temporal neural network, RCCNet to model spatial-temporal correlations. Specifically, we adopt Node2vec, which can encode the road network-based graph directly to capture spatial correlations between road districts. Further, we calculate couriers’ historical time-series similarity to build a graph and employ graph convolutional networks to capture the correlation between couriers. We also leverage historical sequential information with long short-term memory networks. We conduct experiments with real-world express datasets. Compared with other competitive baseline methods widely used in industry, the experiment results demonstrate its superior performance over multiple baselines.","New York, NY, USA",124,"Yi, Jinhui and Yan, Huan and Wang, Haotian and Yuan, Jian and Li, Yong",,10.1145/3690649,,2157-6904,December 2024,ACM Trans. Intell. Syst. Technol.,"Express delivery, delivery timely rate, spatial-temporal prediction",,,6,21,,Association for Computing Machinery,,RCCNet: A Spatial-Temporal Neural Network Model for Logistics Delivery Timely Rate Prediction,https://doi.org/10.1145/3690649,15,2024
inproceedings,10.1145/3511808.3557386,"The proliferation of real-time monitoring applications such as Artificial Intelligence for IT Operations (AIOps) and the Internet of Things (IoT) has led to the generation of a vast amount of time-series data. To extract the underlying value of the data, both the industry and the academia are in dire need of efficient and effective methods for time-series analysis. To this end, in this paper, we propose a Multi-layer perceptron (&lt;u&gt;M&lt;/u&gt;LP)-&lt;u&gt;a&lt;/u&gt;ttention based multivariate time-se&lt;u&gt;ri&lt;/u&gt;es a&lt;u&gt;na&lt;/u&gt;lysis model MARINA. MARINA is designed to simultaneously learn the temporal and spatial correlations among multivariate time-series. Also, the model is versatile in that it is suitable for major time-series analysis tasks such as forecasting and anomaly detection. Through extensive comparisons with the representative multivariate time-series forecasting and anomaly detection algorithms, MARINA is shown to achieve state-of-the-art (SOTA) performance in both forecasting and anomaly detection tasks.","New York, NY, USA",,"Xie, Jiandong and Cui, Yue and Huang, Feiteng and Liu, Chao and Zheng, Kai",Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management,10.1145/3511808.3557386,9781450392365,,,,"anomaly detection, forecasting, time-series analysis","Atlanta, GA, USA",,,10,2230–2239,Association for Computing Machinery,CIKM '22,MARINA: An MLP-Attention Model for Multivariate Time-Series Analysis,https://doi.org/10.1145/3511808.3557386,,2022
inproceedings,10.1145/3580305.3599440,"Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.","New York, NY, USA",,"Li, Shuangli and Zhou, Jingbo and Liu, Ji and Xu, Tong and Chen, Enhong and Xiong, Hui",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599440,9798400701030,,,,"graph neural networks, relationship inference, spatial graph","Long Beach, CA, USA",,,12,1316–1327,Association for Computing Machinery,KDD '23,Multi-Temporal Relationship Inference in Urban Areas,https://doi.org/10.1145/3580305.3599440,,2023
article,10.5555/3722577.3722866,"aeon is a unified Python 3 library for all machine learning tasks involving time series. The package contains modules for time series forecasting, classification, extrinsic regression and clustering, as well as a variety of utilities, transformations and distance measures designed for time series data. aeon also has a number of experimental modules for tasks such as anomaly detection, similarity search and segmentation. aeon follows the scikit-learn API as much as possible to help new users and enable easy integration of aeon estimators with useful tools such as model selection and pipelines. It provides a broad library of time series algorithms, including efficient implementations of the very latest advances in research. Using a system of optional dependencies, aeon integrates a wide variety of packages into a single interface while keeping the core framework with minimal dependencies. The package is distributed under the 3-Clause BSD license and is available at https://github.com/aeon-toolkit/aeon.",,289,"Middlehurst, Matthew and Ismail-Fawaz, Ali and Guillaume, Antoine and Holder, Christopher and Guijo-Rubio, David and Bulatova, Guzal and Tsaprounis, Leonidas and Mentel, Lukasz and Walter, Martin and Sch\",,,,1532-4435,January 2024,J. Mach. Learn. Res.,"Python, open source, time series, machine learning, data mining, forecasting, classification, extrinsic regression, clustering",,,1,10,,JMLR.org,,aeon: a Python toolkit for learning from time series,,25,2024
article,10.1145/3721289,"Scheduling determines the execution order and time of operations in a program. The order is related to operation dependencies, including data and resource dependencies. Data dependency is intrinsic in a program, showing operation data flow. Resource dependency is determined by scheduling methods, resolving operation resource contention. Existing scheduling methods focus on data dependency, rather than building and exploiting operation dependency graph (ODG) with extra resource dependency. As ODG contains all dependencies determining operation execution order, it provides global program information, facilitating efficient scheduling. In this work, we propose ODGS, a dependency-aware scheduling method for high-level synthesis with graph neural network (GNN) and reinforcement learning (RL). We adopt GNN to perceive accurate relations between operations. We use the relations to guide an RL agent in building a complete ODG. We perform feedback-guided iterative scheduling with ODG to converge to a high-quality solution. Experiments show that our method reduces 16.4\% latency and 26.5\% resource usage on average, compared with the latest RL-based method. Moreover, we reduce an average 2.9\% latency over the GNN-based method under the same resource usage. The same resource usage is obtained by improving the GNN-based method with manual resource constraint tuning. Without tuning, its basic version consumes an average 237.6\% more resources than our method.","New York, NY, USA",58,"Shen, Minghua and Qin, Aoxiang and Xiao, Nong",,10.1145/3721289,,1544-3566,June 2025,ACM Trans. Archit. Code Optim.,"Operation dependency graph, resource dependency, high-level synthesis",,,2,25,,Association for Computing Machinery,,ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning,https://doi.org/10.1145/3721289,22,2025
inproceedings,10.1145/3696410.3714596,"Traffic prediction plays a pivotal role in intelligent transportation systems. Most existing studies only predict traffic flow for a specific time period based on traffic data from a short period, such as an hour, overlooking the influence of periodicity present in traffic data. Moreover, most of the existing advanced methods rely on manually constructed spatio-temporal graphs for joint modeling, or use pure spatial and pure temporal modules to separately model spatial and temporal features, which limits the learning of complex spatio-temporal patterns in traffic data due to structural inadequacies in the model. To address these issues, we propose a novel approach by constructing a learnable long-range spatio-temporal graph, which can better capture complex patterns in traffic data. We introduce a new model, LLGformer, which improves upon traditional Transformer-style models, facilitating more efficient learning of traffic flow data by integrating long-range historical information. Leveraging attention mechanisms on a spatiotemporal graph enables direct interaction of information across different time slices and locations. Additionally, we propose two optimization strategies to further boost the speed of training and inference. Extensive experiments on four real-world datasets show that the new model significantly outperforms state-of-the-art methods.","New York, NY, USA",,"Jin, Di and Huo, Cuiying and Shi, Jiayi and He, Dongxiao and Wei, Jianguo and Yu, Philip S.",Proceedings of the ACM on Web Conference 2025,10.1145/3696410.3714596,9798400712746,,,,"spatio-temporal graph, traffic flow prediction, transformer","Sydney NSW, Australia",,,12,2860–2871,Association for Computing Machinery,WWW '25,LLGformer: Learnable Long-range Graph Transformer for Traffic Flow Prediction,https://doi.org/10.1145/3696410.3714596,,2025
inproceedings,10.1145/3711896.3736969,"Contrastive learning has emerged as a competent approach for unsupervised representation learning. However, the design of an optimal augmentation strategy, although crucial for contrastive learning, is less explored for time series classification tasks. Existing predefined time-domain augmentation methods are primarily adopted from vision and are not specific to time series data. Consequently, this cross-modality incompatibility may distort the semantically relevant information of time series by introducing mismatched patterns into the data. To address this limitation, we present a novel perspective from the frequency domain and identify three advantages for downstream classification: 1) the frequency component naturally encodes global features, 2) the orthogonal nature of the Fourier basis allows easier isolation and independent modifications of critical and unimportant information, and 3) a compact set of frequency components can preserve semantic integrity. To fully utilize the three properties, we propose the lightweight yet effective Frequency-Refined Augmentation (FreRA) tailored for time series contrastive learning on classification tasks, which can be seamlessly integrated with contrastive learning frameworks in a plug-and-play manner. Specifically, FreRA automatically separates critical and unimportant frequency components. Accordingly, we propose semantic-aware Identity Modification and semantic-agnostic Self-adaptive Modification to protect semantically relevant information in the critical frequency components and infuse variance into the unimportant ones respectively. Theoretically, we prove that FreRA generates semantic-preserving views. Empirically, we conduct extensive experiments on two benchmark datasets, including UCR and UEA archives, as well as five large-scale datasets on diverse applications. FreRA consistently outperforms ten leading baselines on time series classification, anomaly detection, and transfer learning tasks, demonstrating superior capabilities in contrastive representation learning and generalization in transfer learning scenarios across diverse datasets. The code is available at https://github.com/Tian0426/FreRA.","New York, NY, USA",,"Tian, Tian and Miao, Chunyan and Qian, Hangwei",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736969,9798400714542,,,,"automatic augmentation, contrastive learning, self-supervised learning, time series classification","Toronto ON, Canada",,,12,2835–2846,Association for Computing Machinery,KDD '25,FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification,https://doi.org/10.1145/3711896.3736969,,2025
inproceedings,10.1145/3637528.3671809,"The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users' subsequent movements. The main challenge is how to effectively capture users' personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model.","New York, NY, USA",,"Feng, Shanshan and Meng, Feiyu and Chen, Lisi and Shang, Shuo and Ong, Yew Soon",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671809,9798400704901,,,,"next POI recommendation, rotations, temporal information","Barcelona, Spain",,,12,759–770,Association for Computing Machinery,KDD '24,ROTAN: A Rotation-based Temporal Attention Network for Time-Specific Next POI Recommendation,https://doi.org/10.1145/3637528.3671809,,2024
article,10.5555/3546258.3546344,"Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.",,86,"Damianou, Andreas and Lawrence, Neil D. and Ek, Carl Henrik",,,,1532-4435,January 2021,J. Mach. Learn. Res.,"representation learning, factor analysis, Gaussian processes, inter-battery factor analysis",,,1,51,,JMLR.org,,Multi-view learning as a nonparametric nonlinear inter-battery factor analysis,,22,2021
article,10.1145/3463506,"Feature extraction is crucial for human activity recognition (HAR) using body-worn movement sensors. Recently, learned representations have been used successfully, offering promising alternatives to manually engineered features. Our work focuses on effective use of small amounts of labeled data and the opportunistic exploitation of unlabeled data that are straightforward to collect in mobile and ubiquitous computing scenarios. We hypothesize and demonstrate that explicitly considering the temporality of sensor data at representation level plays an important role for effective HAR in challenging scenarios. We introduce the Contrastive Predictive Coding (CPC) framework to human activity recognition, which captures the temporal structure of sensor data streams. Through a range of experimental evaluations on real-life recognition tasks, we demonstrate its effectiveness for improved HAR. CPC-based pre-training is self-supervised, and the resulting learned representations can be integrated into standard activity chains. It leads to significantly improved recognition performance when only small amounts of labeled training data are available, thereby demonstrating the practical value of our approach. Through a series of experiments, we also develop guidelines to help practitioners adapt and modify the framework towards other mobile and ubiquitous computing scenarios.","New York, NY, USA",65,"Haresamudram, Harish and Essa, Irfan and Pl\",,10.1145/3463506,,,June 2021,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,"contrastive predictive coding, human activity recognition, representation learning",,,2,26,,Association for Computing Machinery,,Contrastive Predictive Coding for Human Activity Recognition,https://doi.org/10.1145/3463506,5,2021
inproceedings,10.1145/3718751.3718862,"Financial markets are characterized by strong volatility, high noise, and intricate interdependencies among stocks, making accurate stock return prediction a challenging task. In this study, we propose a novel approach called Spatiotemporal Hypergraph Learning (STHL) for stock return sequence prediction. STHL integrates temporal characteristics and correlation-based spatial structures between stocks, enabling better prediction accuracy. We construct hypergraphs and graphs representing stock relationships using prior knowledge and data-driven methods. Hybrid graph learning techniques capture essential features, which, combined with temporal information, enhance prediction performance. The proposed multi-stock recommendation system provides investors with ranked stock recommendations, optimizing their investment decisions.","New York, NY, USA",,"Tang, Yuyin","Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management",10.1145/3718751.3718862,9798400709753,,,,"Financial time series analysis, Investment recommendations, Spatial structure modeling, Spatiotemporal hypergraph learning, Stock return prediction, Temporal feature extraction","
",,,5,694–698,Association for Computing Machinery,ICBAR '24,Spatiotemporal Hypergraph Learning for Stock Return Sequence Prediction,https://doi.org/10.1145/3718751.3718862,,2025
article,10.14778/3659437.3659458,"Prefetching is a crucial technique employed in traditional databases to enhance interactivity, particularly in the context of data exploration. Data exploration is a query processing paradigm in which users search for insights buried in the data, often not knowing what exactly they are looking for. Data exploratory tools deal with multiple challenges such as the need for interactivity with no a priori knowledge being present to help with the system tuning. The state-of-the-art prefetchers are specifically designed for navigational workloads only, where the number of possible actions is limited. The prefetchers that work with SQL-based workloads, on the other hand, mainly rely on data logical addresses rather than the data semantics. They fail to predict complex access patterns in cases where the database size is substantial, resulting in an extensive address space, or when there is frequent co-accessing of data. In this paper, we propose SeLeP, a semantic prefetcher that makes prefetching decisions for both types of workloads, based on the encoding of the data values contained inside the accessed blocks. Following the popular path of using machine learning approaches to automatically learn the hidden patterns, we formulate the prefetching task as a time-series forecasting problem and use an encoder-decoder LSTM architecture to learn the data access pattern. Our extensive experiments, across real-life exploratory workloads, demonstrate that SeLeP improves the hit ratio up to 40\% and reduces I/O time up to 45\% compared to the state-of-the-art, attaining 96\% hit ratio and 84\% I/O reduction on average.",,,"Zirak, Farzaneh and Choudhury, Farhana and Borovica-Gajic, Renata",,10.14778/3659437.3659458,,2150-8097,April 2024,Proc. VLDB Endow.,,,,8,13,2064–2076,VLDB Endowment,,SeLeP: Learning Based Semantic Prefetching for Exploratory Database Workloads,https://doi.org/10.14778/3659437.3659458,17,2024
inproceedings,10.1145/3627673.3680072,"Accurate workload forecasting is critical for efficient resource management in cloud computing systems, enabling effective scheduling and autoscaling. Despite recent advances with transformer-based forecasting models, challenges remain due to the non-stationary, nonlinear characteristics of workload time series and the long-term dependencies. In particular, inconsistent performance between long-term history and near-term forecasts hinders long-range predictions. This paper proposes a novel framework leveraging self-supervised multiscale representation learning to capture both long-term and near-term workload patterns. The long-term history is encoded through multiscale representations while the near-term observations are modeled via temporal flow fusion. These representations of different scales are fused using an attention mechanism and characterized with normalizing flows to handle non-Gaussian/non-linear distributions of time series. Extensive experiments on 9 benchmarks demonstrate superiority over existing methods.","New York, NY, USA",,"Wang, Shiyu and Chu, Zhixuan and Sun, Yinbo and Liu, Yu and Guo, Yuliang and Chen, Yang and Jian, Huiyang and Ma, Lintao and Lu, Xingyu and Zhou, Jun",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3680072,9798400704369,,,,"multiscale representation, time series, workload forecasting","Boise, ID, USA",,,9,4948–4956,Association for Computing Machinery,CIKM '24,Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting,https://doi.org/10.1145/3627673.3680072,,2024
inproceedings,10.1145/3690624.3709273,"Spatio-temporal prediction focuses on jointly modeling spatial correlations and temporal evolution and has a wide range of applications. Due to the heterogeneity of spatio-temporal data, accurate prediction relies on effectively integrating topological structures and sequential patterns. Although recurrent graph learning methods excel at capturing dynamic graph patterns, explicitly inferring future snapshots from historical dynamic graphs remains a significant challenge. Recently, prompt-based graph learning has shown the potential to improve future snapshot inference by leveraging node or task-specific prompts. However, these methods fail to fully capture edge information resulting in incomplete and less accurate representations of future snapshot structures. To bridge this gap, we propose ProST, a framework that Prompts future snapshots on dynamic graphs for Spatio-Temporal prediction, which leverages dynamic graph pre-training to generate a premise graph containing historical graph information and then employs prompts on the premise graph to infer explicit future snapshots. Specifically, this framework comprises three steps: Firstly, dynamic graph pre-training is performed using multi-granularity evolution graph convolution to obtain the premise graph with both local and global features of dynamic graphs. Secondly, prompt subgraphs are used to prompt node pairs and edge features within the premise graph. The subgraph prompt aggregation mechanism propagates this information to generate future snapshots. Finally, we freeze the parameters of the pre-trained model and update the subgraph prompt parameters using meta-learning to adapt to downstream spatio-temporal prediction tasks. Extensive experiments on real-world datasets validate that ProST achieves state-of-the-art performance.","New York, NY, USA",,"Xia, Kaiwen and Lin, Li and Wang, Shuai and Zhang, Qi and Wang, Shuai and He, Tian",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709273,9798400712456,,,,"dynamic graph, snapshot, spatio-temporal prediction, subgraph prompt","Toronto ON, Canada",,,12,1645–1656,Association for Computing Machinery,KDD '25,ProST: Prompt Future Snapshot on Dynamic Graphs for Spatio-Temporal Prediction,https://doi.org/10.1145/3690624.3709273,,2025
inproceedings,10.1145/3711896.3737186,"In logistic systems, demand prediction is an essential task providing the basis for improving the quality of terminal services, such as pick-up and delivery efficiency. However, the geographical scope of operations across multiple cities brings challenges due to the sparsity of user behavior data, hindering accurate predictions. Despite cross-city prediction methods potentially solving this problem by relying on the label of overlapping users in different cities, annotating these overlapping users is expensive. Additionally, the dynamic and diverse nature of user behaviors complicates feature transfer between cities. In this work, we define the logistics demand prediction problem as forecasting pick-up and delivery demand for zones, the smallest operational units in logistics systems, in different cities. To address the challenge, we propose TSTL, a Transferable Spatio-Temporal Learning framework for cross-city logistics prediction with sparse user data. TSTL advances existing methods from two aspects: (1) User-level invariant representation module extracts consistent user representations for overlapping and non-overlapping users across cities. (2) User-zone graph aggregation module enhances user embeddings by integrating dynamic interactions, such as logistics behaviors, into inherent user relations. Finally, the multi-city transfer module fine-tunes model parameters for city-invariant knowledge adoption and predicts future logistics demand. We implement and evaluate TSTL on one of the largest logistics systems. Extensive offline experiments and real-world deployment demonstrate the effectiveness of TSTL.","New York, NY, USA",,"Xia, Kaiwen and Lin, Li and Zhang, Xinrui and Wang, Haotian and Wang, Shuai and He, Tian",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737186,9798400714542,,,,"cross-city, logistics prediction, spatio-temporal mining, user modeling","Toronto ON, Canada",,,12,5071–5082,Association for Computing Machinery,KDD '25,A Transferable Spatio-temporal Learning Framework for Cross-city Logistics Demand Prediction,https://doi.org/10.1145/3711896.3737186,,2025
inproceedings,10.1145/3766918.3766929,"Credit card fraud is assuming growing proportions as a major threat to the financial position of American household, leading to unpredictable changes in household economic behavior. To solve this problem, in this paper, a new hybrid analysis method is presented by using the Enhanced ANFIS. The model proposes several advances of the conventional ANFIS framework and employs a multi-resolution wavelet decomposition module and a temporal attention mechanism. The model performs discrete wavelet transformations on historical transaction data and macroeconomic indicators to generate localized economic shock signals. The transformed features are then fed into a deep fuzzy rule library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian membership functions. The model proposes a temporal attention encoder that adaptively assigns weights to multi-scale economic behavior patterns, increasing the effectiveness of relevance assessment in the fuzzy inference stage and enhancing the capture of long-term temporal dependencies and anomalies caused by fraudulent activities. The proposed method differs from classical ANFIS which has fixed input–output relations since it integrates fuzzy rule activation with the wavelet basis selection and the temporal correlation weights via a modular training procedure. Experimental results show that the RMSE was reduced by 17.8\% compared with local neuro-fuzzy models and conventional LSTM models.","New York, NY, USA",,"Wang, Zhuqi and Zhang, Qinghe and Cheng, Zhuopei",Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business,10.1145/3766918.3766929,9798400716027,,,,"Adaptive Neuro-Fuzzy Inference System, American Households, Credit Card Fraud, Economic Fluctuations","
",,,5,68–72,Association for Computing Machinery,GAIB '25,Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System,https://doi.org/10.1145/3766918.3766929,,2025
article,10.1145/3432249,"Urban crime is an ongoing problem in metropolitan development and attracts general concern from the international community. As an effective means of defending urban safety, crime prediction plays a crucial role in patrol force allocation and public safety. However, urban crime data is a macro result of crime patterns overlapped by various irrelevant factors that cause inhomogeneous noises—local outliers and irregular waves. These noises might obstruct the learning process of crime prediction models and result in a deviation of performance. To tackle the problem, we propose a novel paradigm of &lt;underline&gt;Du&lt;/underline&gt;al-&lt;underline&gt;ro&lt;/underline&gt;bust Enhanced Spatial-temporal Learning &lt;underline&gt;Net&lt;/underline&gt;work&nbsp;(DuroNet), an encoder-decoder architecture that possesses an adaptive robustness for reducing the effect of outliers and waves. The robustness is mainly reflected on two aspects. One is a locality enhanced module that employs local temporal context information to smooth the deviation of outliers and dynamic spatial information to assist in understanding normal points. The other is a self-attention-based pattern representation module to weaken the effect of irregular waves by learning attentive weights. Finally, extensive experiments are conducted on two real-world crime datasets before and after adding Gaussian noises. The results demonstrate the superior performance of our DuroNet over the state-of-the-art methods.","New York, NY, USA",24,"Hu, Kaixi and Li, Lin and Liu, Jianquan and Sun, Daniel",,10.1145/3432249,,1533-5399,February 2021,ACM Trans. Internet Technol.,"Crime prediction, noise data, representation learning, robust neural network, spatial-temporal correlation",,,1,24,,Association for Computing Machinery,,DuroNet: A Dual-robust Enhanced Spatial-temporal Learning Network for Urban Crime Prediction,https://doi.org/10.1145/3432249,21,2021
inproceedings,10.1145/3690624.3709328,"Recent years have witnessed the perfect encounter of deep learning and quantitative trading has achieved great success in stock investment. Numerous deep learning-based models have been developed for forecasting stock returns, leveraging the powerful representation capabilities of neural networks to identify patterns and factors influencing stock prices. These models can effectively capture general patterns in the market, such as stock price trends, volume-price relationships, and time variations. However, the impact of special irrationality factors -- such as market sentiment, speculative behavior, market manipulation, and psychological biases -- has not been fully considered in existing deep stock forecasting models due to their relative abstraction as well as lack of explicit labels and data description. To fill this gap, we propose UMI, a Universal multi-level Market Irrationality factor model to enhance stock return forecasting. The UMI model learns factors that can reflect irrational behaviors in market from both individual stock and overall market levels. For the stock-level, UMI construct an estimated rational price for each stock, which is cointegrated with the stock's actual price. The discrepancy between the actual and the rational prices serves as a factor to indicate stock-level irrational events. Additionally, we define market-level irrational behaviors as anomalous synchronous fluctuations of stocks within a market. Using two self-supervised representation learning tasks, i.e., sub-market comparative learning and market synchronism prediction, the UMI model incorporates market-level irrationalities into a market representation vector, which is then used as the market-level irrationality factor. We also developed a forecasting model that captures both temporal and relational dependencies among stocks, accommodating the UMI factors. Extensive experiments on U.S. and Chinese stock markets with competitive baselines demonstrate our model's effectiveness and the universality of our factors in improving various forecasting models. We provide our code at https://github.com/lIcIIl/UMI.","New York, NY, USA",,"Yang, Chen and Wang, Jingyuan and Jiang, Xiaohan and Wu, Junjie",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709328,9798400712456,,,,"deep learning, market irrationality, self-supervised learning, stock return forecasting","Toronto ON, Canada",,,12,1739–1750,Association for Computing Machinery,KDD '25,Learning Universal Multi-level Market Irrationality Factors to Improve Stock Return Forecasting,https://doi.org/10.1145/3690624.3709328,,2025
article,10.1145/3568683,"Urban vibrancy describes the prosperity, diversity, and accessibility of urban areas, which is vital to a city’s socio-economic development and sustainability. While many efforts have been made for statically measuring and evaluating urban vibrancy, there are few studies on the evolutionary process of urban vibrancy, yet we know little about the relationship between urban vibrancy evolution and sophisticated spatiotemporal dynamics. In this article, we make use of multi-sourced urban data to develop a data-driven framework, U-Evolve, to investigate urban vibrancy evolution. Specifically, we first exploit the spatiotemporal characteristics of urban areas to create multi-view time-dependent graphs. Then, we analyze the contextual features and graph patterns of multi-view time-dependent graphs in terms of informing future urban vibrancy variations. Our analysis validates the informativeness of multi-view time-dependent graphs for characterizing and informing future urban vibrancy evolution. After that, we construct a feature based model to forecast future urban vibrancy evolution and quantify each feature’s importance. Moreover, to further enhance the forecasting effectiveness, we propose a graph learning based model to capture spatiotemporal autocorrelation of urban areas based on multi-view time-dependent graphs in an end-to-end manner. Finally, extensive experiments on two metropolises, Beijing and Shanghai, demonstrate the effectiveness of our forecasting models. The U-Evolve framework has also been deployed in the production environment to deliver real-world urban development and planning insights for various cities in China.","New York, NY, USA",68,"Liu, Hao and Guo, Qingyu and Zhu, Hengshu and Fu, Yanjie and Zhuang, Fuzhen and Ma, Xiaojuan and Xiong, Hui",,10.1145/3568683,,1556-4681,June 2023,ACM Trans. Knowl. Discov. Data,"Urban vibrancy forecasting, spatiotemporal data mining, graph neural network",,,5,24,,Association for Computing Machinery,,Characterizing and Forecasting Urban Vibrancy Evolution: A Multi-View Graph Mining Perspective,https://doi.org/10.1145/3568683,17,2023
inproceedings,10.1145/3770177.3770306,"This paper proposes a two-stage self-supervised pretraining modeling method for stock price sequence prediction in financial markets. The method is designed to address challenges such as limited labeled data, complex structural patterns, and non-stationary temporal features. The framework consists of two phases: pretraining and fine-tuning. In the pretraining phase, two self-supervised tasks are constructed. One captures long-term trends, while the other models short-term fluctuations. In the fine-tuning phase, the learned representations are used for regression prediction to improve the model's ability to fit future price movements. In the encoder design, the method integrates multi-layer temporal sequence modeling units. This enables multi-granularity semantic extraction and structure-aware representation learning. For the experimental part, a dataset is built based on Tesla's historical stock data from 2010 to 2024. The model is systematically evaluated under different time windows, hidden dimensions, sampling frequencies, and perturbation settings. The experimental results show that the proposed method outperforms existing baseline models across multiple metrics. It effectively captures temporal dependencies while maintaining strong prediction stability and robustness. This study validates the effectiveness of the two-stage architecture in financial time series modeling. It also demonstrates the practical potential of self-supervised learning in low-supervision financial prediction tasks.","New York, NY, USA",,"Xu, Qingqing",Proceedings of the 2025 International Conference on Economic Management and Big Data Application,10.1145/3770177.3770306,9798400720109,,,,"Stock Price Prediction, self-supervised pre-training, structure perception, time series modeling","
",,,7,778–784,Association for Computing Machinery,ICEMBDA '25,Unsupervised Temporal Encoding for Stock Price Prediction through Dual-Phase Learning,https://doi.org/10.1145/3770177.3770306,,2025
inproceedings,10.1145/3696409.3700258,"The prediction of information cascades is a crucial task in data mining, which aims to understand the patterns of information diffusion at macroscopic and microscopic levels. The objective of the macro level is to predict the popularity of information cascades in the future. However, the current macro information cascade prediction algorithms produce a fixed popularity prediction value for a specific future time, which lacks flexibility. In this paper, we propose a novel information cascade prediction algorithm, named CasInformer. This algorithm views information cascades as a sequence of cascaded graph snapshots and employs time series prediction to make inferences. CasInformer enhances the selection method of cascaded snapshots and utilizes diverse information to encode supplementary data, which significantly enhances prediction accuracy compared to existing algorithms. CasInformer can predict the cascading popularity at multiple different times to obtain the future trend of information cascading popularity, which is more practical in real scenarios. Experimental results on real datasets show that CasInformer has achieved significant improvement in both prediction accuracy and prediction ability compared to existing research.","New York, NY, USA",96,"Chen, Dongming and Nie, Mingshuo and Sun, Zhengping and Chen, Huilin and Wang, Dongqi",Proceedings of the 6th ACM International Conference on Multimedia in Asia,10.1145/3696409.3700258,9798400712739,,,,"Information cascade prediction, Dynamic graph, Time series prediction, Transformer, Position encoding","
",,,1,,Association for Computing Machinery,MMAsia '24,An Information Cascade Prediction Algorithm Based on Time Series,https://doi.org/10.1145/3696409.3700258,,2024
inproceedings,10.1145/3696410.3714931,"Multi-modal time series data is common in web technologies like the Internet of Things (IoT). Existing methods for multi-modal time series representation learning aim to disentangle the modality-shared and modality-specific latent variables. Although achieving notable performances on downstream tasks, they usually assume an orthogonal latent space. However, the modality-specific and modality-shared latent variables might be dependent on real-world scenarios. Therefore, we propose a general generation process, where the modality-shared and modality-specific latent variables are dependent, and further develop a Multi-modAl TEmporal Disentanglement (MATE) model. Specifically, our MATE model is built on a temporally variational inference architecture with the modality-shared and modality-specific prior networks for the disentanglement of latent variables. Furthermore, we establish identifiability results to show that the extracted representation is disentangled. More specifically, we first achieve the subspace identifiability for modality-shared and modality-specific latent variables by leveraging the pairing of multi-modal data. Then we establish the component-wise identifiability of modality-specific latent variables by employing sufficient changes of historical latent variables. Extensive experimental studies on 12 datasets show a general improvement in different downstream tasks, highlighting the effectiveness of our method in real-world scenarios.","New York, NY, USA",,"Cai, Ruichu and Jiang, Zhifan and Zheng, Kaitao and Li, Zijian and Chen, Weilin and Chen, Xuexin and Shen, Yifan and Chen, Guangyi and Hao, Zhifeng and Zhang, Kun",Proceedings of the ACM on Web Conference 2025,10.1145/3696410.3714931,9798400712746,,,,"multimodal time series, time series representation","Sydney NSW, Australia",,,20,3247–3266,Association for Computing Machinery,WWW '25,Learning Disentangled Representation for Multi-Modal Time-Series Sensing Signals,https://doi.org/10.1145/3696410.3714931,,2025
inproceedings,10.1145/3583780.3614759,"In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning1.","New York, NY, USA",,"Zhang, Weiqi and Zhang, Jianfeng and Li, Jia and Tsung, Fugee",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3614759,9798400701245,,,,"co-training, contrastive learning, noisy data, time series","Birmingham, United Kingdom",,,11,3308–3318,Association for Computing Machinery,CIKM '23,A Co-training Approach for Noisy Time Series Learning,https://doi.org/10.1145/3583780.3614759,,2023
inproceedings,10.1145/3637528.3671921,"Dynamic pricing, which suggests the optimal prices based on the dynamic demands, has received considerable attention in academia and industry. On online hotel booking platforms, room demand fluctuates due to various factors, notably hotel popularity and competition. In this paper, we propose a dynamic pricing approach with popularity and competitiveness-aware demand learning. Specifically, we introduce a novel demand function that incorporates popularity and competitiveness coefficients to comprehensively model the price elasticity of demand. We develop a dynamic demand prediction network that focuses on learning these coefficients in the proposed demand function, enhancing the interpretability and accuracy of price suggestion. The model is trained in a multi-task framework that effectively leverages the correlations of demands among groups of similar hotels to alleviate data sparseness in room-level occupancy prediction. Comprehensive experiments conducted on real-world datasets validate the superiority of our method over state-of-the-art baselines in both demand prediction and dynamic pricing. Our model has been successfully deployed on a popular online travel platform, serving tens of millions of users and hoteliers.","New York, NY, USA",,"Zhu, Fanwei and Xiao, Wendong and Yu, Yao and Liu, Zemin and Chen, Zulong and Cai, Weibin",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671921,9798400704901,,,,"dynamic pricing, multi-task learning, occupancy prediction","Barcelona, Spain",,,11,4641–4651,Association for Computing Machinery,KDD '24,Dynamic Hotel Pricing at Online Travel Platforms: A Popularity and Competitiveness Aware Demand Learning Approach,https://doi.org/10.1145/3637528.3671921,,2024
inproceedings,10.1145/3678717.3691237,"Graph neural networks, as well as attention mechanisms, have gained widespread popularity for traffic flow forecasting due to their capacity to incorporate the complicated interactions behind flow dynamics. However, existing solutions either formulate a graph-based skeleton with narrow (e.g., static) interaction capture or build the spatiotemporal (e.g., dynamic) attention without proper comprehension of diverse risks, which inevitably burdens the generalization of high-accuracy traffic trends. In this study, we introduce Gboot (Graph bootstrap) enhancement framework for traffic flow forecasting. Gboot takes the traffic flow forecasting problem from a dependency dynamic learning perspective by treating each traffic sensor as the graph node while regarding the observed flows at each sensor as the node feature. In addition to exposing the explicit spatial connectivity behind traffic flows, we hierarchically devise temporal-aware and factual-aware graph learning blocks to consider temporal interactive dynamics and factual interactive dynamics. The former shows the trend dependencies behind flow signals and the latter uncovers different views of traffic situations (e.g., current observation vs. historical observation). More importantly, we present a Dual-view Bootstrap (DvBoot) mechanism in Gboot, which includes both risk-free and risk-aware stands. DvBoot attempts to flexibly align these two views in the latent space to enhance the generalization capability of capturing dynamic dependencies. Experiments on several real-world traffic datasets demonstrate the superiority of our Gboot over representative approaches.","New York, NY, USA",,"Gao, Qiang and Wang, Zizheng and Huang, Li and Trajcevski, Goce and Zhang, Kunpeng and Chen, Xueqin",Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems,10.1145/3678717.3691237,9798400711077,,,,"bootstrap learning, dependency dynamics, exponential moving average, risk enhancement, traffic flow forecasting","Atlanta, GA, USA",,,13,147–159,Association for Computing Machinery,SIGSPATIAL '24,Enhancing Dependency Dynamics in Traffic Flow Forecasting via Graph Risk Bootstrap,https://doi.org/10.1145/3678717.3691237,,2024
article,10.1145/3516367,"Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a Self-supervised Transformer for Time-Series (STraTS) model, which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers, which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using time-series forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS, which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at .","New York, NY, USA",105,"Tipirneni, Sindhu and Reddy, Chandan K.",,10.1145/3516367,,1556-4681,December 2022,ACM Trans. Knowl. Discov. Data,"Time-series, neural networks, deep learning, healthcare, Transformer, self-supervised learning",,,6,17,,Association for Computing Machinery,,Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series,https://doi.org/10.1145/3516367,16,2022
article,10.1145/3712702,"Urban land use, intrinsically linked to people’s daily activities, undergoes continuous evolution, presenting a complex interplay that remains partially understood. To bridge this gap, our study leverages fine-grained human mobility data to predict these changes, adopting a novel approach that conceptualizes “community-level” land use shifts as a regression problem and represents citywide changes through dynamic graphs. We harness recent advancements in graph neural networks (GNNs), which, despite their success in various applications, face challenges in directly predicting land use changes due to the temporal mismatch between the slow evolution of urban land and the immediacy of human mobility data. Our research stands out by introducing a temporal skeleton for dynamic GNNs to synchronize human activity graphs with urban land use changes, a dynamic heterogeneous GNN approach for integrating diverse human activity data to capture essential temporal dependencies, and a novel algorithm powered by causal inference to elucidate the primary factors influencing land use predictions at the community level, all of which contribute to a training process informed by the generated causal graph. Empirically validated on three real-world datasets, our model demonstrates a performance leap over state-of-the-art baselines, marking a pivotal step toward understanding and predicting the dynamics of urban land use.","New York, NY, USA",44,"Fan, Yu and Lu, Xinjiang and Liu, Hao and Wang, Pengfei and Liu, Liang and Ma, Huadong and Zhou, Jingbo",,10.1145/3712702,,2157-6904,April 2025,ACM Trans. Intell. Syst. Technol.,"Land Use Change Prediction, Dynamic Graph, Graph Neural Networks",,,2,24,,Association for Computing Machinery,,Towards Predicting Urban Land Use Changes: A Dynamic Graph Alignment Perspective,https://doi.org/10.1145/3712702,16,2025
inproceedings,10.1145/3727353.3727401,The El Ni\~{n,"New York, NY, USA",,"Cui, Yangyang and Yin, Bo","Proceedings of the 2025 4th International Conference on Big Data, Information and Computer Network",10.1145/3727353.3727401,9798400712425,,,,"ENSO, ETSformer, GAT, LSTM, sea surface temperature","
",,,7,283–289,Association for Computing Machinery,BDICN '25,Research on Long-Term ENSO Prediction Method Based on Adaptive Spatio-Temporal Attention Network Using K-MEANS Clustering Algorithm,https://doi.org/10.1145/3727353.3727401,,2025
inproceedings,10.1145/3442381.3450116,"Many applications, e.g., healthcare, education, call for effective methods methods for constructing predictive models from high dimensional time series data where the relationship between variables can be complex and vary over time. In such settings, the underlying system undergoes a sequence of unobserved transitions among a finite set of hidden states. Furthermore, the relationships between the observed variables and their temporal dynamics may depend on the hidden state of the system. To further complicate matters, the hidden state sequences underlying the observed data from different individuals may not be aligned relative to a common frame of reference. Against this background, we consider the novel problem of jointly learning the state-dependent inter-variable relationships as well as the pattern of transitions between hidden states from multi-variate time series data. To solve this problem, we introduce the State-Regularized Vector Autoregressive Model (SrVARM) which combines a state-regularized recurrent neural network to learn the dynamics of transitions between discrete hidden states with an augmented autoregressive model which models the inter-variable dependencies in each state using a state-dependent directed acyclic graph (DAG). We propose an efficient algorithm for training SrVARM by leveraging a recently introduced reformulation of the combinatorial problem of optimizing the DAG structure with respect to a scoring function into a continuous optimization problem. We report results of extensive experiments with simulated data as well as a real-world benchmark that show that SrVARM outperforms state-of-the-art baselines in recovering the unobserved state transitions and discovering the state-dependent relationships among variables.","New York, NY, USA",,"Hsieh, Tsung-Yu and Sun, Yiwei and Tang, Xianfeng and Wang, Suhang and Honavar, Vasant G.",Proceedings of the Web Conference 2021,10.1145/3442381.3450116,9781450383127,,,,"Bayesian Networks, Deep Neural Networks, Dynamic structure learning, State space model, State-regularized vector autoregressive model","Ljubljana, Slovenia",,,11,2270–2280,Association for Computing Machinery,WWW '21,SrVARM: State Regularized Vector Autoregressive Model for Joint Learning of Hidden State Transitions and State-Dependent Inter-Variable Dependencies from Multi-variate Time Series,https://doi.org/10.1145/3442381.3450116,,2021
inproceedings,10.1145/3583780.3615492,"Collaborative Filtering (CF) has been widely applied for personalized recommendations in various industrial applications. However, due to the training strategy of Empirical Risk Minimization, CF models tend to favor popular items, resulting in inferior performance on sparse users and items. To enhance the CF representation learning of sparse users and items without sacrificing the performance of popular items, we propose a novel Popularity- aware Distributionally Robust Optimization (PDRO) framework. In particular, PDRO emphasizes the optimization of sparse users/items, while incorporating item popularity to preserve the performance of popular items through two modules. First, an implicit module develops a new popularity-aware DRO objective, paying more attention to items that will potentially become popular over time. Second, an explicit module that directly predicts the popularity of items to help the estimation of user-item matching scores. We apply PDRO to a micro-video recommendation scenario and implement it on two representative backend models. Extensive experiments on a real-world industrial dataset, as well as two public benchmark datasets, validate the efficacy of our proposed PDRO. Additionally, we perform an offline A/B test on the industrial dataset, further demonstrating the superiority of PDRO in real-world application scenarios.","New York, NY, USA",,"Zhao, Jujia and Wang, Wenjie and Lin, Xinyu and Qu, Leigang and Zhang, Jizhi and Chua, Tat-Seng",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3615492,9798400701245,,,,"distributionally robust optimization, popularity, recommendation","Birmingham, United Kingdom",,,7,4967–4973,Association for Computing Machinery,CIKM '23,Popularity-aware Distributionally Robust Optimization for Recommendation System,https://doi.org/10.1145/3583780.3615492,,2023
inproceedings,10.1145/3627673.3680086,"In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods manually build specific CL Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually requires excessive prior knowledge about the data, and massive experiments to determine the detailed CL configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns CLS for time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled search space of size over 3 \texttimes{","New York, NY, USA",,"Jing, Baoyu and Wang, Yansen and Sui, Guoxin and Hong, Jing and He, Jingrui and Yang, Yuqing and Li, Dongsheng and Ren, Kan",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3680086,9798400704369,,,,"automated machine learning, contrastive learning, time series","Boise, ID, USA",,,9,4612–4620,Association for Computing Machinery,CIKM '24,Automated Contrastive Learning Strategy Search for Time Series,https://doi.org/10.1145/3627673.3680086,,2024
inproceedings,10.1145/3701551.3703494,"Anomaly detection in high-dimensional time series data is pivotal for numerous industrial applications. Recent advances in multivariate time series anomaly detection (TSAD) have increasingly leveraged graph structures to model inter-variable relationships, typically employing Graph Neural Networks (GNNs). Despite their promising results, existing methods often rely on a single graph representation, which are insufficient for capturing the complex, diverse relationships inherent in multivariate time series. To address this, we propose the Prospective Multi-Graph Cohesion (PMGC) framework for multivariate TSAD. PMGC exploits spatial correlations by integrating a long-term static graph with a series of short-term instance-wise dynamic graphs, regulated through a graph cohesion loss function. Our theoretical analysis shows that this loss function promotes diversity among dynamic graphs while aligning them with the stable long-term relationships encapsulated by the static graph. Additionally, we introduce a ","New York, NY, USA",,"Chen, Jiazhen and Feng, Mingbin and Wirjanto, Tony S.",Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining,10.1145/3701551.3703494,9798400713293,,,,"graph neural networks, time series anomaly detection, unsupervised learning","Hannover, Germany",,,9,98–106,Association for Computing Machinery,WSDM '25,Prospective Multi-Graph Cohesion for Multivariate Time Series Anomaly Detection,https://doi.org/10.1145/3701551.3703494,,2025
inproceedings,10.1145/3690624.3709236,,"New York, NY, USA",,"Long, Qingyue and Yuan, Yuan and Li, Yong",,10.1145/3690624.3709236,9798400712456,,,,,,,,,,Association for Computing Machinery,,A Universal Model for Human Mobility Prediction,https://doi.org/10.1145/3690624.3709236,,2025
inproceedings,10.1145/3485832.3485913,"We show that knowledge of wallet addresses from the current time state of a blockchain network, such as Bitcoin, increases the performance of illicit activity detection. Based on this finding we introduce two new methods for the sampling of classifier training data so that precedence is given to transaction information from the recent past and the current time state. This sampling enables streaming classification in which a decision on the class of a transaction needs to be made based on data seen to date. Our new approach provides insight into how the dynamics of the blockchain network plays a central role in the detection of illicit transactions, and is independent of the classifier choice. Our proposed sampling methods enable graph convolution network (GCN) and random forest (RF) classifiers to better adapt to changes in the network due to significant events, such as the closure of a large ‘Darknet’ marketplace. We introduce Graphlet spectral correlation analysis for exposing the effect of such network re-organisation due to major events. Finally, based on our analysis, we propose a new two-stage random forest classifier that feeds back intermediate predictions of neighbours to improve the classification decision. Our methodology enables practical streaming classification, even in the scenario of very limited information on the feature space of each transaction.","New York, NY, USA",,"Eloul, Shaltiel and Moran, Sean J and Mendel, Jacob",Proceedings of the 37th Annual Computer Security Applications Conference,10.1145/3485832.3485913,9781450385794,,,,"Bitcoin, Blockchain, Fraud, Graph classification, Network Dynamics","Virtual Event, USA",,,12,761–772,Association for Computing Machinery,ACSAC '21,Improving Streaming Cryptocurrency Transaction Classification via Biased Sampling and Graph Feedback,https://doi.org/10.1145/3485832.3485913,,2021
inproceedings,10.1145/3447548.3467275,"Dynamic Graph Neural Networks (DGNNs) have become one of the most promising methods for traffic speed forecasting. However, when adapting DGNNs for traffic speed forecasting, existing approaches are usually built on a static adjacency matrix (no matter predefined or self-learned) to learn spatial relationships among different road segments, even if the impact of two road segments can be changeable dynamically during a day. Moreover, the future traffic speed cannot only be related with the current traffic speed, but also be affected by other factors such as traffic volumes. To this end, in this paper, we aim to explore these dynamic and multi-faceted spatio-temporal characteristics inherent in traffic data for further unleashing the power of DGNNs for better traffic speed forecasting. Specifically, we design a dynamic graph construction method to learn the time-specific spatial dependencies of road segments. Then, a dynamic graph convolution module is proposed to aggregate hidden states of neighbor nodes to focal nodes by message passing on the dynamic adjacency matrices. Moreover, a multi-faceted fusion module is provided to incorporate the auxiliary hidden states learned from traffic volumes with the primary hidden states learned from traffic speeds. Finally, experimental results on real-world data demonstrate that our method can not only achieve the state-of-the-art prediction performances, but also obtain the explicit and interpretable dynamic spatial relationships of road segments.","New York, NY, USA",,"Han, Liangzhe and Du, Bowen and Sun, Leilei and Fu, Yanjie and Lv, Yisheng and Xiong, Hui",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467275,9781450383325,,,,,"Virtual Event, Singapore",,,9,547–555,Association for Computing Machinery,KDD '21,Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed Forecasting,https://doi.org/10.1145/3447548.3467275,,2021
inproceedings,10.1145/3583780.3614671,"Prediction of couriers' delivery timely rates in advance is essential to the logistics industry, enabling companies to take preemptive measures to ensure the normal operation of delivery services. This becomes even more critical during anomaly conditions like the epidemic outbreak, during which couriers' delivery timely rate will decline markedly and fluctuates significantly. Existing studies pay less attention to the logistics scenario. Moreover, many works focusing on prediction tasks in anomaly scenarios fail to explicitly model abnormal events, e.g., treating external factors equally with other features, resulting in great information loss. Further, since some anomalous events occur infrequently, traditional data-driven methods perform poorly in these scenarios. To deal with them, we propose a deep spatial-temporal attention model, named DeepSTA. To be specific, to avoid information loss, we design an anomaly spatio-temporal learning module that employs a recurrent neural network to model incident information. Additionally, we utilize Node2vec to model correlations between road districts, and adopt graph neural networks and long short-term memory to capture the spatial-temporal dependencies of couriers. To tackle the issue of insufficient training data in abnormal circumstances, we propose an anomaly pattern attention module that adopts a memory network for couriers' anomaly feature patterns storage via attention mechanisms. The experiments on real-world logistics datasets during the COVID-19 outbreak in 2022 show the model outperforms the best baselines by 12.11\% in MAE and 13.71\% in MSE, demonstrating its superior performance over multiple competitive baselines.","New York, NY, USA",,"Yi, Jinhui and Yan, Huan and Wang, Haotian and Yuan, Jian and Li, Yong",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3614671,9798400701245,,,,"anomaly learning, delivery timely rate prediction, spatial-temporal modeling","Birmingham, United Kingdom",,,7,4916–4922,Association for Computing Machinery,CIKM '23,DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions,https://doi.org/10.1145/3583780.3614671,,2023
inproceedings,10.1145/3589334.3645378,"Urban region profiling from web-sourced data is of utmost importance for urban computing. We are witnessing a blossom of LLMs for various fields, especially in multi-modal data research such as vision-language learning, where text modality serves as a supplement for images. As textual modality has rarely been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions: i) Can text modality enhance urban region profiling? ii) and if so, in what ways and which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of text modality into urban imagery, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP ). Specifically, it first generates a detailed textual description for each satellite image by Image-to-Text LLMs. Then, the model is trained on image-text pairs, seamlessly unifying language supervision for urban visual representation learning, jointly with contrastive loss and language modeling loss. Results on urban indicator prediction in four major metropolises show its superior performance, with an average improvement of 6.1\% on R2 compared to the state-of-the-art methods. Our code and dataset are available at https://github.com/StupidBuluchacha/UrbanCLIP.","New York, NY, USA",,"Yan, Yibo and Wen, Haomin and Zhong, Siru and Chen, Wei and Chen, Haodong and Wen, Qingsong and Zimmermann, Roger and Liang, Yuxuan",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645378,9798400701719,,,,"language-image pretraining, spatio-temporal data, urban computing","Singapore, Singapore",,,12,4006–4017,Association for Computing Machinery,WWW '24,UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web,https://doi.org/10.1145/3589334.3645378,,2024
inproceedings,10.1145/3580305.3599448,"Multimodal spatiotemporal data (MST) consists of multiple simultaneous spatiotemporal modalities that interact with each other in a dynamic manner. Due to the complexity of MST and the recent desire for the explainability of artificial intelligent systems, disentangled representation learning for MST (DisentMST) has become a significant task, which aims to learn disentangled representations that can expose the underlying spatial semantics, temporal dynamic patterns, and inter-modality interaction modes of the complex MST. One limitation of existing approaches is that they might fail to tolerate the real-world incomplete MST data, where missing information might break the cross-modal spatiotemporal dynamics and bring noise and ambiguity to the learning process. Another limitation is that no existing work systematically reveals the structure of different types of disentangled information. To tackle the two limitations, we define a novel two-level hierarchically structured disentanglement task for MST, which reveals informative and structured disentangled representations for MST as well as digests the real-world MST with incompleteness. We propose a new framework, BiDisentMST, which leverages Gaussian Processes and Graph Factorization on the latent space to achieve our purposes. The experimental results demonstrate the effectiveness of our proposed framework compared with baselines with respect to disentanglement and imputation results.","New York, NY, USA",,"Chen, Jiayi and Zhang, Aidong",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599448,9798400701030,,,,"autoencoder, graph factorization, missing data, multiple modalities, spatiotemporal disentanglement","Long Beach, CA, USA",,,13,213–225,Association for Computing Machinery,KDD '23,On Hierarchical Disentanglement of Interactive Behaviors for Multimodal Spatiotemporal Data with Incompleteness,https://doi.org/10.1145/3580305.3599448,,2023
inproceedings,10.1145/3690624.3709388,"Predictive Maintenance (PDM) systems are essential for preemptive monitoring of sensor signals to detect potential machine component failures in industrial assets such as bearings in rotating machinery. Existing PDM systems face two primary challenges: 1) Irregular Signal Acquisition, where data collection from the sensors is intermittent, and 2) Signal Heterogeneity, where the full spectrum of sensor modalities is not effectively integrated. To address these challenges, we propose a Curriculum Learning Framework for Multi-Modal Predictive Maintenance - MentorPDM. MentorPDM consists of 1) a graph-augmented pretraining module that captures intrinsic and structured temporal correlations across time segments via a temporal contrastive learning objective and 2) a bi-level curriculum learning module that captures task complexities for weighing the importance of signal modalities and samples via modality and sample curricula. Empirical results from MentorPDM show promising performance with better generalizability in PDM tasks compared to existing benchmarks. The efficacy of the MentorPDM model will be further demonstrated in real industry testbeds and platforms.","New York, NY, USA",,"Zhang, Shuaicheng and Wang, Tuo and Adams, Stephen and Bhattacharya, Sanmitra and Tiyyagura, Sunil Reddy and Bowen, Edward and Veeramani, Balaji and Zhou, Dawei",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709388,9798400712456,,,,"gnn, pre-training strategies, predictive maintenance","Toronto ON, Canada",,,11,2837–2847,Association for Computing Machinery,KDD '25,MentorPDM: Learning Data-Driven Curriculum for Multi-Modal Predictive Maintenance,https://doi.org/10.1145/3690624.3709388,,2025
inproceedings,10.1145/3637528.3671929,"Anomalous node detection in a static graph faces significant challenges due to the rarity of anomalies and the substantial cost of labeling their deviant structure and attribute patterns. These challenges give rise to data-centric problems, including extremely imbalanced data distributions and intricate graph learning, which significantly impede machine learning and deep learning methods from discerning the patterns of graph anomalies with few labels. While these issues remain crucial, much of the current research focuses on addressing the induced technical challenges, treating the shortage of labeled data as a given. Distinct from previous efforts, this work focuses on tackling the data-centric problems by generating auxiliary training nodes that conform to the original graph topology and attribute distribution. We categorize this approach as data-centric, aiming to enhance existing anomaly detectors by training them on our synthetic data. However, the methods for generating nodes and the effectiveness of utilizing synthetic data for graph anomaly detection remain unexplored in the realm. To answer these questions, we thoroughly investigate the denoising diffusion model. Drawing from our observations on the diffusion process, we illuminate the shifts in graph energy distribution and establish two principles for designing denoising neural networks tailored to graph anomaly generation. From the insights, we propose a diffusion-based graph generation method to synthesize training nodes, which can be promptly integrated to work with existing anomaly detectors. The empirical results on eight widely-used datasets demonstrate our generated data can effectively enhance the nine state-of-the-art graph detectors' performance.","New York, NY, USA",,"Ma, Xiaoxiao and Li, Ruikun and Liu, Fanzhen and Ding, Kaize and Yang, Jian and Wu, Jia",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671929,9798400704901,,,,"generative graph diffusion, graph anomaly detection","Barcelona, Spain",,,12,2153–2164,Association for Computing Machinery,KDD '24,Graph Anomaly Detection with Few Labels: A Data-Centric Approach,https://doi.org/10.1145/3637528.3671929,,2024
inproceedings,10.1145/3711896.3737194,"The chemical industry is faced with the urgent challenge of effectively harnessing the vast amounts of time-series data generated by thousands of sensors, which is essential for forecasting chemical states, achieving accurate real-time control of production processes. Traditional forecasting methods suffer from high computational latency and struggle with the complexity of spatiotemporal dependencies. As a result, modeling this data becomes challenging. This paper introduces a novel approach, referred to as ASTNet, designed to address these challenges. ASTNet integrates an asynchronous spatiotemporal modeling framework that combines temporal and spatial encoders, enabling concurrent learning of temporal and spatial dependencies while reducing computational latency. Additionally, it introduces a gated graph fusion mechanism that adaptively combines static (meta) and evolving (dynamic) sensor graphs, enhancing the handling of heterogeneous sensor data and spatial correlations. Extensive experiments on three real-world chemical sensor datasets demonstrate that ASTNet outperforms SOTA methods in terms of both prediction accuracy and computational efficiency, making ASTNet successfully deployed in chemical engineering industrial scenarios.","New York, NY, USA",,"Tu, Shihao and Yang, Yang and Ding, Wenyue and Lu, Yicheng and Ren, Qingkai and Zhang, Yupeng and Zhang, Yin",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737194,9798400714542,,,,"chemical process, deep learning, forecasting, multivariate time series","Toronto ON, Canada",,,12,4913–4924,Association for Computing Machinery,KDD '25,ASTNet: Asynchronous Spatio-Temporal Network for Large-Scale Chemical Sensor Forecasting,https://doi.org/10.1145/3711896.3737194,,2025
inproceedings,10.1145/3697355.3697401,"Precise electricity consumption forecasting is pivotal in the energy schedule of new electric power systems. It is also significant for improving robustness of smart power grid. Existing multivariate time series predictions have made effective achievements in modeling sequential tendency and periodicity, but they lack of considering time series noises due to data sensing or transferring. Therefore, we focus on a robust approach to capture intricate correlations of multivariate time series data for forecasting. Specifically, we exploit gated dilated causal convolution as projection layer to capture latent semantic information from a temporal perspective. Furthermore, we combine time series decomposition and adaptive normalization to learn latent representations of each time series. Finally, we devise spatio-temporal modeling for capturing heterogeneous correlations. Extensive experiments are implemented on real-scenario public datasets. The performances show the effectiveness of proposed approach for electricity consumption forecasting.","New York, NY, USA",,"Wang, Hao and Sun, Fuyong and Si, Jinxin and Ma, Qiuzhe and Zeng, Wenjing and Zang, Xiuhuan and Cao, Junxi and Song, Shuaibing and Wang, Nan",Proceedings of the 2024 8th International Conference on Big Data and Internet of Things,10.1145/3697355.3697401,9798400717529,,,,"electricity consumption forecasting, spatio-temporal modeling, data mining, smart grid","
",,,6,276–281,Association for Computing Machinery,BDIOT '24,Robust Spatio-Temporal Graph Neural Network for Electricity Consumption Forecasting,https://doi.org/10.1145/3697355.3697401,,2024
inproceedings,10.1145/3627673.3679544,"Predicting stock price movements is a high-stakes task that demands explainability for human decision-makers. A key shortcoming in current methods is treating sub-predictions independently, without learning from accumulated experiences. We propose a novel triplet network for contrastive learning to enhance the explainability of stock movement prediction by considering instances of ","New York, NY, USA",,"Du, Kelvin and Mao, Rui and Xing, Frank and Cambria, Erik",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679544,9798400704369,,,,"AI, NLP, contrastive learning, explainability, stock price","Boise, ID, USA",,,9,529–537,Association for Computing Machinery,CIKM '24,Explainable Stock Price Movement Prediction using Contrastive Learning,https://doi.org/10.1145/3627673.3679544,,2024
inproceedings,10.1145/3736425.3771958,"Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility.1","New York, NY, USA",,"Gupta, Ragini and Raina, Naman and Chen, Bo and Chen, Li and Danilov, Claudiu and Eckhardt, Josh and Bernard, Keyshla and Nahrstedt, Klara","Proceedings of the 12th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",10.1145/3736425.3771958,9798400719455,,,,"spatio-temporal forecasting, graph neural networks, time series foundation models","Colorado School of Mines, Golden, CO, USA",,,7,400–406,Association for Computing Machinery,BuildSys '25,No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models,https://doi.org/10.1145/3736425.3771958,,2025
inproceedings,10.1145/3701716.3715214,"Designing effective models for learning time series representations is foundational for time series analysis. Many previous works explore time series representation modeling approaches and make progress in this area. Despite their effectiveness, they lack adaptive perception of local patterns in temporally dependent basic units and fail to capture the multi-scale dependency among these units. Instead of relying on prevalent methods centered around self-attention mechanisms, we propose ConvTimeNet, a hierarchical pure convolutional model designed for time series analysis. ConvTimeNet introduces a deformable patch layer that adaptively perceives local patterns of temporally dependent basic units in a data-driven manner. Based on the extracted local patterns, hierarchical pure convolutional blocks are designed to capture dependency relationships among the representations of basic units at different scales. Moreover, a large kernel mechanism is employed to ensure that convolutional blocks can be deeply stacked, thereby achieving a larger receptive field. In this way, local patterns and their multi-scale dependencies can be effectively modeled within a single model. Extensive experiments comparing a wide range of different types of models demonstrate that pure convolutional models still exhibit strong viability, effectively addressing the aforementioned two challenges and showing superior performance across multiple tasks. The code is available for reproducibility. https://github.com/Mingyue-Cheng/ConvTimeNet","New York, NY, USA",,"Cheng, Mingyue and Yang, Jiqian and Pan, Tingyue and Liu, Qi and Li, Zhi and Wang, Shijin",Companion Proceedings of the ACM on Web Conference 2025,10.1145/3701716.3715214,9798400713316,,,,"deep convolution network, time series classification","Sydney NSW, Australia",,,10,171–180,Association for Computing Machinery,WWW '25,ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis,https://doi.org/10.1145/3701716.3715214,,2025
inproceedings,10.1145/3637528.3671508,"Microservice architecture has become a driving force in enhancing the modularity and scalability of web applications, as evidenced by the Alipay platform's operational success. However, a prevalent issue within such infrastructures is the suboptimal utilization of CPU resources due to inflexible resource allocation policies. This inefficiency necessitates the development of dynamic, accurate workload prediction methods to improve resource allocation. In response to this challenge, we present STAMP, a &lt;u&gt;S&lt;/u&gt;patio &lt;u&gt;T&lt;/u&gt;emporal Gr&lt;u&gt;a&lt;/u&gt;ph Network for &lt;u&gt;M&lt;/u&gt;icroservice Workload &lt;u&gt;P&lt;/u&gt;rediction. STAMP is designed to comprehensively address the multifaceted interdependencies between microservices, the temporal variability of workloads, and the critical role of system state in resource utilization. Through a graph-based representation, STAMP effectively maps the intricate network of microservice interactions. It employs time series analysis to capture the dynamic nature of workload changes and integrates system state insights to enhance prediction accuracy. Our empirical analysis, using three distinct real-world datasets, establishes that STAMP exceeds baselines by achieving an average boost of 5.72\% in prediction precision, as measured by RMSE. Upon deployment in Alipay's microservice environment, STAMP achieves a 33.10\% reduction in resource consumption, significantly outperforming existing online methods. This research solidifies STAMP as a validated framework, offering meaningful contributions to the field of resource management in microservice architecture-based applications.","New York, NY, USA",,"Luo, Yang and Gao, Mohan and Yu, Zhemeng and Ge, Haoyuan and Gao, Xiaofeng and Cai, Tengwei and Chen, Guihai",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671508,9798400704901,,,,"microservice, spatio temporal gnn, workload prediction","Barcelona, Spain",,,11,5521–5531,Association for Computing Machinery,KDD '24,Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction,https://doi.org/10.1145/3637528.3671508,,2024
article,10.14778/3641204.3641217,"Spatio-Temporal Graph Neural Network (STGNN) has been used as a common workhorse for traffic forecasting. However, most of them require prohibitive quadratic computational complexity to capture long-range spatio-temporal dependencies, thus hindering their applications to long historical sequences on large-scale road networks in the real-world. To this end, in this paper, we propose BigST, a linear complexity spatio-temporal graph neural network, to efficiently exploit long-range spatio-temporal dependencies for large-scale traffic forecasting. Specifically, we first propose a scalable long sequence feature extractor to encode node-wise long-range inputs (e.g., thousands of time-steps in the past week) into low-dimensional representations encompassing rich temporal dynamics. The resulting representations can be pre-computed and hence significantly reduce the computational overhead for prediction. Then, we build a linearized global spatial convolution network to adaptively distill time-varying graph structures, which enables fast runtime message passing along spatial dimensions in linear complexity. We empirically evaluate our model on two large-scale real-world traffic datasets. Extensive experiments demonstrate that BigST can scale to road networks with up to one hundred thousand nodes, while significantly improving prediction accuracy and efficiency compared to state-of-the-art traffic forecasting models.",,,"Han, Jindong and Zhang, Weijia and Liu, Hao and Tao, Tao and Tan, Naiqiang and Xiong, Hui",,10.14778/3641204.3641217,,2150-8097,January 2024,Proc. VLDB Endow.,,,,5,10,1081–1090,VLDB Endowment,,BigST: Linear Complexity Spatio-Temporal Graph Neural Network for Traffic Forecasting on Large-Scale Road Networks,https://doi.org/10.14778/3641204.3641217,17,2024
inproceedings,10.1145/3768292.3770389,"Predicting stock price movements can be framed as a classification task, where the goal is to anticipate whether a stock will increase, decrease, or remain stable. Most existing approaches rely solely on the movement patterns of individual stocks or stock pairs, overlooking the more complex, higher-order connections that exist among groups of stocks. In practice, stocks are often interrelated in higher orders, for example, by belonging to the same industry sector or being jointly held within the same investment fund. To address this, we compare 4 hypergraph neural network-based approaches to make spatio-temporal predictions for stock movement prediction, which explicitly leverages these higher-order dependencies. We use two heterogeneous hypergraphs, where one hypergraph represents sector-based associations and the other one represents fund-holding relationships among stocks. In general, we found the hierarchical hypergraph attention mechanism and temporal attention to be effective in achieving better performance. A hierarchical hypergraph attention mechanism models these relationships by weighting the contributions of stock nodes, hyperedges, and even the hypergraphs themselves. Temporal attention captures time-dependent dynamics of both stock and sector sequences, effectively accounting for the influence of past states. Experiments on real-world datasets demonstrate that the methods specializing in hypergraph integration achieve superior performance compared to existing methods, both in terms of predictive accuracy and profitability.","New York, NY, USA",,"Alaygut, Tuna and Sefer, Emre",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3770389,9798400722202,,,,"Hypergraph Neural Network, Spatiotemporal Prediction, Stock Movement Prediction","
",,,9,700–708,Association for Computing Machinery,ICAIF '25,Hypergraph Neural Networks to Predict Stock Movements By Exploring Higher-order Relationships,https://doi.org/10.1145/3768292.3770389,,2025
inproceedings,10.1145/3477495.3531992,"Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER1. https://github.com/zjunlp/MKGformer.","New York, NY, USA",,"Chen, Xiang and Zhang, Ningyu and Li, Lei and Deng, Shumin and Tan, Chuanqi and Xu, Changliang and Huang, Fei and Si, Luo and Chen, Huajun",Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,10.1145/3477495.3531992,9781450387323,,,,"knowledge graph completion, multimodal, named entity recognition, relation extraction","Madrid, Spain",,,12,904–915,Association for Computing Machinery,SIGIR '22,Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion,https://doi.org/10.1145/3477495.3531992,,2022
inproceedings,10.1145/3437963.3441750,"E-commerce business is revolutionizing our shopping experiences by providing convenient and straightforward services. One of the most fundamental problems is how to balance the demand and supply in market segments to build an efficient platform. While conventional machine learning models have achieved great success on data-sufficient segments, it may fail in a large-portion of segments in E-commerce platforms, where there are not sufficient records to learn well-trained models. In this paper, we tackle this problem in the context of market segment demand prediction. The goal is to facilitate the learning process in the target segments by leveraging the learned knowledge from data-sufficient source segments. Specifically, we propose a novel algorithm, RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a meta-learning paradigm. The multi-pattern fusion network considers both local and seasonal temporal patterns for segment demand prediction. In the meta-learning paradigm, transferable knowledge is regarded as the model parameter initialization of MPFN, which are learned from diverse source segments. Furthermore, we capture the segment relations by combining data-driven segment representation and segment knowledge graph representation and tailor the segment-specific relations to customize transferable model parameter initialization. Thus, even with limited data, the target segment can quickly find the most relevant transferred knowledge and adapt to the optimal parameters. We conduct extensive experiments on two large-scale industrial datasets. The results justify that our RMLDP outperforms a set of state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a real-world E-commerce platform. The online A/B testing results further demonstrate the practicality of RMLDP.","New York, NY, USA",,"Shi, Jiatu and Yao, Huaxiu and Wu, Xian and Li, Tong and Lin, Zedong and Wang, Tengfei and Zhao, Binqiang",Proceedings of the 14th ACM International Conference on Web Search and Data Mining,10.1145/3437963.3441750,9781450382977,,,,"market segment demand prediction, periodicity, segment relation extraction","Virtual Event, Israel",,,9,220–228,Association for Computing Machinery,WSDM '21,Relation-aware Meta-learning for E-commerce Market Segment Demand Prediction with Limited Records,https://doi.org/10.1145/3437963.3441750,,2021
article,10.1145/3439346,"Crowd flow prediction is a vital problem for an intelligent transportation system construction in a smart city. It plays a crucial role in traffic management and behavioral analysis, thus it has raised great attention from many researchers. However, predicting crowd flows timely and accurately is a challenging task that is affected by many complex factors such as the dependencies of adjacent regions or recent crowd flows. Existing models mainly focus on capturing such dependencies in spatial or temporal domains and fail to model relations between crowd flows of distant regions. We notice that each region has a relatively fixed daily flow and some regions (even very far away from each other) may share similar flow patterns which show strong correlations among them. In this article, we propose a novel model named Double-Encoder which follows a general encoder–decoder framework for multi-step citywide crowd flow prediction. The model consists of two encoder modules named ST-Encoder and FR-Encoder to model spatial-temporal dependencies and daily flow correlations, respectively. We conduct extensive experiments on two real-world datasets to evaluate the performance of the proposed model and show that our model consistently outperforms state-of-the-art methods.","New York, NY, USA",58,"Zang, Tianzi and Zhu, Yanmin and Xu, Yanan and Yu, Jiadi",,10.1145/3439346,,1556-4681,August 2021,ACM Trans. Knowl. Discov. Data,"CNN, ConvLSTM, Multi-step crowd flow prediction, encoder-decoder framework, latent representation",,,4,20,,Association for Computing Machinery,,Jointly Modeling Spatio–Temporal Dependencies and Daily Flow Correlations for Crowd Flow Prediction,https://doi.org/10.1145/3439346,15,2021
inproceedings,10.1145/3637528.3672030,"Recently, integrated warehouse and distribution logistics systems are widely used in E-commerce industries to adjust to constantly changing customer demands. It makes the prediction of purchase demand and delivery supply capacity a crucial problem to streamline operations and improve efficiency. The interaction between such demand and supply not only relies on their economic relationships but also on consumer psychology caused by daily events, such as epidemics, promotions, and festivals. Although existing studies have made great efforts in the joint prediction of demand and supply considering modeling the demand-supply interactions, they seldom refer to the impacts of diverse events. In this work, we propose MulSTE, a Multi-view Spatio-Temporal learning framework with heterogeneous Event fusion. Firstly, an Event Fusion Representation (EFR) module is designed to fuse the textual, numerical, and categorical heterogeneous information for emergent and periodic events. Secondly, a Multi-graph Adaptive Convolution Recurrent Network (MGACRN) is developed as the spatio-temporal encoder (ST-Encoder) to capture the evolutional features of demand, supply, and events. Thirdly, the Event Gated Demand-Supply Interaction Attention (EGIA) module is designed to model the demand-supply interactions during events. The evaluations are conducted on two real-world datasets collected from JD Logistics and public websites. The experimental results show that our method outperforms state-of-the-art baselines in various metrics.","New York, NY, USA",,"Lin, Li and Lu, Zhiqiang and Wang, Shuai and Liu, Yunhuai and Hong, Zhiqing and Wang, Haotian and Wang, Shuai",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3672030,9798400704901,,,,"demand-supply prediction, event representation, graph neural network, spatio-temporal graphs","Barcelona, Spain",,,12,1781–1792,Association for Computing Machinery,KDD '24,MulSTE: A Multi-view Spatio-temporal Learning Framework with Heterogeneous Event Fusion for Demand-supply Prediction,https://doi.org/10.1145/3637528.3672030,,2024
inproceedings,10.1145/3557995.3566122,"Time series prediction models have played a vital role in guiding effective policymaking and response during the COVID-19 pandemic by predicting future cases and deaths at the country, state, and county levels. However, for emerging diseases, there is not sufficient historic data to fit traditional supervised prediction models. In addition, such models do not consider human mobility between regions. To mitigate the need for supervised models and to include human mobility data in the prediction, we propose Spatial Probabilistic Contrastive Predictive Coding (SP-CPC) which leverages Contrastive Predictive Coding (CPC), an unsupervised time-series representation learning approach. We augment CPC to incorporate a covariate mobility matrix into the loss function, representing the relative number of individuals traveling between each county on a given day. The proposal distribution learned by the algorithm is then sampled by the Metropolis-Hastings algorithm to give a final prediction of the number of COVID-19 cases. We find that the model applied to COVID-19 data can make accurate short-term predictions, more accurate than ARIMA and simple time-series extrapolation methods, one day into the future. However, for longer-term prediction windows of seven or more days into the future, we find that our predictions are not as competitive and require future research.","New York, NY, USA",,"Susarla, Anish and Liu, Austin and Thai, Duy Hoang and Le, Minh Tri and Z\",Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Spatial Computing for Epidemiology,10.1145/3557995.3566122,9781450395434,,,,"COVID-19, contrastive predictive coding, metropolis-hastings, mobility data, spatiotemporal prediction","Seattle, Washington",,,9,26–34,Association for Computing Machinery,SpatialEpi '22,Spatiotemporal disease case prediction using contrastive predictive coding,https://doi.org/10.1145/3557995.3566122,,2022
inproceedings,10.1145/3583780.3614868,"Traffic forecasting is an essential problem in urban planning and computing. The complex dynamic spatial-temporal dependencies among traffic objects (e.g., sensors and road segments) have been calling for highly flexible models; unfortunately, sophisticated models may suffer from poor robustness especially in capturing the trend of the time series (1st-order derivatives with time), leading to unrealistic forecasts. To address the challenge of balancing dynamics and robustness, we propose TrendGCN, a new scheme that extends the flexibility of GCNs and the distribution-preserving capacity of generative and adversarial loss for handling sequential data with inherent statistical correlations. On the one hand, our model simultaneously incorporates spatial (node-wise) embeddings and temporal (time-wise) embeddings to account for heterogeneous space-and-time convolutions; on the other hand, it uses GAN structure to systematically evaluate statistical consistencies between the real and the predicted time series in terms of both the temporal trending and the complex spatial-temporal dependencies. Compared with traditional approaches that handle step-wise predictive errors independently, our approach can produce more realistic and robust forecasts. Experiments on six benchmark traffic forecasting datasets and theoretical analysis both demonstrate the superiority and the state-of-the-art performance of TrendGCN. Source code is available at https://github.com/juyongjiang/TrendGCN.","New York, NY, USA",,"Jiang, Juyong and Wu, Binqing and Chen, Ling and Zhang, Kai and Kim, Sunghun",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3614868,9798400701245,,,,"robustness, spatial-temporal embeddings, traffic forecasting","Birmingham, United Kingdom",,,10,987–996,Association for Computing Machinery,CIKM '23,Enhancing the Robustness via Adversarial Learning and Joint Spatial-Temporal Embeddings in Traffic Forecasting,https://doi.org/10.1145/3583780.3614868,,2023
inproceedings,10.1145/3736425.3770103,"Accurate energy load forecasting is essential for optimising power systems across buildings, cities, and smart grids. Recently, large language models (LLMs) have shown remarkable capability in capturing complex temporal patterns in energy consumption data, outperforming both traditional and deep learning techniques. However, their reliance on detailed smart meter (SM) data poses significant privacy risks, as such fine-grained information is susceptible to inference attacks. To overcome these challenges, we introduce Privacy-Preserving Time-LLM, an innovative forecasting framework that combines LLM architectures with SM data encoded via Differentially Private Bloom Filters (DP-BF). This encoding safe-guards sensitive consumption data while preserving high predictive performance. Designed for secure cloud deployment, the framework reduces privacy risks associated with honest-but-curious service providers. It employs Low-Rank Adaptation (LoRA) for efficient fine-tuning and utilises Rotary Position Embedding (RoPE) to model temporal dependencies without accessing raw time-series inputs. We benchmark our approach against the widely used differentially private training method DP-SGD. Experimental results demonstrate that the Time-LLM trained on DP-BF-Encoded SM data consistently outperforms its DP-SGD counterpart, reducing forecasting error by approximately 29\% on average, highlighting an improved balance between privacy and utility. Compared to a state-of-the-art CNN baseline, our method achieves nearly 52\% better forecasting accuracy on DP-BF-Encoded data while maintaining up to 99\% membership privacy. Moreover, under adversarial attacks, models trained with DP-BF-Encoding show over 80\% reduced vulnerability relative to models trained on raw data, significantly enhancing robustness and stability. To the best of our knowledge, this is the first differentially private LLM-based framework for energy load forecasting using DP-BF-Encoding. It opens new possibilities for privacy-preserving analytics in smart grid environments, with extensibility to other time-series applications such as occupancy detection and demand disaggregation.","New York, NY, USA",,"Zaman, Zakia and Gauravaram, Praveen and Jha, Sanjay and Hu, Wen","Proceedings of the 12th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",10.1145/3736425.3770103,9798400719455,,,,"LLM, load forecast, privacy","Colorado School of Mines, Golden, CO, USA",,,11,128–138,Association for Computing Machinery,BuildSys '25,Bloom-LLM: Privacy-Preserving Large Language Model for Load Forecasting,https://doi.org/10.1145/3736425.3770103,,2025
inproceedings,10.1145/3711896.3736980,"Variable Subset Forecasting (VSF) poses critical challenges in time series analysis when entire variables become unavailable during inference. Existing imputation methods relying on inter-variable correlations fail catastrophically in VSF due to two inherent limitations: (1) Missing variable collapse, where the complete absence of certain variables invalidates correlation-based dependency learning, and (2) Temporal covariate shift, where time-evolving data distributions destabilize correlation patterns learned from training data. To address these fundamental issues, we propose Generative Imputation with Multi-level Causal Consistency (GIMCC ), establishing causality-driven imputation as the first principled solution for VSF. Our key innovation lies in enforcing causal invariance through dual consistency constraints: global causal isomorphism ensures the imputed variables preserve the ground-truth causal graph structure of the complete system, while local causal subgraph alignment maintains consistency between observed variables and their causal neighborhood dependencies. By decoupling causality from spurious correlations, GIMCC provides time-invariant imputation signals robust to distribution shifts, which explicitly preserves causal relationships via multivariate spectral convolutions. Extensive experiments across five real-world domains demonstrate that GIMCC achieves average improvements of 20-60\% in MAE/RMSE over correlation-based imputation baselines, remarkably outperforming full-variable training ( Oracle ) in temporal covariate shift scenarios. Our work bridges the critical gap between causal analysis and practical forecasting systems under variable absence, offering theoretically grounded guarantees for real-world deployment.","New York, NY, USA",,"Hao, Qi and Gao, Yue and Liang, Runchang and Zhang, Yunhe and Wang, Pengyang",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736980,9798400714542,,,,"granger causality, variable subset forecasting","Toronto ON, Canada",,,12,838–849,Association for Computing Machinery,KDD '25,Generative Imputation with Multi-level Causal Consistency for Variable Subset Forecasting,https://doi.org/10.1145/3711896.3736980,,2025
inproceedings,10.1145/3711896.3737252,"Nuclear radiation, which refers to the energy emitted from atomic nuclei during decay, poses significant risks to human health and environmental safety. Recently, advancements in monitoring technology have facilitated the effective recording of nuclear radiation levels and related factors, such as weather conditions. The abundance of monitoring data enables the development of accurate and reliable nuclear radiation forecasting models, which play a crucial role in informing decision-making for individuals and governments. However, this task is challenging due to the imbalanced distribution of monitoring stations over a wide spatial range and the non-stationary radiation variation patterns. In this study, we introduce NRFormer, a novel framework tailored for the nationwide prediction of nuclear radiation variations. By integrating a non-stationary temporal attention module, an imbalance-aware spatial attention module, and a radiation propagation prompting module, NRFormer collectively captures complex spatio-temporal dynamics of nuclear radiation. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed framework against 11 baselines. NRFormer has been deployed online to provide 1-24-day nuclear radiation forecasts, empowering individuals and governments with timely, data-driven decisions for emergency response and public safety. Our framework is designed for general applicability and can be readily adapted for deployment in other regions. The deployed system is available at https://NRFormer.github.io and the dataset and code of the predictive model are available at https://github.com/usail-hkust/NRFormer.","New York, NY, USA",,"Lyu, Tengfei and Han, Jindong and Liu, Hao",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737252,9798400714542,,,,"imbalance-aware transformer, radiation forecasting system, spatio-temporal modeling","Toronto ON, Canada",,,12,4705–4716,Association for Computing Machinery,KDD '25,NRFormer: Nationwide Nuclear Radiation Forecasting with Spatio-Temporal Transformer,https://doi.org/10.1145/3711896.3737252,,2025
inproceedings,10.1145/3746252.3761048,"As a core task in Intelligent Transportation Systems (ITS), traffic flow prediction is essential for resource allocation and real-time route planning. Effectively capturing complex temporal correlations and dynamic spatial dependencies in traffic flow data is critical yet challenging for accurate prediction. However, existing approaches are still limited by the insufficient capability for spatial-temporal pattern decoupling and the underutilization of frequency domain information. To address these issues, we propose a novel Frequency-Enhanced Dynamic Decoupling Graph Convolutional Network (FEDDGCN), which introduces a gated decoupling mechanism integrating temporal and spatial embeddings to decouple traffic flow into prominent periodic and perturbative component. It also achieves effective pattern separation by incorporating frequency domain analysis with Fourier filters. Furthermore, a dual-branch spatial-temporal learning module, employing a divide-and-conquer strategy, is designed to achieve separate modeling for the two distinct components. Specially, the dynamic graph convolution modules are utilized to learn spatial dependencies and temporal and frequency attention mechanisms further capture complex temporal correlations for prominent periodic and perturbative components.Extensive experiments on multiple real-world datasets demonstrate that FEDDGCN achieves superior predictive performance compared with state-of-the-art methods.","New York, NY, USA",,"Zhang, Wendong and Xiang, Ruobai and Liao, Zhifang and Lan, Peng and Liang, Qihao",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761048,9798400720406,,,,"decoupling methods, fourier transform, graph convolution network, traffic flow prediction","Seoul, Republic of Korea",,,10,4222–4231,Association for Computing Machinery,CIKM '25,FEDDGCN: A Frequency-Enhanced Decoupling Dynamic Graph Convolutional Network for Traffic Flow Prediction,https://doi.org/10.1145/3746252.3761048,,2025
inproceedings,10.1145/3637528.3671709,"In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the model performance. Finally, we propose Disentangled Spatial-Temporal Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal understanding abilities. Our main observations are: 1) LLMs have preliminary spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph tasks show increasing difficulties for LLMs as the graph size and density increase, while not sensitive to the time span and data generation mechanism, 3) the proposed DST2 prompting method can help to improve LLMs' spatial-temporal understanding abilities on dynamic graphs for most tasks. The data and codes are publicly available at Github.","New York, NY, USA",,"Zhang, Zeyang and Wang, Xin and Zhang, Ziwei and Li, Haoyang and Qin, Yijian and Zhu, Wenwu",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671709,9798400704901,,,,"benchmark, disentanglement, dynamic graph, evaluation, large language model, spatial-temporal","Barcelona, Spain",,,12,4350–4361,Association for Computing Machinery,KDD '24,LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?,https://doi.org/10.1145/3637528.3671709,,2024
inproceedings,10.1145/3711896.3736856,"Understanding human mobility patterns is a complex challenge that requires modeling both node-oriented time series (e.g., population) and edge-oriented time series (e.g., population flows) within graph topologies across time. While previous methods have focused on either node-oriented time series or interactions, the synergistic integration of these two modalities has proven difficult to achieve. In this paper, we propose BINTS (BI-modal learning for Networked Time Series), a novel bi-modal learning framework that employs soft contrastive learning along the temporal axis. BINTS captures modality similarities and temporal patterns by simultaneously learning from evolving node-oriented time series and interactions, solving the limitations of single-modality approaches. To evaluate our method, we curate comprehensive multi-modal human mobility datasets spanning diverse locations and times. Our experimental results demonstrate that BINTS significantly outperforms existing forecasting models by capturing synergies across different data modalities. Overall, we establish BINTS as a powerful technique for holistically understanding and forecasting complex mobility dynamics. For reproducibility, the source code of our framework is available at https://github.com/kaist-dmlab/BINTS.","New York, NY, USA",,"Nam, Youngeun and Na, Jihye and Yoon, Susik and Song, Hwanjun and Lee, Jae-Gil and Lee, Byung Suk",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736856,9798400714542,,,,"bi-modality, contrastive learning, forecasting, time series","Toronto ON, Canada",,,12,2162–2173,Association for Computing Machinery,KDD '25,Bi-Modal Learning for Networked Time Series,https://doi.org/10.1145/3711896.3736856,,2025
inproceedings,10.1145/3746252.3761239,"The financial market plays a crucial role in the modern economy by influencing capital allocation, corporate valuation, and investor behavior. However, its complex dependencies and non-stationary dynamics present significant challenges for financial stock prediction. Previous predictive approaches are typically categorized into Univariate Time Series (UTS) and Multivariate Time Series (MTS) paradigms. UTS methods overlook both cross-feature and cross-stock influences, while MTS methods can only capture one of these simultaneously. Although some recent approaches claim to model 3D Multivariate Time Series (3D-MTS) dependencies, they often discard substantial information and fail to capture the dynamics of the stock market. To address these limitations, we propose FinD3, a Financial 3D model using Dual cubic state spaces and Dynamic hypergraphs. To extract the inherent complex relationships in 3D-MTS, we propose a novel Dual Cubic State Space Model (DCSSM) to capture both cross-feature and cross-stock patterns. Furthermore, to more accurately reflect the dynamics of the stock market, we present an Evolving Hypergraph Attention (EHA) module, which captures dynamic changes in financial markets and updates the hypergraph based on a priori hypergraph. Experimental results demonstrate that FinD3 achieves state-of-the-art performance in quantitative trading performance on two real-world stock market datasets, offering a promising solution to practical quantitative trading challenges. The code is available at: https://github.com/decisionintelligence/FinD3.","New York, NY, USA",,"Mei, Jieyuan and Tian, Jindong and Xu, Ronghui and Wei, Hanyue and Guo, Chenjuan and Yang, Bin",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761239,9798400720406,,,,"hypergraph attention, state space model, stock market, time series","Seoul, Republic of Korea",,,11,2084–2094,Association for Computing Machinery,CIKM '25,FinD3: A Dual 3D State Space Model with Dynamic Hypergraph for Financial Stock Prediction,https://doi.org/10.1145/3746252.3761239,,2025
inproceedings,10.1145/3701716.3717376,"Unsupervised methods, particularly reconstruction-based methods have become the dominant approach for multivariate time series anomaly detection (TSAD), which distinguish between normal and abnormal series based on the magnitude of the reconstruction error. However, in this process, the heterophilic connections (normal \l{","New York, NY, USA",,"Lan, Disen and Zhang, Guibin and Guo, Rongjin",Companion Proceedings of the ACM on Web Conference 2025,10.1145/3701716.3717376,9798400713316,,,,"diffusion model, graph neural networks, time series anomaly detection","Sydney NSW, Australia",,,8,2207–2214,Association for Computing Machinery,WWW '25,Diffusion Graph Model for Time Series Anomaly Detection via Anomaly-aware Graph Sparsification and Augmentation,https://doi.org/10.1145/3701716.3717376,,2025
inproceedings,10.1145/3534678.3539329,"Time-series data contains temporal order information that can guide representation learning for predictive end tasks (e.g., classification, regression). Recently, there are some attempts to leverage such order information to first pre-train time-series models by reconstructing time-series values of randomly masked time segments, followed by an end-task fine-tuning on the same dataset, demonstrating improved end-task performance. However, this learning paradigm decouples data reconstruction from the end task. We argue that the representations learnt in this way are not informed by the end task and may, therefore, be sub-optimal for the end-task performance. In fact, the importance of different timestamps can vary significantly in different end tasks. We believe that representations learnt by reconstructing important timestamps would be a better strategy for improving end-task performance. In this work, we propose TARNet, Task-Aware Reconstruction Network, a new model using Transformers to learn task-aware data reconstruction that augments end-task performance. Specifically, we design a data-driven masking strategy that uses self-attention score distribution from end-task training to sample timestamps deemed important by the end task. Then, we mask out data at those timestamps and reconstruct them, thereby making the reconstruction task-aware. This reconstruction task is trained alternately with the end task at every epoch, sharing parameters in a single model, allowing the representation learnt through reconstruction to improve end-task performance. Extensive experiments on tens of classification and regression datasets show that TARNet significantly outperforms state-of-the-art baseline models across all evaluation metrics.","New York, NY, USA",,"Chowdhury, Ranak Roy and Zhang, Xiyuan and Shang, Jingbo and Gupta, Rajesh K. and Hong, Dezhi",Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3534678.3539329,9781450393850,,,,"data reconstruction, self-attention, self-supervision, time series","Washington DC, USA",,,9,212–220,Association for Computing Machinery,KDD '22,TARNet: Task-Aware Reconstruction for Time-Series Transformer,https://doi.org/10.1145/3534678.3539329,,2022
inbook,10.1145/3729706.3729772,"To address the challenges posed by the intricate dynamic spatiotemporal dependencies within traffic networks, and the nonlinear nature of traffic data, we propose a traffic flow prediction model that uses an attention mechanism and temporal graph convolutional network (AMTGCN). First, an encoder-decoder architecture and a Graph Convolutional Recurrent Unit (NTGCCU) have been used to capture spatiotemporal dependencies effectively. Then, an attention mechanism module is employed to enhance the prioritization and emphasis on critical spatiotemporal characteristics. Finally, a memory network module has been added to discern abrupt changes in traffic conditions. To validate the efficacy and practicality of the proposed model, experimental analyses are conducted on real-world datasets. Experimental results show that the proposed method exhibits a higher prediction accuracy than current mainstream traffic forecasting models.","New York, NY, USA",,"Feng, Mengdie and Chen, Xinying","Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy",,9798400712715,,,,,,,,7,418–424,Association for Computing Machinery,,Traffic Flow Prediction in Attention Mechanism and Temporal Graph Convolutional Networks,https://doi.org/10.1145/3729706.3729772,,2025
inproceedings,10.1145/3582515.3609561,"Sea surface temperature (SST) is uniquely important to the Earth’s atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. While such techniques achieve certain levels of success, they often have significant limitations in exploring dynamic spatio-temporal dependencies between signals. To solve this problem, this paper proposes a novel graph convolution network architecture with static and dynamic learning layers for SST forecasting. Specifically, two adaptive adjacency matrices are firstly constructed to respectively model the stable long-term and short-term evolutionary patterns hidden in the multivariate SST signals. Then, a personalized convolution layer is designed to fuse these information. The developed network can be learned in an end-to-end manner. Our experiments on real SST datasets demonstrate the state-of-the-art performances of the proposed approach on the forecasting task.","New York, NY, USA",,"Zhang, Gaowei and Wang, Wei and Wang, Yi",Proceedings of the 2023 ACM Conference on Information Technology for Social Good,10.1145/3582515.3609561,9798400701160,,,,"SST, graph neural networks, transformer","Lisbon, Portugal",,,7,403–409,Association for Computing Machinery,GoodIT '23,Towards Spatio-temporal Sea Surface Temperature Forecasting via Dynamic Personalized Graph Network,https://doi.org/10.1145/3582515.3609561,,2023
article,10.1109/TASLP.2021.3068598,"The emotion of human is always expressed in a multimodal perspective. Analyzing multimodal human sentiment remains challenging due to the difficulties of the interpretation in inter-modality dynamics. Mainstream multimodal learning architectures tend to design various fusion strategies to learn inter-modality interactions, which barely consider the fact that the language modality is far more important than the acoustic and visual modalities. In contrast, we learn inter-modality dynamics in a different perspective via acoustic- and visual-LSTMs where language features play dominant role. Specifically, inside each LSTM variant, a well-designed gating mechanism is introduced to enhance the language representation via the corresponding auxiliary modality. Furthermore, in the unimodal representation learning stage, instead of using RNNs, we introduce ‘channel-aware’ temporal convolution network to extract high-level representations for each modality to explore both temporal and channel-wise interdependencies. Extensive experiments demonstrate that our approach achieves very competitive performance compared to the state-of-the-art methods on three widely-used benchmarks for multimodal sentiment analysis and emotion recognition.",,,"Mai, Sijie and Xing, Songlong and Hu, Haifeng",,10.1109/TASLP.2021.3068598,,2329-9290,2021,"IEEE/ACM Trans. Audio, Speech and Lang. Proc.",,,,,14,1424–1437,IEEE Press,,Analyzing Multimodal Sentiment Via Acoustic- and Visual-LSTM With Channel-Aware Temporal Convolution Network,https://doi.org/10.1109/TASLP.2021.3068598,29,2021
inproceedings,10.1145/3768184.3768253,"Across diverse benchmarks, large language models set new performance standards., but they are still struggling with high-precision numerical prediction tasks, because large language models are not sensitive to high-precision numbers and often have accuracy problems when faced with thousands of digits. To solve this problem, we propose a novel output layer structure, the Attentional Multi-Frequency Regressor (AMFR). Unlike the traditional linear regression head that simply maps hidden states, AMFR first projects the hidden representation into a multi-frequency space and constructs multi-scale frequency features using learnable sine-cosine basis functions; it then adaptively fuses the frequency components through an integrated attention mechanism to generate more refined high-precision numerical predictions. Experimental results show that AMFR significantly reduces the prediction error in high-precision numerical regression tasks such as molecular property prediction and time series prediction, effectively capturing important information at different frequencies. Our work provides an effective way to improve the performance of large language models in numerical reasoning tasks.","New York, NY, USA",,"Li, Mingxuan","Proceedings of the 2025 2nd International Conference on Image Processing, Intelligent Control and Computer Engineering",10.1145/3768184.3768253,9798400721113,,,,"Large Language Models, Multi-Frequency Projection, Attention-Based Fusion, High-Precision Prediction","
",,,5,389–393,Association for Computing Machinery,IPICE '25,AMFR: Attentive Multi-Frequency Regressor for High-Precision Numerical Prediction in Large Language Models,https://doi.org/10.1145/3768184.3768253,,2025
inproceedings,10.1145/3724154.3724271,"As the financial industry becomes more interconnected and reliant on digital systems, fraud detection systems must evolve to meet growing threats. Cloud-enabled Transformer models present a transformative opportunity to address these challenges. By leveraging the scalability, flexibility, and advanced AI capabilities of cloud platforms, companies can deploy fraud detection solutions that adapt to real-time data patterns and proactively respond to evolving threats. Using the Graph self-attention Transformer neural network module, we can directly excavate gang fraud features from the transaction network without constructing complicated feature engineering. Finally, the fraud prediction network is combined to optimize the topological pattern and the temporal transaction pattern to realize the high-precision detection of fraudulent transactions. The results of anti-fraud experiments on credit card transaction data show that the proposed model outperforms the 7 baseline models on all evaluation indicators: In the transaction fraud detection task, the average accuracy (AP) increased by 20\% and the area under the ROC curve (AUC) increased by 2.7\% on average compared with the benchmark graph attention neural network (GAT), which verified the effectiveness of the proposed model in the detection of credit card fraud transactions.","New York, NY, USA",,"Deng, Tingting and Bi, Shuochen and Xiao, Jue",Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management,10.1145/3724154.3724271,9798400711862,,,,"Cloud computing, Credit Card Transaction, Fraud Detection, Graph Neural Network, Transformer Model","
",,,6,702–707,Association for Computing Machinery,BDEIM '24,Transformer-Based Financial Fraud Detection with Cloud-Optimized Real-Time Streaming,https://doi.org/10.1145/3724154.3724271,,2025
inproceedings,10.1145/3711896.3737226,"A long-standing challenge for pushing sensor-based human activity recognition (HAR) to industrial usage is the distribution shift between training data and testing data: significant variations in data distribution lead to a notable decline in performance. Recently, Large Language Models (LLMs) have demonstrated exceptional generalization capability, which provides a new opportunity to mitigate the distribution shift problem of HAR. However, since LLMs are inherently designed and trained on textual data, their potential to enhance generalization in HAR applications remains an open question. In this paper, we introduce LLM4HAR, a novel LLM-based model to improve cross-domain HAR. LLM4HAR consists of three main modules: (i) the Sensor Data Adaptation module, which aligns IMU signals with LLMs via sensor embedding(ii) the Sensor Knowledge Learning module, which injects sensor knowledge into LLMs for activity recognition, and (iii) the Efficiency Enhancement module, which employs a partial training strategy and reduces the model size by more than 10 times. Extensive evaluations show that LLM4HAR outperforms the existing methods by 13.82\% in average F1 score, demonstrating the feasibility and effectiveness of transferring knowledge from pretrained LLMs to enhance HAR. Further, LLM4HAR has been adopted by JD Logistics to support downstream applications such as Courier Welfare Improvement and Map Data Generation.","New York, NY, USA",,"Hong, Zhiqing and Song, Yiwei and Li, Zelong and Yu, Anlan and Zhong, Shuxin and Ding, Yi and He, Tian and Zhang, Desheng",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737226,9798400714542,,,,"HAR, IMU, IoT data mining, human activity recognition, large language model, logistics, smart city","Toronto ON, Canada",,,11,4511–4521,Association for Computing Machinery,KDD '25,LLM4HAR: Generalizable On-device Human Activity Recognition with Pretrained LLMs,https://doi.org/10.1145/3711896.3737226,,2025
inproceedings,10.1145/3690624.3709192,"Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-Tracker, a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a) Interpretable: it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b) Automatic: it has no hyperparameters and continuously models tensor data streams fully automatically; (c) Scalable: the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/D-Tracker.","New York, NY, USA",,"Higashiguchi, Shingo and Matsubara, Yasuko and Kawabata, Koki and Murayama, Taichi and Sakurai, Yasushi",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709192,9798400712456,,,,"interest diffusion, tensor decomposition, time series","Toronto ON, Canada",,,12,460–471,Association for Computing Machinery,KDD '25,D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams,https://doi.org/10.1145/3690624.3709192,,2025
inproceedings,10.1145/3711896.3737135,"Probabilistic time series imputation has been widely applied in real-world scenarios due to its ability for uncertainty estimation and denoising diffusion probabilistic models (DDPMs) have achieved great success in probabilistic time series imputation tasks with its power to model complex distributions. However, current DDPM-based probabilistic time series imputation methodologies are confronted with two types of challenges: 1) The backbone modules of the denoising parts are not capable of achieving sequence modeling with low time complexity. 2) The architecture of denoising modules can not handle the dependencies in the time series data effectively. To address the first challenge, we explore the potential of state space model, namely Mamba, as the backbone denoising module for DDPMs. To tackle the second challenge, we carefully devise several SSM-based blocks for time series data modeling. Experimental results demonstrate that our approach can achieve state-of-the-art time series imputation results on multiple real-world datasets. Our datasets and code are available at https://github.com/decisionintelligence/SSD-TS/","New York, NY, USA",,"Gao, Hongfan and Shen, Wangmeng and Qiu, Xiangfei and Xu, Ronghui and Yang, Bin and Hu, Jilin",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737135,9798400714542,,,,"diffusion models, state space models, time series imputation","Toronto ON, Canada",,,12,649–660,Association for Computing Machinery,KDD '25,SSD-TS: Exploring the Potential of Linear State Space Models for Diffusion Models in Time Series Imputation,https://doi.org/10.1145/3711896.3737135,,2025
inproceedings,10.1145/3679409.3679427,"In the realm of time series tasks, fuzzy cognitive maps (FCMs) have demonstrated their potency as a robust model for predicting system states and representing knowledge in an interpretable manner. Recent research efforts have been directed towards enriching the core FCM framework by integrating features like temporal aspects, uncertainty, or fuzzy rules to improve interpretability. Moreover, attempts have been made to integrate fuzzy neural networks or wavelets to enhance the accuracy of time series predictions. However, striking a balance between precision and interpretability in predictions across diverse domains remains a significant challenge. To address the need for capturing spatial relationships between nodes, we introduce graph convolutional networks for predicting multivariate time series. In this work, we propose an innovative extension of FCM, termed fuzzy cognitive map enhanced by deep graph convolution (CE-FCM), and validate its efficacy across four publicly available datasets. Our findings affirm that CE-FCM furnishes pivotal insights for constructing interpretable predictors, thereby offering valuable utility for real-world applications.","New York, NY, USA",,"Liu, Yuxiang and Pang, Renning",Proceedings of the 2024 3rd International Symposium on Control Engineering and Robotics,10.1145/3679409.3679427,9798400709951,,,,,"Changsha, China",,,5,94–98,Association for Computing Machinery,ISCER '24,CE-FCM: Convolution-Enhanced Fuzzy Cognitive Map for Multivariate Time Series Prediction,https://doi.org/10.1145/3679409.3679427,,2024
inproceedings,10.1145/3747227.3747240,"Aiming at the poor adaptation of most current models to dynamic maps and the difficulty in capturing long-term spatio-temporally relevant features, this paper proposes a hybrid adaptive feature-enhanced graph neural network (HAFEGNN) model. To address these challenges, we propose a new approach based on graph neural networks that integrates multiple state-of-the-art enhancement feature representation mechanisms. The model enhances the representation of dynamic spatio-temporal features by integrating multi-scale convolution, channel self-attention and spatial self-attention mechanisms. On top of that, temporal self-attention mechanism is also introduced to focus on learning long-term dependencies, which leads to better understanding of patterns in historical data and making more accurate predictions. The model further incorporates Dynamic Graph Convolutional Recurrent Network (DGCRN) to capture spatio-temporal dynamics. To enhance the adaptability to dynamic graph structures, it integrates spatial location embedding with graph attention mechanisms to form a spatially aware module, which improves the prediction accuracy. Our experimental evaluation on the widely used PeMS04 and PeMS08 datasets demonstrates the effectiveness of the proposed HAFEGNN. The model achieves significant improvements in key metrics such as MAE, RMSE, and MAPE, outperforming existing state-of-the-art methods and validating its ability to handle complex traffic data. This study provides a new solution for traffic flow prediction and demonstrates its potential in handling complex traffic data.","New York, NY, USA",,"Chen, Yunhao and Xu, Hui",Proceedings of the 2025 International Conference on Machine Learning and Neural Networks,10.1145/3747227.3747240,9798400714382,,,,"feature enhancement, graph neural network, spatiotemporal correlation, traffic flow prediction","
",,,9,82–90,Association for Computing Machinery,MLNN '25,Traffic Flow Prediction Using Spatio-Temporal Graph Neural Networks Based on Hybrid Adaptive Feature Enhancement,https://doi.org/10.1145/3747227.3747240,,2025
inproceedings,10.1145/3768292.3770349,"Modeling large-scale time series has gained significant attention in recent years. However, its direct application in finance remains challenging due to substantial differences in data characteristics across domains. Specifically, financial systems feature inherent stochasticity and low signal-to-noise ratios, rendering traditional methods and pre-training approaches ineffective. This underscores the urgent need for a foundation model tailored to financial time series. To bridge this gap, we propose LENS, a pre-trained model for this domain. LENS effectively captures the complexity of financial stochastic systems through a carefully crafted model architecture and mitigates noise during pre-training by using an invertible embedding module. We provide a rigorous theoretical explanation of the model’s effectiveness and validate its performance through extensive experiments. Pre-trained on a dataset comprising 100 billion financial observations, LENS achieves exceptional results across a wide range of critical downstream tasks. Moreover, our work offers practical insights into developing pre-trained time series models in high-noise environments, paving the way for further advancements in this pivotal research domain.","New York, NY, USA",,"Xu, Yuanjian and Hao, Jianing and Liu, Anxian and Li, Zhenzhuo and Meng, Shichang and Yuan, Shuai and Zhang, Guang",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3770349,9798400722202,,,,"Large Model, Time Series Analysis, Pre-training Technology, Financial Time Series","
",,,8,771–778,Association for Computing Machinery,ICAIF '25,LENS: Large Pre-trained Transformer for Exploring Financial Time Series Regularities,https://doi.org/10.1145/3768292.3770349,,2025
inproceedings,10.1145/3580305.3599934,"In microservice systems, the identification of root causes of anomalies is imperative for service reliability and business impact. This process is typically divided into two phases: (i)constructing a service dependency graph that outlines the sequence and structure of system components that are invoked, and (ii) localizing the root cause components using the graph, traces, logs, and Key Performance Indicators (KPIs) such as latency. However, both phases are not straightforward due to the highly dynamic and complex nature of the system, particularly in large-scale commercial architectures like Microsoft Exchange.In this paper, we propose a new framework that employs Hierarchical Reinforcement Learning from Human Feedback (HRLHF) to address these challenges. Our framework leverages the static topology of the microservice system and efficiently employs the feedback of engineers to reduce uncertainty in the discovery of the service dependency graph. The framework utilizes reinforcement learning to reduce the number of queries required from O(N2) to O(1), enabling the construction of the dependency graph with high accuracy and minimal human effort. Additionally, we extend the discovered dependency graphs to window causal graphs that capture the characteristics of time series over a specified time period, resulting in improved root cause analysis accuracy and robustness. Evaluations on both real datasets from Microsoft Exchange and synthetic datasets with injected anomalies demonstrate superior performance on various metrics compared to state-of-the-art methods. It is worth mentioning that, our framework has been integrated as a crucial component in Microsoft M365 Exchange service.","New York, NY, USA",,"Wang, Lu and Zhang, Chaoyun and Ding, Ruomeng and Xu, Yong and Chen, Qihang and Zou, Wentao and Chen, Qingjun and Zhang, Meng and Gao, Xuedong and Fan, Hao and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599934,9798400701030,,,,"causal discovery, reinforcement learning from human feedback, root cause analysis","Long Beach, CA, USA",,,10,5116–5125,Association for Computing Machinery,KDD '23,Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback,https://doi.org/10.1145/3580305.3599934,,2023
article,10.1145/3665141,"Smart cities have drawn a lot of interest in recent years, which employ Internet of Things (IoT)-enabled sensors to gather data from various sources and help enhance the quality of residents’ life in multiple areas, e.g. public safety. Accurate crime prediction is significant for public safety promotion. However, the complicated spatial-temporal dependencies make the task challenging, due to two aspects: 1) spatial dependency of crime includes correlations with spatially adjacent regions and underlying correlations with distant regions, e.g. mobility connectivity and functional similarity; 2) there are near-repeat and long-range temporal correlations between crime occurrences across time. Most existing studies fall short in tackling with multi-view correlations, since they usually treat them equally without consideration of different weights for these correlations. In this paper, we propose a novel model for region-level crime prediction named as Heterogeneous Dynamic Multi-view Graph Neural Network (HDM-GNN). The model can represent the dynamic spatial-temporal dependencies of crime with heterogeneous urban data, and fuse various types of region-wise correlations from multiple views. Global spatial dependencies and long-range temporal dependencies can be derived by integrating the multiple GAT modules and Gated CNN modules. Extensive experiments are conducted to evaluate the effectiveness of our method using several real-world datasets. Results demonstrate that our method outperforms state-of-the-art baselines. All the code are available at https://github.com/ZJUDataIntelligence/HDM-GNN.","New York, NY, USA",,"Zhou, Binbin and Zhou, Hang and Wang, Weikun and Chen, Liming and Ma, Jianhua and Zheng, Zengwei",,10.1145/3665141,,1550-4859,,ACM Trans. Sen. Netw.,"crime prediction, graph neural network, data fusion, spatio-temporal prediction",,Just Accepted,,,,Association for Computing Machinery,,HDM-GNN: A Heterogeneous Dynamic Multi-view Graph Neural Network for Crime Prediction,https://doi.org/10.1145/3665141,,2024
article,10.1145/3632268.3632281,"Analysis and management of big data are important areas of research for data researchers and scientists. Both the industry and governmental agencies have invested tremendous resources and effort in the area of big data analysis and management in the past decade. Within the realm of big data, spatial and spatio-temporal data are still one of the fastest-growing types of data. With advances in remote sensors, sensor networks, and the proliferation of location sensing devices in daily life activities and common business practices, the generation of disparate, dynamic, and geographically distributed spatiotemporal data has continued to explode in recent years. In addition, significant progress in ground, air, and space-borne sensor technologies has led to unprecedented access to earth science data for scientists from different disciplines. For example, NASA recently collected its 10 millionth Landsat image [4] and the volume of satellite imagery being collected has reached the petabyte scale. Analysis of this large-scale data poses new challenges to researchers.","New York, NY, USA",,"Shashidharan, Ashwin and Gadiraju, Krishna Karthik and Vatsavai, Ranga Raju and Chandola, Varun",,10.1145/3632268.3632281,,,November 2022,SIGSPATIAL Special,,,,1,2,43–44,Association for Computing Machinery,,The 10th ACM SIGSPATIAL International Workshop on Analytics for Big Spatial Data (BigSpatial 2022),https://doi.org/10.1145/3632268.3632281,14,2023
inproceedings,10.1145/3746972.3746978,"This study proposes a multi-asset portfolio risk prediction model that integrates Long Short-Term Memory (LSTM) networks with Copula functions. The model is designed to capture both the temporal dynamics of financial asset returns and the nonlinear dependencies among assets. LSTM is used to model the marginal distributions of individual asset return series. Copula functions are employed to describe the joint distribution structure across multiple assets. This allows for accurate prediction of key portfolio risk metrics such as Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR). In the experimental design, several baseline models are constructed for performance comparison. Further analyses are conducted to assess the model's risk prediction ability under varying numbers of assets and to evaluate risk coverage across different confidence intervals. The experimental results show that the proposed LSTM-Copula model outperforms mainstream methods across multiple evaluation metrics. It demonstrates higher robustness and predictive accuracy, particularly in high-dimensional and sparse data settings. This approach offers a novel path for financial risk management by combining statistical modeling with deep learning. It provides strong empirical results and holds substantial practical value for applications in complex financial environments.","New York, NY, USA",,"Xu, Weiyao and Ma, Kunyuan and Wu, You and Chen, Yuan and Yang, Zhirui and Xu, Zhen",Proceedings of the 2025 International Conference on Digital Economy and Intelligent Computing,10.1145/3746972.3746978,9798400713576,,,,"LSTM, Multi-asset portfolio, copula function, risk prediction","
",,,5,32–36,Association for Computing Machinery,DEIC '25,LSTM-Copula Hybrid Approach for Forecasting Risk in Multi-Asset Portfolios,https://doi.org/10.1145/3746972.3746978,,2025
inbook,10.1145/3729706.3729756,"Stock price prediction is a challenging problem in the field of finance that receives significant attention. Researchers have developed combinations of neural network models, such as RNNs, GNNs, and Transformers, which show great potential for stock forecasting. However, these models often struggle with the complex nonlinear dynamics of the stock market and neglect the intertwined spatial relationships and temporal features among stocks. This oversight can adversely affect prediction accuracy. Therefore, we propose a stock price prediction model named MSMixer-GCN in this paper, which is based on a multi-scale convolutional MLP Mixer and a spatio-temporal graph convolutional network. Firstly, we adopt a multi-scale convolution combined with the MLP-Mixer approach to construct the MS-MLP Mixer module, which captures trends in stock indicators and temporal dimensions, thereby enhancing the utilization of historical time series information. Next, we develop a dynamic spatio-temporal graph convolutional network that allows spatial structural information to interact with features across different time scales, significantly improving the accuracy of stock price predictions. Finally, through extensive experiments on three datasets from the United States stock market, our proposed MSMixer-GCN model demonstrates outstanding performance, surpassing state-of-the-art models in terms of IC ranking metrics and the Sharpe Ratio, achieving average relative performance gains of 6.25\% and 8.67\%, respectively. This confirms the superior performance and generalization capability of the model.","New York, NY, USA",,"He, Ruoyan and Xiao, Hao","Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy",,9798400712715,,,,,,,,8,314–321,Association for Computing Machinery,,"MSMixer-GCN: A Hybrid Deep Learning Model for Stock Price Prediction Using Multi-Scale Convolution, MLP-Mixer and Graph Convolution Networks",https://doi.org/10.1145/3729706.3729756,,2025
inproceedings,10.1145/3649476.3658696,"Due to the continuous technology scaling and the ever-increasing complexity and size of the hardware designs, manufacturing defects have become a key obstacle in meeting end-user demand. Despite decades of research, traditional test-generation techniques often struggle to scale to massive and complex designs. Such scalability issues stem from the numerous backtracking the traditional test generation techniques perform before converging to a test pattern. In this work, we present DETECTive that leverages deep learning on graphs to learn fault characteristics and predict test pattern(s) to expose faults without requiring backtracking. DETECTive is trained on small circuits, and its learned knowledge is transferable to predict test patterns for circuits that contain up to 29 \texttimes{","New York, NY, USA",,"Petrolo, Vincenzo and Medya, Sourav and Graziano, Mariagrazia and Pal, Debjit",Proceedings of the Great Lakes Symposium on VLSI 2024,10.1145/3649476.3658696,9798400706059,,,,"ATPG, Deep Learning, Graph Neural Networks","Clearwater, FL, USA",,,6,32–37,Association for Computing Machinery,GLSVLSI '24,DETECTive: Machine Learning-driven Automatic Test Pattern Prediction for Faults in Digital Circuits,https://doi.org/10.1145/3649476.3658696,,2024
inproceedings,10.1145/3580305.3599549,"Given its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance ","New York, NY, USA",,"Wang, Jingyuan and Yang, Chen and Jiang, Xiaohan and Wu, Junjie",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599549,9798400701030,,,,"attentions, dynamic time warping, time series, wavelet","Long Beach, CA, USA",,,13,2361–2373,Association for Computing Machinery,KDD '23,WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analysis,https://doi.org/10.1145/3580305.3599549,,2023
inbook,10.1145/3748825.3748973,"Large-scale advertisement systems play a crucial role in the digital economy, yet they often face challenges related to reliability and performance. Addressing these issues is critical for maintaining effective ad serving and ensuring a positive user experience. We present RAID, a framework that utilizes intelligent detection mechanisms to monitor ad performance continuously. By employing advanced machine learning algorithms capable of identifying anomalies, RAID effectively mitigates reliability concerns within ad systems. The hybrid architecture merges real-time data processing with predictive modeling, enabling the anticipation of potential failures. Utilizing reinforcement learning, RAID further refines its detection strategies, optimizing resource allocation while minimizing service disruptions. Results from extensive real-world experiments clearly indicate that RAID diminishes reliability incidents significantly while enhancing overall system performance.","New York, NY, USA",,"Zhu, Bingxin",Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence,,9798400714337,,,,,,,,4,964–967,Association for Computing Machinery,,RAID: Reliability Automation through Intelligent Detection in Large-Scale Ad Systems,https://doi.org/10.1145/3748825.3748973,,2025
inproceedings,10.1145/3746252.3761543,"The recent surge in solar plant installations has notably decreased the reliance on fossil fuels while also presenting significant challenges to power grid. Therefore, the accurate forecasting of centralized and distributed solar power has become critically important. Although site-specific forecasting models typically perform better for utility-scale solar power plants, the model maintenance can be troublesome as the number of solar plants grows. Furthermore, the rapid growth and difficulties in real-time data collection associated with distributed solar systems exacerbate the complexity of regional gross solar power forecasting. To address these issues, we propose SolarMAE, a unified regional solar power forecasting framework enabling end-to-end precise forecasting for both centralized and distributed solar systems. It adopts masked autoencoder (MAE) pre-training strategy for numerical weather prediction (NWP) reconstruction at first, aiming to derive spatiotemporal correlations within meteorological variables, and then fine-tunes a temporal convolutional neural network which predicts future solar power generation. Experiments show that this framework outperforms state-of-the-art centralized or distributed solar power forecasting methods in accuracy, and significantly reduces model maintenance cost. It also demonstrates strong few-shot learning capabilities, which is particularly useful for the cold start problem of newly installed solar plants. The unified solar power forecasting system has been deployed in a province in eastern China, serving solar systems with over 73 GW gross installed capacity and more than 400 centralized solar plants.","New York, NY, USA",,"Wang, Jin and Peng, Bingqing and Wang, Wenwei and Hu, Yuanjie and Chen, Yuejiang and Niu, Peisong and Sun, Liang",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761543,9798400720406,,,,"masked auto encoder, solar power forecasting, zero-shot learning","Seoul, Republic of Korea",,,9,6093–6101,Association for Computing Machinery,CIKM '25,SolarMAE: A Unified framework for Regional Centralized and Distributed Solar Power Forecasting with Weather Pre-training,https://doi.org/10.1145/3746252.3761543,,2025
inproceedings,10.1145/3762249.3762309,"This study addresses the growing complexity of transaction behaviors and the highly concealed nature of money laundering paths in current financial AML scenarios. It proposes a Transformer-based risk monitoring model for anti-money laundering. The approach is grounded in transaction sequence modeling and integrates the structural information of transaction graphs. A context-aware classifier is introduced to enable accurate identification and risk scoring of high-risk accounts. The model first applies feature embedding and positional encoding to each transaction. It then uses multiple Transformer layers to capture long-range behavioral dependencies. At the same time, it incorporates account interaction information from the graph structure. This enhances the model's ability to detect abnormal transaction chains across accounts. At the output stage, a classifier that fuses sequential semantics with graph context is used to determine the overall money laundering risk of each account. Multiple experiments were conducted on the publicly available Elliptic dataset. Results show that the proposed method outperforms existing mainstream models on evaluation metrics such as AUC, F1-Score, and Accuracy. It also demonstrates stronger discriminative power and greater stability in identifying high-risk accounts. Further analysis of model depth sensitivity and case-based verification supports the model's effectiveness in real-world complex transaction environments. The proposed method offers a more adaptable technical solution for financial institutions dealing with large-scale suspicious behavior detection tasks.","New York, NY, USA",,"Wu, You and Qin, Ying and Su, Xin and Lin, Yuxiu","Proceedings of the 2025 2nd International Conference on Digital Economy, Blockchain and Artificial Intelligence",10.1145/3762249.3762309,9798400713491,,,,"Anti-money laundering, Transformer model, financial behavior analysis, transaction graph modeling","
",,,6,388–393,Association for Computing Machinery,DEBAI '25,Transformer-Based Risk Monitoring for Anti-Money Laundering with Transaction Graph Integration,https://doi.org/10.1145/3762249.3762309,,2025
inproceedings,10.1145/3689062.3689080,"Multimodal humor detection has become an active research area in the field of artificial intelligence. This paper presents the solution for the MuSe-Humor sub-challenge of cross-cultural humor detection. The goal of the MuSe-Humor sub-challenge is to predict whether humor exists in a given dataset, which includes data from various modalities such as text, audio, and video. The training data consists of German recordings and their transcriptions, while the test data is in English. This cross-cultural testing introduces new challenges, distinguishing it from ordinary multimodal humor detection tasks. To address this issue, we propose a method called Dual-Phase Processing (DPP). The proposed method first preprocesses the text data using a large language model to extract more effective features for humor detection from the raw data. It also partially addresses cross-cultural differences through bilingual annotation. Finally, by applying composite temporal smoothing to the results after decision-level fusion of the three modalities, the accuracy of humor prediction is greatly improved. The experimental results on the official test set demonstrate the effectiveness of our model. Our model achieves an impressive AUC score of 0.9366 on the test set, far surpassing the baseline and securing the first-place ranking.","New York, NY, USA",,"Chen, Shun and Yao, Hailiang and Xu, Mingyu and Wen, Zhuofan and Sun, Haiyang and Sun, Licai and Lian, Zheng and Liu, Bin and Zhang, Fengyu and Zhang, Siyuan and Tao, Jianhua",Proceedings of the 5th on Multimodal Sentiment Analysis Challenge and Workshop: Social Perception and Humor,10.1145/3689062.3689080,9798400711992,,,,"cross-cultural humor recognition, large language model, multimodal sentiment analysis, temporal prediction smoothing","Melbourne VIC, Australia",,,9,70–78,Association for Computing Machinery,MuSe'24,DPP: A Dual-Phase Processing Method for Cross-Cultural Humor Detection,https://doi.org/10.1145/3689062.3689080,,2024
inproceedings,10.1145/3589334.3645391,"Accurate customer LifeTime Value (LTV) predictions are crucial for customer relationship management, especially in Supply Chain Platforms (SCP), which involve effectively managing the service resources in business decision-making. Previous LTV prediction methods usually rely on ample historical customer data, which is not available in the early stages of a customer's lifecycle. It makes the modeling of the historical customer data a difficult task due to the data sparsity. Besides, the long-tail distribution of customer LTV also brings new challenges to the prediction of LTV. To tackle the above issues, we propose CDLtvS, a novel Cross Domain method for customer Lifetime value prediction in SCP. It leverages rich cross-domain information from upstream platforms to enhance LTV predictions in downstream platforms. Firstly, CDLtvS pre-trains the customer representations by an LTV modeling framework named LtvS in source and target domains separately. Specifically, LtvS incorporates the Expert Mask Network (ExMN), which not only effectively models the long-tail distribution of LTV in single-domain but also resolves cross-domain learning model bias resulting from this distribution. Then, the various-level alignment mechanism is introduced to keep the consistency of knowledge transferring from source to target domains on both sparse and non-sparse data. Comprehensive experiments on real-world data from JD, one of the world's largest supply chain platforms, demonstrate that CDLtvS achieves a normalized mean average error of 0.3378 in LTV prediction, outperforming 16.3\% to the baseline. Additionally, the improvements of ≥2.3\% across various data sparsity levels (0\% -- 80\%) provide valuable insights into cross-domain LTV modeling.","New York, NY, USA",,"Zhou, Zhiyuan and Lin, Li and Wang, Hai and Zhou, Xiaolei and Wei, Gong and Wang, Shuai",Proceedings of the ACM Web Conference 2024,10.1145/3589334.3645391,9798400701719,,,,"cross-domain knowledge, economics, lifetime value prediction, supply chain platform","Singapore, Singapore",,,10,4037–4046,Association for Computing Machinery,WWW '24,A Cross Domain Method for Customer Lifetime Value Prediction in Supply Chain Platform,https://doi.org/10.1145/3589334.3645391,,2024
article,10.1145/3701988,"Sequential recommendation, leveraging user-item interaction histories to provide personalized and timely suggestions, has drawn significant research interest recently. With the power of exploiting spatio-temporal dynamics, Dynamic Graph Neural Networks (DyGNNs) show great potential in sequential recommendation by modeling the dynamic relationship between users and items. However, spatio-temporal distribution shifts naturally exist in out-of-distribution sequential recommendation, where both user-item relationships and temporal sequences demonstrate pattern shifts. The out-of-distribution scenarios may lead to the failure of existing DyGNNs in handling spatio-temporal distribution shifts in sequential recommendation, given that the patterns they exploit tend to be variant w.r.t labels under distribution shifts. In this article, we propose Disentangled Intervention-based Dynamic graph Attention networks with Invariance Promotion (I-DIDA) to handle spatio-temporal distribution shifts in sequential recommendation by discovering and utilizing invariant patterns, i.e., structures and features whose predictive abilities are stable across distribution shifts. Specifically, we first propose a disentangled spatio-temporal attention network to capture the variant and invariant patterns. By utilizing the disentangled patterns, we design a spatio-temporal intervention mechanism to create multiple interventional distributions and an environment inference module to infer the latent spatio-temporal environments, and minimize the invariance loss to leverage the invariant patterns with stable predictive abilities under distribution shifts. Extensive experiments demonstrate the superiority of our method over state-of-the-art sequential recommendation baselines under distribution shifts.","New York, NY, USA",19,"Zhang, Zeyang and Wang, Xin and Chen, Haibo and Li, Haoyang and Zhu, Wenwu",,10.1145/3701988,,1046-8188,January 2025,ACM Trans. Inf. Syst.,"Recommender systems, Sequential Recommendation, Graph Machine Learning, Dynamic Graph Neural Network, Out-Of-Distribution Generalization",,,1,42,,Association for Computing Machinery,,Disentangled Dynamic Graph Attention Network for Out-of-Distribution Sequential Recommendation,https://doi.org/10.1145/3701988,43,2024
inproceedings,10.1145/3637528.3671836,"Synthesizing electronic health records (EHR) data has become a preferred strategy to address data scarcity, improve data quality, and model fairness in healthcare. However, existing approaches for EHR data generation predominantly rely on state-of-the-art generative techniques like generative adversarial networks, variational autoencoders, and language models. These methods typically replicate input visits, resulting in inadequate modeling of temporal dependencies between visits and overlooking the generation of time information, a crucial element in EHR data. Moreover, their ability to learn visit representations is limited due to simple linear mapping functions, thus compromising generation quality. To address these limitations, we propose a novel EHR data generation model called EHRPD. It is a diffusion-based model designed to predict the next visit based on the current one while also incorporating time interval estimation. To enhance generation quality and diversity, we introduce a novel time-aware visit embedding module and a pioneering predictive denoising diffusion probabilistic model (P-DDPM). Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM. We conduct experiments on two public datasets and evaluate EHRPD from fidelity, privacy, and utility perspectives. The experimental results demonstrate the efficacy and utility of the proposed EHRPD in addressing the aforementioned limitations and advancing EHR data generation.","New York, NY, USA",,"Zhong, Yuan and Wang, Xiaochen and Wang, Jiaqi and Zhang, Xiaokun and Wang, Yaqing and Huai, Mengdi and Xiao, Cao and Ma, Fenglong",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671836,9798400704901,,,,"diffusion models, electronic health records, medical data synthesis, multimodal data mining","Barcelona, Spain",,,12,4607–4618,Association for Computing Machinery,KDD '24,Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models,https://doi.org/10.1145/3637528.3671836,,2024
article,10.1145/3777547,"Spatial-temporal prediction has become an important task in many applications, such as traffic forecasting. Due to the spatial-temporal nature of data, most state-of-the-art methods heavily depend on graph neural networks to model the inherent spatial relationships. However, most of them process the spatial data by applying the prior adjacency knowledge or learning a static adaptive adjacency matrix. Thus, their prediction performance is limited on dynamic situations where the spatial dependencies change w.r.t time. Furthermore, considering the stochastic training process, learning an adaptive adjacency matrix from scratch also makes it difficult for the neural network to achieve stable parameters and performance. To address the above challenges, this paper proposes three practical extensions that incorporate dynamic causal knowledge into the training of graph convolution networks. We first analyze the dynamic causal graphs between traffic nodes with one dynamic causal discovery algorithm in each extended model. Subsequently, the spatial module employs dynamic causal graphs to reveal the evolving connections among nodes. Extensive experiments demonstrate that our method has successfully enhanced state-of-the-art traffic forecasting models on two benchmarks.","New York, NY, USA",,"Pan, Yicheng and Wang, Haowei and Ma, Meng and Wang, Ping",,10.1145/3777547,,2374-0353,,ACM Trans. Spatial Algorithms Syst.,"spatial-temporal prediction, graph convolution, causality analysis, traffic forecasting.",,Just Accepted,,,,Association for Computing Machinery,,Enhancing Spatial-Temporal Prediction Models with Dynamic Causal Graphs,https://doi.org/10.1145/3777547,,2025
article,10.1145/3771999,"The choice of method names significantly influences code comprehension and maintenance, posing a considerable challenge, especially for novice developers. Automating the prediction of appropriate method names based on the method code body has emerged as a promising approach to address this challenge. In recent years, numerous machine/deep learning (ML/DL)-based method name prediction (MNP) techniques have been proposed. However, a systematic review of these techniques is currently lacking, hindering future researchers from understanding the research status, development trends, challenges, and opportunities in this field. To fill this gap, in this paper, we conduct a systematic literature review on learning-based MNP studies. Specifically, we first perform a thorough review of the literature concerning publication venue, publication year, and contribution types. This analysis enables us to discern trends in studies related to MNP. Second, we depict the general workflow of learning-based MNP techniques, which involves three consecutive subprocesses: context extraction, context preprocessing, and context-based prediction. Subsequently, we investigate contemporary techniques/solutions applied in the three subprocesses. Third, we scrutinize the widely used experimental databases, evaluation metrics, and replication packages utilized in MNP studies. Moreover, we summarize existing empirical studies on MNP to facilitate a quick understanding of their focus and findings for subsequent researchers. Finally, based on a systematic review and summary of existing work, we outline several open challenges and opportunities in MNP that remain to be addressed in future work.","New York, NY, USA",,"Qian, Hanwei and Liu, Wei and Xu, Tingting and Yin, Jie and Feng, Xia and Sun, Weisong and Ding, Ziqi and Chen, Yuchen and Miao, Yun and Li, Jiaxun and Zhao, Jianhua and Fang, Chunrong",,10.1145/3771999,,1049-331X,,ACM Trans. Softw. Eng. Methodol.,"Method Name Prediction, Method Name Suggestion, Method Name Recommendation, Machine Learning, Deep Learning, AI and Software Engineering",,Just Accepted,,,,Association for Computing Machinery,,A Survey of Learning-based Method Name Prediction,https://doi.org/10.1145/3771999,,2025
inproceedings,10.1145/3637528.3671546,"As the focus of space exploration shifts from national agencies to private companies, the interest in space industry has been steadily increasing. With the increasing number of satellites, the risk of collisions between satellites and space debris has escalated, potentially leading to significant property and human losses. Therefore, accurately modeling the orbit is critical for satellite operations. In this work, we propose the Decomposed Attention Segment Recurrent Neural Network (DASR) model, adding two key components, Multi-Head Attention and Tensor Train Decomposition, to SegRNN for orbit prediction. The DASR model applies Multi-Head Attention before segmenting at input data and before the input of the GRU layers. In addition, Tensor Train (TT) Decomposition is applied to the weight matrices of the Multi-Head Attention in both the encoder and decoder. For evaluation, we use three real-world satellite datasets from the Korea Aerospace Research Institute (KARI), which are currently operating: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5 satellites. Our proposed model demonstrates superior performance compared to other SOTA baseline models. We demonstrate that our approach has 94.13\% higher predictive performance than the second-best model in the KOMPSAT-3 dataset, 89.79\% higher in the KOMPSAT-3A dataset, and 76.71\% higher in the KOMPSAT-5 dataset.","New York, NY, USA",,"Jeong, SeungWon and Woo, Soyeon and Chung, Daewon and Woo, Simon S. and Shin, Youjin",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671546,9798400704901,,,,"model compression, orbit prediction, parameter reduction, tensor-train decomposition, time series prediction","Barcelona, Spain",,,11,5172–5182,Association for Computing Machinery,KDD '24,Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction,https://doi.org/10.1145/3637528.3671546,,2024
inproceedings,10.1145/3674029.3674059,"The use of Graph Neural Networks (GNNs) in time series analysis represents a rising field of study, particularly in the context of GNN Graph Classification, a technique traditionally applied in disciplines such as biology and chemistry. Our research repurposes GNN Graph Classification for the analysis of time series for climate data, focusing on two distinct methodologies: the city-graph method, which effectively captures static temporal snapshots, and the sliding window graph method, adept at tracking dynamic temporal changes. This innovative application of GNN Graph Classification within time series data enables the uncovering of nuanced data trends. We demonstrate how GNNs can construct meaningful graphs from time series data, showcasing their versatility across different analytical contexts. A key finding is GNNs’ adeptness at adapting to changes in graph structure, which significantly improves outlier detection. This enhances our understanding of climate patterns and suggests broader applications of GNN Graph Classification in analyzing complex data systems beyond traditional time series analysis. Our research seeks to fill a gap in current studies by providing an examination of GNNs in climate change analysis, highlighting the potential of these methods in capturing and interpreting intricate data trends.","New York, NY, USA",,"Romanova, Alex",Proceedings of the 2024 9th International Conference on Machine Learning Technologies,10.1145/3674029.3674059,9798400716379,,,,"Anomaly Detection, City-Graph Method, GNN Graph Classification, Graph Neural Networks, Sliding Window Graph, Time Series Analysis","Oslo, Norway",,,7,181–187,Association for Computing Machinery,ICMLT '24,GNN Graph Classification for Time Series: A New Perspective on Climate Change Analysis,https://doi.org/10.1145/3674029.3674059,,2024
inproceedings,10.1145/3736426.3736434,"The impacts of digital transformation on the business performance of enterprises are examined using the LSTM models in the present work. Corporate giants like Alibaba, Huawei, and Tencent have, by embracing digital transformation and introducing technological solutions like cloud computing and artificial intelligence (AI), improved their decision-making and business efficiency. To predict the impacts of these digital technologies on business performance, the long short-term memory (LSTM) model provides a solution. In the present work, an LSTM model is employed to predict the business performance of some large enterprises in China and it is revealed that the models achieve a high accuracy in prediction, with an R2 of 0.99 and an accuracy of 97\% for the prediction of Huawei at an embedding size of 40. With a focus on the hyperparameter tuning and dropout rates, we find that increasing the units of the LSTM model can improve the prediction accuracy, but the returns reduce when the units are beyond 150. Our LSTM model provides a practical solution for business performance prediction under the impact of digital transformation.","New York, NY, USA",,"Feng, Lina and Li, Ling and Qi, Shaoli",Proceedings of the 2025 International Conference on Digital Management and Information Technology,10.1145/3736426.3736434,9798400714238,,,,"LSTM, business performance prediction, digital technology, digital transformation","
",,,5,42–46,Association for Computing Machinery,DMIT '25,Prediction of Impacts of Digital Transformation on Business Performance using LSTM model,https://doi.org/10.1145/3736426.3736434,,2025
inproceedings,10.1145/3711896.3737033,"Incomplete time series classification is both practically valuable and challenging as missing values in time series data are prevalent in real-world scenarios. Current approaches suffer from two major limitations. First, they overemphasize the consistency of data reconstruction during missing value imputation while neglecting the task-effectiveness of the imputed results for the classification. Second, they fail to systematically establish a synergistic optimization mechanism between data imputation and feature representation. To address these challenges, we propose a Hierarchical Conditional Information Bottleneck (HCIB) framework, which achieves incomplete time series classification through end-to-end joint optimization. Specifically, at the data imputation level, we re-examine the dual effects of missing data: the loss of critical information (Loss) versus the gain in interference suppression (Gain), elucidating this duality through bias-variance trade-off theory. Building on this analysis, we propose a task-information sufficiency criterion and extend the information bottleneck theory into a task-driven imputation framework by incorporating label information as a conditional constraint. At the feature representation level, we construct a hierarchical information bottleneck architecture to learn compressed yet informative temporal representations from the task-oriented imputed data. Furthermore, we derive the optimizable objective function for HCIB and design specialized neural network architectures for time series. Comprehensive experiments on multivariate and univariate time series datasets across multiple domains consistently demonstrate that the proposed method achieves significant improvements in classification performance compared to SOTA approaches.","New York, NY, USA",,"Zhang, Shuo and Wang, Jing and Nie, Shiqin and Yue, Jinghang and Zhu, Weikang and Lin, Youfang",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737033,9798400714542,,,,"incomplete time series classification, information bottleneck, time series","Toronto ON, Canada",,,12,3796–3807,Association for Computing Machinery,KDD '25,Loss or Gain: Hierarchical Conditional Information Bottleneck Approach for Incomplete Time Series Classification,https://doi.org/10.1145/3711896.3737033,,2025
inproceedings,10.1145/3664647.3681095,"Learning semantic-rich representations from unlabeled time series data with intricate dynamics is a notable challenge. Traditional contrastive learning techniques predominantly focus on segment-level augmentations through time slicing, a practice that, while valuable, often results in sampling bias and suboptimal performance due to the loss of global context. Furthermore, they typically disregard the vital frequency information to enrich data representations. To this end, we propose a novel self-supervised general-purpose framework called Temporal-Frequency and Contextual Consistency (TFCC). Specifically, this framework first performs two instance-level augmentation families over the entire series to capture nuanced representations alongside critical long-term dependencies. Then, TFCC advances by initiating dual cross-view forecasting tasks between the original series and its augmented counterpart in both time and frequency domains to learn robust representations. Finally, three specially designed consistency modules 'temporal, frequency, and temporal-frequency' aid in further developing discriminative representations on top of the learned robust representations. Extensive experiments on multiple benchmarks demonstrate TFCC's superiority over the state-of-the-art classification and forecasting methods and exhibit exceptional efficiency in semi-supervised and transfer learning scenarios.","New York, NY, USA",,"Wu, Yuhan and Meng, Xiyu and He, Yang and Zhang, Junru and Zhang, Haowen and Dong, Yabo and Lu, Dongming",Proceedings of the 32nd ACM International Conference on Multimedia,10.1145/3664647.3681095,9798400706868,,,,"contrastive learning, multi-view self-supervised learning, time series analysis, time-frequency mining","Melbourne VIC, Australia",,,9,9582–9590,Association for Computing Machinery,MM '24,Multi-view Self-Supervised Contrastive Learning for Multivariate Time Series,https://doi.org/10.1145/3664647.3681095,,2024
article,10.1145/3649142,"Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.","New York, NY, USA",138,"Chen, April and Rossi, Ryan A. and Park, Namyong and Trivedi, Puja and Wang, Yu and Yu, Tong and Kim, Sungchul and Dernoncourt, Franck and Ahmed, Nesreen K.",,10.1145/3649142,,1556-4681,July 2024,ACM Trans. Knowl. Discov. Data,"Fairness, Bias, Graph Neural Networks",,,6,23,,Association for Computing Machinery,,Fairness-Aware Graph Neural Networks: A Survey,https://doi.org/10.1145/3649142,18,2024
inproceedings,10.1145/3711896.3736904,"Spatio-temporal prediction is a pivotal task with broad applications in traffic management, climate monitoring, energy scheduling, etc. However, existing methodologies often struggle to balance model expressiveness and computational efficiency, especially when scaling to large real-world datasets. To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph Separation Networks), a novel framework that decouples temporal and spatial modeling to enhance both efficiency and precision. Therein, the temporal dimension is modeled using lightweight large language models, which effectively capture low-rank temporal dynamics. Concurrently, the spatial dimension is addressed through an adaptive hypergraph neural network, which dynamically constructs hyperedges to model intricate, higher-order interactions. A carefully designed gating mechanism is integrated to seamlessly fuse temporal and spatial representations. By leveraging the fundamental principles of low-rank temporal dynamics and spatial interactions, STH-SepNet offers a pragmatic and scalable solution for spatio-temporal prediction in real-world applications. Extensive experiments on large-scale real-world datasets across multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting predictive performance while maintaining computational efficiency. This work may provide a promising lightweight framework for spatio-temporal prediction, aiming to reduce computational demands and while enhancing predictive performance. Our code is avaliable at https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.","New York, NY, USA",,"Chen, Jiawen and Shao, Qi and Chen, Duxin and Yu, Wenwu",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3736904,9798400714542,,,,"adaptive hypergraph neural networks, graph neural networks, large language models, spatio-temporal prediction","Toronto ON, Canada",,,12,167–178,Association for Computing Machinery,KDD '25,Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs,https://doi.org/10.1145/3711896.3736904,,2025
article,10.1145/3776557,"Spatial-temporal imputation remains a challenging problem in transportation, environment and healthcare, where the missing value is filled based on spatial, temporal, and cross correlations. Previous research mainly focused on feature-level correlation integration and comprehension with the hand-crafted enhancement strategy. Meanwhile, the recently prevalent large language models (LLMs) provide token-level understanding for language linguistics, and whether they could be applied for spatial-temporal correlation enhancement is under exploration. To this end, we proposed an LLM-native framework STOMA to fully utilize the intrinsic relevance. We designed semantic enhancing methods by converting the complex correlations, e.g. spatial correlation in network, temporal correlation with periodicity and cross correlation from human behavior, into the embedded tokens. Specifically, we reform dynamic time warping as an asymmetric correlation constructor for complex dynamics. We adapt the proposed backbone along with the spatial-temporal fine-tuning technique, and the empirical results demonstrate the effectiveness of our methods over recent LLM-inspired methods evaluating on real-world datasets.","New York, NY, USA",,"Xue, Xin and Zhou, Haoyi and Li, Lanhao and Lin, Yihan and Chen, Tianyu and Li, Jianxin",,10.1145/3776557,,2157-6904,,ACM Trans. Intell. Syst. Technol.,"Spatial-temporal Modeling, Neural Networks, Graph, Time series",,Just Accepted,,,,Association for Computing Machinery,,Leveraging LLMs for Semantic Correlation Enhancement in Spatial-temporal Imputation,https://doi.org/10.1145/3776557,,2025
inproceedings,10.1145/3690624.3709299,"Many modern recommender systems represent user and item attributes as embedding vectors, relying on them for accurate recommendations. However, entangled embeddings often capture not only intrinsic property factors (e.g., user interest in item property) but also popularity factors (e.g., user conformity to item popularity) indistinguishably. These embeddings, influenced by popularity distribution, may face challenges when the popularity distribution at test time differs from historical distribution. Existing remedies in the literature involve disentangled embedding learning, which aims to separately capture intrinsic and popularity factors, demonstrating plausible generalization during popularity distribution shifts. However, we highlight that these methods often overlook a crucial aspect of popularity shifts-their temporal nature-in both training and inference phases. To address this, we propose Temporal Popularity distribution shift generalizABle recommender system (TPAB), a novel disentanglement framework incorporating temporal popularity. TPAB introduce a new (1) temporal-aware embedding design for users and items. Within this design, (2) popularity coarsening and (3) popularity bootstrapping are proposed to enhance generalization further. We also provide theoretical analysis showing that the bootstrapping loss eliminates the effect of popularity on the learned model. During inference, we infer test-time popularity and corresponding embeddings, using them alongside property embeddings for prediction. Extensive experiments on real-world datasets validate TPAB, showcasing its outstanding generalization ability during temporal popularity distribution shifts.","New York, NY, USA",,"Yoo, Hyunsik and Qiu, Ruizhong and Xu, Charlie and Wang, Fei and Tong, Hanghang",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709299,9798400712456,,,,"embedding disentanglement, recommender systems, temporal popularity distribution shifts","Toronto ON, Canada",,,11,1833–1843,Association for Computing Machinery,KDD '25,Generalizable Recommender System During Temporal Popularity Distribution Shifts,https://doi.org/10.1145/3690624.3709299,,2025
inproceedings,10.1145/3649601.3698743,"As autonomous vehicles (AVs) become vital to modern systems, their vulnerability to cyber-attacks, especially GPS spoofing, presents a significant security threat. This study addresses these challenges by applying a suite of machine learning models to enhance the detection of anomalous GPS signals. We concentrate on autoencoder-based architectures, training models like LSTM-VAE, LSTM-AE, MLP-VAE, MLP-AE, Stacked-LSTM-VAE, and Stacked-LSTM-AE exclusively with authentic GPS data. This strategy improves spoofed signal detection by recognizing deviations from normal patterns through reconstruction error analysis. Our comparative analysis highlights the distinct capabilities of these models in distinguishing between authentic and spoofed signals, with the MLP-AE and Stacked-LSTM models showing notable performance differences. The MLP-AE model demonstrated significant detection abilities with an accuracy of 95.40\%, precision of 93.09\%, and ROC-AUC score of 94.35\%. Similarly, Stacked-LSTM models employing deeper learning structures proved highly effective in managing complex noisy data, crucial for high-stakes applications like AV navigation. The results highlight the potential of combining autoencoder models with MLP or Stacked-LSTM to enhance AV security against GPS spoofing, affirming the value of unsupervised learning for anomaly detection.","New York, NY, USA",,"Mirzakhaninafchi, Hasan and Pack, Chulwoo and Kim, Dongyoun and Chang, Young and Won, Kwanghee",Proceedings of the International Conference on Research in Adaptive and Convergent Systems,10.1145/3649601.3698743,9798400706066,,,,"anomaly detection, time series, sequential data, GPS spoofing attacks, autoencoder, autonomous vehicle security, cyber-attack countermeasures","HABITA79 Pompeii, Pompei, Italy",,,8,113–120,Association for Computing Machinery,RACS '24,Comparative Analysis of Deep Learning-based Anomaly Detection Models for GPS Spoofing Detection,https://doi.org/10.1145/3649601.3698743,,2025
inproceedings,10.1145/3768292.3770430,"Financial markets are complex adaptive systems where assets continuously influence each other through dynamic and nonlinear interactions. Accurate mid-price forecasting in high-frequency trading (HFT) depends on capturing these market-wide dynamics. While order flow imbalance (OFI) features have proven more effective than raw limit order book (LOB) data for short-term forecasting, most existing models remain limited to single-asset dynamics, ignoring informative signals from related instruments. We propose OF-MATNet, a deep learning-based multi-asset forecasting framework that leverages OFI data from multiple Nasdaq-listed assets. Our approach captures nonlinear cross-asset dependencies through a Transformer-based architecture with multi-axis attention mechanisms over time, assets, and order book levels, enhanced with positional encoding. Informative peer assets for each target are selected using rolling-window Granger causality tests conducted during the training phase, enabling the model to exploit statistically validated cross-asset influences. Experiments on 110 assets show that OF-MATNet significantly outperforms both our single-asset baseline (OF-SATNet) and state-of-the-art models such as DeepLOB, BINCTABL, and TLOB. OF-MATNet achieves consistent R2 improvements in over 90\% of cases, with larger gains for assets highly influenced by peers or at longer prediction horizons. Further analysis reveals that temporal attention contributes most to forecasting, but cross-asset and level-wise information are critical in enhancing accuracy. These findings underscore the practical value of modeling nonlinear cross-asset relationships for strategic financial decision-making.","New York, NY, USA",,"Bandealinaeini, Hamidreza and Sharifkhani, Mohammad and Salavati, Erfan",Proceedings of the 6th ACM International Conference on AI in Finance,10.1145/3768292.3770430,9798400722202,,,,"Order Book, Order Flow Imbalance, Mid-Price, High Frequency Trading","
",,,9,525–533,Association for Computing Machinery,ICAIF '25,Attention-Based Multi-Asset Order Flow Networks for Enhanced Mid-Price Prediction,https://doi.org/10.1145/3768292.3770430,,2025
inproceedings,10.1145/3745034.3745131,"Cardiovascular disease remains one of the leading causes of mortality worldwide, with coronary heart disease(CHD) being a predominant form of cardiac pathology. Electrocardiogram (ECG) is widely employed for cardiac disease detection; however, accurate interpretation of ECG signals requires substantial clinical expertise. To address this challenge, this paper proposes a novel approach that combines Butterworth filtering and​continuous wavelet transform (CWT) to extract time-frequency domain features from ECG signals. Subsequently, a self-attention mechanism and contrastive learning framework are utilized to pre-train an ECG time-frequency feature extractor, ultimately enabling the development of a dedicated prediction network for CHD diagnosis. The proposed method is rigorously evaluated using ECG data from the publicly available MIMIC-III dataset. Experimental results demonstrate exceptional performance, achieving 90\% accuracy and 90\% F1-score in CHD prediction, outperforming existing models such as convolutional neural networks (CNNs).","New York, NY, USA",,"Du, Xinyu and He, Jian and Zhang, Chenlong and Zang, Yang",Proceedings of the 4th International Conference on Biomedical and Intelligent Systems,10.1145/3745034.3745131,9798400714399,,,,"Contrastive learning, Coronary heart disease prediction, ECG analysis, Self-attention mechanism, Time-frequency feature fusion","
",,,7,639–645,Association for Computing Machinery,IC-BIS '25,Coronary Heart Disease Prediction Technique Based on Self-Attention Mechanism and Contrastive Learning,https://doi.org/10.1145/3745034.3745131,,2025
inproceedings,10.1145/3705677.3705678,"Accurate prediction of marine time series data is of great practical significance for the protection of marine ecosystems and the sustainable utilization of marine resources. Marine time series data, such as those containing marine parameters such as chlorophyll, turbidity, CDOM, etc., are characterized by high complexity and significant time periodicity. In this paper, oriented to the real ocean time series data prediction scenarios, we design an ocean time series prediction framework based on improved PSO and multi-scale learning for the problems of different inter- and intra-series correlations between ocean time series data on different time scales and the need to set the hyper-parameters individually on different datasets. The framework firstly extracts the significant periodicity in ocean time series data by frequency domain analysis technique and decomposes it to multiple time scales. Secondly, an adaptive graph convolutional network is incorporated to allow correlations between sequences to be independently explored and learned at different time scales, while a self-attention mechanism is incorporated to accurately capture correlations and dependencies within the time series. Finally, an adaptive and dynamic particle swarm optimization algorithm (ANDPSO) is proposed, which reduces the risk of the algorithm falling into a local optimal solution and helps the framework to select the key hyperparameters through the dynamic improvement of inertia weights and learning factors as well as the introduction of population diversity judgment mechanism. Experimental results show that the prediction performance of the framework outperforms the baseline model on a real ocean time series dataset, and ablation experiments are conducted to confirm the indispensability of each component, aiming to provide support for real ocean time series prediction scenarios.","New York, NY, USA",,"Xiao, Yukun and Zhao, Zhigang and Song, Jian and Li, Xiang and Wang, Chunxiao and Zhang, Jian","Proceedings of the 4th International Conference on Computer, Internet of Things and Control Engineering",10.1145/3705677.3705678,9798400711848,,,,"Marine time series data, Ocean time-series prediction, particle swarm optimization","
",,,9,1–9,Association for Computing Machinery,CITCE '24,An Ocean Time Series Prediction Framework Based on Improved PSO with Multiscale Learning,https://doi.org/10.1145/3705677.3705678,,2025
inproceedings,10.1145/3696673.3723071,"With the proliferation of social media platforms, cyber social threats such as fake news, hate speech, and cyberbullying have increased significantly. With the availability of abundant data for building machine learning models, artificial intelligence (AI) is making a deep impact in almost every domain, including detecting and mitigating cyber social threats on social media platforms. As the role of data in AI has significantly intensified in recent years, the concept of data-centric AI has emerged, gradually shifting the research focus from improving model design to enhancing the quality and quantity of data. In this paper, we first present the limitations of model-centric AI in the context of cyber social threats, then discuss the existing literature that uses data-centric AI techniques to detect and mitigate cyber social threats. We finally present the major challenges that social media data presents to deal with cyber social threats using data-centric AI and describe future research opportunities.","New York, NY, USA",,"Kulkarni, Adita",Proceedings of the 2025 ACM Southeast Conference,10.1145/3696673.3723071,9798400712777,,,,"cyber social threats, artificial intelligence, data-centric AI, model-centric AI","Southeast Missouri State University, Cape Girardeau, MO, USA",,,10,85–94,Association for Computing Machinery,ACMSE 2025,Cyber Social Threats: A Data-centric AI Perspective,https://doi.org/10.1145/3696673.3723071,,2025
inproceedings,10.1145/3731715.3733447,"Multimodal emotion recognition in conversations aims to accurately detect emotions by integrating audio, text, and video modalities, playing an important role in various systems. Existing approaches utilize convolutional and recurrent networks to learn short-term emotional information from individual modalities, or employ graph and attention mechanisms to integrate long-term emotional information from multiple modalities. These methods effectively combine emotional information within the conversational content in the time domain.However, psychological research shows that emotional information are not only conveyed in the time domain but also in the frequency domain (e.g., pitch and speech rate). To capture emotions from a more comprehensive perspective, we propose TF-MERC, a framework that integrates both time and frequency domains.TF-MERC uses a multi-domain alignment module to learn modality information within the time or frequency domains. It then employs FATransformer to deeply integrate the multimodal associations between the time and frequency domains, providing a more comprehensive approach for emotion prediction.Experimental results show that TF-MERC outperforms state-of-the-art methods, achieving superior performance across multiple datasets.","New York, NY, USA",,"Cheng, Jiawei and Zhu, Xiaofei and Yang, Zhou",Proceedings of the 2025 International Conference on Multimedia Retrieval,10.1145/3731715.3733447,9798400718779,,,,"emotion recognition, multimodal fusion, nlp","Chicago, IL, USA",,,9,126–134,Association for Computing Machinery,ICMR '25,TF-MERC: Integrating Time-Frequency Information for Multimodal Emotion Recognition in Conversation,https://doi.org/10.1145/3731715.3733447,,2025
inproceedings,10.1145/3594315.3594385,"Wind speed forecasting is still a challenging problem, especially considering the correlations of spatial and temporal domains. However, the changing properties of spatial dependencies over time are ignored in most existing algorithms. In this paper, we propose a novel spatio-temporal machine learning algorithm, named Dynamic Graph Convolutional Transformer (DGCT), for wind speed forecasting. The key contribution of the proposed method is that graph convolutional networks are embedded into self-attention layers of Transformer to capture spatio-temporal correlations to improve the accuracy of forecasting. For the changing properties of spatial dependencies, we model the spatial dependencies as a mixture of global and localized patterns, which are represented by static and dynamic matrices respectively. Moreover, an auxiliary network is designed to generate the dynamic matrix, which further improve the forecasting accuracy. Experiments on two real-world datasets demonstrate that the proposed method outperformed other existing methods consistently.","New York, NY, USA",,"Chang, Xiaodong and Xue, Jiang and Zhao, Jin and Wang, Zhiguo and Tan, Jinxin",Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence,10.1145/3594315.3594385,9781450399029,,,,"Dynamic graph convolutional networks, Spatio-temporal series, Transforme, Wind speed forecasting","Tianjin, China",,,6,644–649,Association for Computing Machinery,ICCAI '23,Dynamic Graph Convolutional Transformer for Short-term Wind Speed Forecasting,https://doi.org/10.1145/3594315.3594385,,2023
inproceedings,10.1145/3746709.3746760,"With the rise of intelligent urban systems, traffic flow prediction has emerged as a vital element within intelligent transportation frameworks. Graph neural networks (GNNs) are crucial for traffic flow prediction due to the non-Euclidean nature of traffic networks. Most GNN-based approaches depend on predefined adjacency matrices, frequently failing to model complex global dependencies. Additionally, these methods primarily use supervised learning, which is highly dependent on data quality. However, traffic data often suffers from missing values caused by sensor malfunctions, hindering effective feature extraction. To overcome these limitations, a novel framework called Adaptive Spatio-Temporal Graph-enhanced Contrastive Learning for Traffic Flow Prediction (ASTGCL) is introduced to improve prediction performance. Specifically, ASTGCL implements two data augmentation strategies: first, it employs an adaptive, learnable adjacency matrix to enhance the predefined matrix, enabling the model to capture both local and global topological features; second, it integrates three traffic data augmentation techniques to reduce the influence of data noise on prediction accuracy. The enhanced adjacency matrix and augmented traffic data are then utilized in a spatio-temporal contrastive learning process to extract higher-order spatio-temporal features from the traffic flow data. Experiments conducted on four real-world datasets reveal that ASTGCL surpasses baseline models, confirming the efficacy of the proposed framework.","New York, NY, USA",,"Wu, Zhendong and Yu, Weihao and Zhang, Tinghua and Huang, Jin",Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications,10.1145/3746709.3746760,9798400713163,,,,"Contrastive Learning, Data augmentation, Traffic flow prediction","
",,,7,296–302,Association for Computing Machinery,CIBDA '25,ASTGCL: Adaptive Spatio-Temporal Graph-enhanced Contrastive Learning for Traffic Flow Prediction,https://doi.org/10.1145/3746709.3746760,,2025
inproceedings,10.1145/3447548.3467419,"Neural ordinary differential equations (NODEs) presented a new paradigm to construct (continuous-time) neural networks. While showing several good characteristics in terms of the number of parameters and the flexibility in constructing neural networks, they also have a couple of well-known limitations: i) theoretically NODEs learn homeomorphic mapping functions only, and ii) sometimes NODEs show numerical instability in solving integral problems. To handle this, many enhancements have been proposed. To our knowledge, however, integrating attention into NODEs has been overlooked for a while. To this end, we present a novel method of attentive dual co-evolving NODE (ACE-NODE): one main NODE for a downstream machine learning task and the other for providing attention to the main NODE. Our ACE-NODE supports both pairwise and elementwise attention. In our experiments, our method outperforms existing NODE-based and non-NODE-based baselines in almost all cases by non-trivial margins.","New York, NY, USA",,"Jhin, Sheo Yon and Jo, Minju and Kong, Taeyong and Jeon, Jinsung and Park, Noseong",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467419,9781450383325,,,,"neural networks, neural ordinary differential equations","Virtual Event, Singapore",,,10,736–745,Association for Computing Machinery,KDD '21,ACE-NODE: Attentive Co-Evolving Neural Ordinary Differential Equations,https://doi.org/10.1145/3447548.3467419,,2021
inproceedings,10.1145/3580305.3599285,"Event forecasting has been a demanding and challenging task throughout the entire human history. It plays a pivotal role in crisis alarming and disaster prevention in various aspects of the whole society. The task of event forecasting aims to model the relational and temporal patterns based on historical events and makes forecasting to what will happen in the future. Most existing studies on event forecasting formulate it as a problem of link prediction on temporal event graphs. However, such pure structured formulation suffers from two main limitations: 1) most events fall into general and high-level types in the event ontology, and therefore they tend to be coarse-grained and offers little utility which inevitably harms the forecasting accuracy; and 2) the events defined by a fixed ontology are unable to retain the out-of-ontology contextual information.To address these limitations, we propose a novel task of context-aware event forecasting which incorporates auxiliary contextual information. First, the categorical context provides supplementary fine-grained information to the coarse-grained events. Second and more importantly, the context provides additional information towards specific situation and condition, which is crucial or even determinant to what will happen next. However, it is challenging to properly integrate context into the event forecasting framework, considering the complex patterns in the multi-context scenario. Towards this end, we design a novel framework named Separation and Collaboration Graph Disentanglement (short as SeCoGD) for context-aware event forecasting. In the separation stage, we leverage the context as a prior guidance to disentangle the event graph into multiple sub-graphs, followed by a context-specific module to model the relational-temporal patterns within each context. In the collaboration stage, we design a cross-context module to retain the collaborative associations among multiple contexts. Since there is no available dataset for this novel task, we construct three large- scale datasets based on GDELT. Experimental results demonstrate hat our model outperforms a list of SOTA methods. The dataset and code are released via https://github.com/yecchen/SeCoGD.","New York, NY, USA",,"Ma, Yunshan and Ye, Chenchen and Wu, Zijian and Wang, Xiang and Cao, Yixin and Chua, Tat-Seng",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599285,9798400701030,,,,"graph disentanglement, graph neural network, temporal event forecasting, temporal knowledge graph","Long Beach, CA, USA",,,10,1643–1652,Association for Computing Machinery,KDD '23,Context-aware Event Forecasting via Graph Disentanglement,https://doi.org/10.1145/3580305.3599285,,2023
inproceedings,10.1145/3679240.3734633,"Smart energy meters generate large volumes of fine-grained time series data that captures building’s energy consumption patterns. This data can be leveraged to detect anomalous energy consumption patterns and reduce energy waste in buildings. Traditional anomaly detection methods for smart meter data rely on statistical and machine learning techniques, which often struggle to model complex temporal patterns, require extensive feature engineering, and have poor scalability. Foundation models for time series, which are pretrained on massive volumes of time series data from diverse domains, have recently emerged as a versatile and scalable tool for time series analysis. They are capable of handling multiple tasks, including anomaly detection, and demonstrating superior performance compared to traditional models in various domains. Despite their potential for cross-domain applications, their applicability to the energy domain and their performance compared to traditional machine learning models remain largely unexplored.Therefore, in this paper, we analyze the applicability and performance of Time Series Foundation Models (TSFMs) for unsupervised energy anomaly detection. Specifically, we compare the performance of two widely used state-of-the-art TSFMs, TimeGPT and MOMENT, against existing anomaly detection techniques in the literature: (a) two statistical methods (Interquartile Range (IQR) and Modified Z-Score (mZ-Score)), (b) two unsupervised machine learning techniques (Isolation Forest and Local Outlier Factor), and (c) a deep learning-based technique, Variational Autoencoder (VAE). Our experimental results, conducted using the LEAD 1.0 dataset, which consists of annotated hourly energy readings of 200 buildings, show that MOMENT outperforms both traditional statistical methods and unsupervised machine learning methods. Our results reveal that fine-tuning of MOMENT marginally improves its performance. VAE trained from scratch surpasses TSFMs in performance despite having a smaller model size. We also analyze the trade-off between performance, scalability, and compute requirements of these models. Our analysis also provides new research directions for using TSFMs in the energy domain.","New York, NY, USA",,"Hela, Basu and Handigol, Praveen Prasad and Arjunan, Pandarasamy",Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems,10.1145/3679240.3734633,9798400711251,,,,"Foundation Models, Anomaly Detection, Time Series Analysis, Transformer Architecture, Deep Learning, Transfer Learning, Self-supervised Learning","
",,,10,656–665,Association for Computing Machinery,E-Energy '25,Are Time Series Foundation models good for Energy Anomaly Detection?,https://doi.org/10.1145/3679240.3734633,,2025
inproceedings,10.1145/3688671.3688745,"In this paper, we introduce TransChem, a novel architecture designed to tackle a wide range of downstream tasks in chemistry. TransChem leverages a pre-train fine-tune scheme. For pre-training, we utilize 2 million molecules from the ZINC15 database. We evaluate TransChem on six benchmark datasets from MoleculeNet, demonstrating its superior performance over state-of-the-art methods that employ pre-training and models that rely on hand-crafted features. We demonstrate that properly designed pre-training objectives can steer the learning process and allow TransChem to capture subtle relationships between atoms and generate informative representations, even when the pre-training dataset is relatively small.","New York, NY, USA",49,"Kelesis, Dimitrios and Spyropoulou, Eirini and Zavitsanos, Elias",Proceedings of the 13th Hellenic Conference on Artificial Intelligence,10.1145/3688671.3688745,9798400709821,,,,"learning on graphs, Transformers, gMLP, pre-training, GNNs","
",,,8,,Association for Computing Machinery,SETN '24,TransChem: Effective Pre-training Enhances Molecular Property Prediction,https://doi.org/10.1145/3688671.3688745,,2024
article,10.14778/3654621.3654637,"Time series data, including univariate and multivariate ones, are characterized by unique composition and complex multi-scale temporal variations. They often require special consideration of decomposition and multi-scale modeling to analyze. Existing deep learning methods on this best fit to univariate time series only, and have not sufficiently considered sub-series modeling and decomposition completeness. To address these challenges, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer, which learns to explicitly decompose and represent the input time series in its different layers. To handle the multi-scale temporal patterns and multivariate dependencies, we propose a novel temporal patching approach to model the time series as multi-scale patches, and employ MLPs to capture intra- and inter-patch variations and channel-wise correlations. In addition, we propose a novel loss function to constrain both the mean and the autocorrelation of the decomposition residual for better decomposition completeness. Through extensive experiments on various real-world datasets for five common time series analysis tasks, we demonstrate that MSD-Mixer consistently and significantly outperforms other state-of-the-art algorithms with better efficiency.",,,"Zhong, Shuhan and Song, Sizhe and Zhuo, Weipeng and Li, Guanyao and Liu, Yang and Chan, S.-H. Gary",,10.14778/3654621.3654637,,2150-8097,March 2024,Proc. VLDB Endow.,,,,7,14,1723–1736,VLDB Endowment,,A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis,https://doi.org/10.14778/3654621.3654637,17,2024
inproceedings,10.1145/3637528.3671968,"Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the &lt;u&gt;D&lt;/u&gt;ynamics-&lt;u&gt;I&lt;/u&gt;nvariant &lt;u&gt;Sk&lt;/u&gt;eleton Neural &lt;u&gt;Net&lt;/u&gt;work (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18\% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet.","New York, NY, USA",,"Li, Ruikun and Wang, Huandong and Piao, Jinghua and Liao, Qingmin and Li, Yong",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671968,9798400704901,,,,"complex network, dynamical systems, graph neural networks, neural ODE","Barcelona, Spain",,,12,1655–1666,Association for Computing Machinery,KDD '24,Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton in Hyperbolic Space,https://doi.org/10.1145/3637528.3671968,,2024
inproceedings,10.1145/3746027.3754810,"This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.2\% accuracy on semantic-level classification and 42.3\% in word-level classification, outperforming baseline methods substantially. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.","New York, NY, USA",,"Zhou, Jinzhao and Cao, Zehong and Duan, Yiqun and Barkley, Connor and Leong, Daniel and Jiang, Xiaowei and Nguyen, Quoc-Toan and Zhao, Ziyi and Do, Thomas and Chang, Yu-Cheng and Liang, Sheng-Fu and Lin, Chin-Teng",Proceedings of the 33rd ACM International Conference on Multimedia,10.1145/3746027.3754810,9798400720352,,,,"active brain-computer-interface, auto-regressive pretraining, eeg silent speech, large brain language model","Dublin, Ireland",,,10,5883–5892,Association for Computing Machinery,MM '25,Pretraining Large Brain Language Model for Active BCI: Silent Speech,https://doi.org/10.1145/3746027.3754810,,2025
article,10.1145/3721435,"Forecasting graph-based, time-dependent data has broad practical applications but presents challenges. Effective models must capture both spatial and temporal dependencies in the data, while also incorporating auxiliary information to enhance prediction accuracy. In this article, we identify limitations in current state-of-the-art models regarding temporal dependency handling. To overcome this, we introduce GSA-Forecaster, a new deep learning model designed for forecasting in graph-based, time-dependent contexts. GSA-Forecaster utilizes graph sequence attention, a new attention mechanism proposed in this article, to effectively manage temporal dependencies. GSA-Forecaster integrates the data’s graph structure directly into its architecture, addressing spatial dependencies. Additionally, it incorporates auxiliary information to refine its predictions further. We validate its performance using real-world graph-based, time-dependent datasets, where it demonstrates superior effectiveness compared to existing state-of-the-art models.","New York, NY, USA",82,"Li, Yang and Wang, Di and Moura, Jos\'{e",,10.1145/3721435,,1556-4681,May 2025,ACM Trans. Knowl. Discov. Data,"spatial dependency, temporal dependency, graph sequence attention, Transformer, GSA-Forecaster",,,4,26,,Association for Computing Machinery,,Forecasting Graph-Based Time-Dependent Data with Graph Sequence Attention,https://doi.org/10.1145/3721435,19,2025
article,10.1145/3688393,"Human activity recognition (HAR) is an active research field that has seen great success in recent years due to advances in sensory data collection methods and activity recognition systems. Deep artificial intelligence (AI) models have contributed to the success of HAR systems lately, although still suffering from limitations such as data scarcity, the high costs of labelling data instances, and datasets’ imbalance and bias. The temporal nature of human activity data, represented as time series data, impose an additional challenge to using AI models in HAR, because most state-of-the-art models do not account for the time component of the data instances. These limitations have inspired the time-series research community to design generative models for sequential data, but very little work has been done to evaluate the quality of such models. In this work, we conduct a comparative quality analysis of three generative models for time-series data, using a case study in which we aim to generate sensory human activity data from a seed public dataset. Additionally, we adapt and clearly explain four evaluation methods of synthetic time-series data from the literature and apply them to assess the quality of the synthetic activity data we generate. We show experimentally that high-quality human activity data can be generated using deep generative models, and the synthetic data can thus be used in HAR systems to augment real activity data. We also demonstrate that the chosen evaluation methods effectively ensure that the generated data meets the essential quality benchmarks of realism, diversity, coherence, and utility. Our findings suggest that using deep generative models to produce synthetic human activity data can potentially address challenges related to data scarcity, biases, and expensive labeling. This holds promise for enhancing the efficiency and reliability of HAR systems.","New York, NY, USA",18,"Alzahrani, Naif and Ca\l{",,10.1145/3688393,,1936-1955,September 2024,J. Data and Information Quality,"Human activity recognition, multivariate time series, generative modeling",,,3,18,,Association for Computing Machinery,,Experience: A Comparative Analysis of Multivariate Time-Series Generative Models: A Case Study on Human Activity Data,https://doi.org/10.1145/3688393,16,2024
inproceedings,10.1145/3746027.3754842,"Real-world time series typically exhibit complex temporal variations, making the time series classification task notably challenging. Recent advancements have demonstrated the potential of multi-scale analysis approaches, which provide an effective solution for capturing these complex temporal patterns. However, existing multi-scale analysis-based time series prediction methods fail to eliminate redundant scale-shared features across multi-scale time series, resulting in the model over- or under-focusing on scale-shared features. To address this issue, we propose a novel end-to-end Disentangled Multi-Scale framework for Time Series classification (DisMS-TS). The core idea of DisMS-TS is to eliminate redundant shared features in multi-scale time series, thereby improving prediction performance. Specifically, we propose a temporal disentanglement module to capture scale-shared and scale-specific temporal representations, respectively. Subsequently, to effectively learn both scale-shared and scale-specific temporal representations, we introduce two regularization terms that ensure the consistency of scale-shared representations and the disparity of scale-specific representations across all temporal scales. Extensive experiments conducted on multiple datasets validate the superiority of DisMS-TS over its competitive baselines, with the accuracy improvement up to 9.71\%.","New York, NY, USA",,"Liu, Zhipeng and Duan, Peibo and Wang, Binwu and Tang, Xuan and Chu, Qi and Zhang, Changsheng and Huang, Yongsheng and Zhang, Bin",Proceedings of the 33rd ACM International Conference on Multimedia,10.1145/3746027.3754842,9798400720352,,,,"feature disentanglement, multi-scale analysis, time series","Dublin, Ireland",,,10,10817–10826,Association for Computing Machinery,MM '25,DisMS-TS: Eliminating Redundant Multi-scale Features for Time Series Classification,https://doi.org/10.1145/3746027.3754842,,2025
inproceedings,10.1145/3637528.3671507,"Rapid urbanization has significantly escalated traffic congestion, underscoring the need for advanced congestion prediction services to bolster intelligent transportation systems. As one of the world's largest ride-hailing platforms, DiDi places great emphasis on the accuracy of congestion prediction to enhance the effectiveness and reliability of their real-time services, such as travel time estimation and route planning. Despite numerous efforts have been made on congestion prediction, most of them fall short in handling heterogeneous and dynamic spatio-temporal dependencies (e.g., periodic and non-periodic congestions), particularly in the presence of noisy and incomplete traffic data. In this paper, we introduce a Congestion Prediction Mixture-of-Experts, CP-MoE, to address the above challenges. We first propose a sparsely-gated Mixture of Adaptive Graph Learners (MAGLs) with congestion-aware inductive biases to improve the model capacity for efficiently capturing complex spatio-temporal dependencies in varying traffic scenarios. Then, we devise two specialized experts to help identify stable trends and periodic patterns within the traffic data, respectively. By cascading these experts with MAGLs, CP-MoE delivers congestion predictions in a more robust and interpretable manner. Furthermore, an ordinal regression strategy is adopted to facilitate effective collaboration among diverse experts. Extensive experiments on real-world datasets demonstrate the superiority of our proposed method compared with state-of-the-art spatio-temporal prediction models. More importantly, CP-MoE has been deployed in DiDi to improve the accuracy and reliability of the travel time estimation system.","New York, NY, USA",,"Jiang, Wenzhao and Han, Jindong and Liu, Hao and Tao, Tao and Tan, Naiqiang and Xiong, Hui",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671507,9798400704901,,,,"congestion prediction, mixture-of-experts, spatiotemporal modeling","Barcelona, Spain",,,12,5206–5217,Association for Computing Machinery,KDD '24,Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction,https://doi.org/10.1145/3637528.3671507,,2024
inproceedings,10.1145/3711650.3711665,"Ebola Virus Disease (EVD) is a highly contagious and fatal disease that poses a serious threat to public health and security. Accurate and timely forecasting of EVD outbreaks is essential for effective prevention and control measures. However, traditional epidemiological models often fail to capture the complex and dynamic nature of human mobility, which plays a key role in EVD transmission. In this paper, we propose a novel framework for EVD forecasting based on spatio-temporal feature learning called ST-FLAM. We use various types of mobility data, such as phone records, Global Positioning System (GPS) traces, and social media posts, to extract meaningful and representative features that reflect the mobility patterns of individuals and populations. Next, we employ ST-FLAM architectures, which incorporate Graph Neural Networks (GNN) and Long Short Term Memory (LSTM), to establish connections and dependencies between mobility features and EVD cases in both space and time. We evaluate the performance of our framework on real-world datasets from the 2014–2016 West Africa EVD outbreak and the 2015–2016 EVD and human mobility in Sierra Leone. We compare our framework with baseline methods to handle traditional epidemiological challenges during an outbreak. We conduct ablation studies and analyse the impact of different mobility data sources, feature extraction methods, and deep learning architectures on EVD forecasting accuracy. Our results show that our framework outperforms the baselines and achieves state-of-the-art performance in EVD forecasting. We also demonstrate that our framework can provide interpretable and actionable insights for EVD prevention and control.","New York, NY, USA",,"Fofanah, Abdul Joseph and Sankoh, Albert Patrick and Dumbuya, Ibrahim and Kamara, Alpha Alimamy and Conteh, Zachariyah Bai","Proceedings of the 2024 13th International Conference on Networks, Communication and Computing",10.1145/3711650.3711665,9798400717352,,,,"Time Series, Ebola Virus Disease, Deep Learning, Spatio-Temporal, Population Mobility","
",,,9,98–106,Association for Computing Machinery,ICNCC '24,ST-FLAM: Evaluating Performance of Deep Learning Models on Mobility Patterns for EVD Forecasting based on Spatio-Temporal Feature Learning,https://doi.org/10.1145/3711650.3711665,,2025
article,10.1145/3735646,"Accurate prediction of Sea Surface Temperature (SST) is of high importance in marine science, benefiting applications ranging from ecosystem protection to extreme weather forecasting and climate analysis. Wide-area SST usually shows diverse SST patterns in different sea areas due to the changes of temperature zones and the dynamics of ocean currents. However, existing studies on SST prediction often focus on small-area predictions and lack the consideration of diverse SST patterns. Furthermore, SST shows an annual periodicity, but the periodicity is not strictly adherent to an annual cycle. Existing SST prediction methods struggle to adapt to this non-strict periodicity. To address these two issues, we proposed the Cross-Region Graph Convolutional Network with Periodicity Shift Adaptation (RGCN-PSA) model which is equipped with the Cross-Region Graph Convolutional Network module and the Periodicity Shift Adaption module. The Cross-Region Graph Convolutional Network module enhances wide-area SST prediction by learning and incorporating diverse SST patterns. Meanwhile, the periodicity Shift Adaptation module accounts for the annual periodicity and enable the model to adapt to the possible temporal shift automatically. We conduct experiments on two real-world SST datasets, and the results demonstrate that our RGCN-PSA model obviously outperforms baseline models in terms of prediction accuracy. The code of RGCN-PSA model is available at .","New York, NY, USA",87,"Peng, Han and Li, Wengen and Jin, Chang and Zhang, Yichao and Guan, Jihong and Yang, Hanchen and Zhou, Shuigeng",,10.1145/3735646,,2157-6904,August 2025,ACM Trans. Intell. Syst. Technol.,"Wide-area SST prediction, graph neural network, diverse SST patterns, periodicity shift adaptation",,,4,23,,Association for Computing Machinery,,Cross-Region Graph Convolutional Network with Periodicity Shift Adaptation for Wide-Area SST Prediction,https://doi.org/10.1145/3735646,16,2025
article,10.1145/3773084,"Machine Learning (ML) workflows—spanning data preprocessing and feature engineering, model selection and hyperparameter optimization, and workflow evaluation—are increasingly embedded in complex software systems. Building these workflows manually demands substantial ML expertise, domain knowledge, and engineering effort. Automated ML (AutoML) frameworks address parts of this challenge but often suffer from constrained search spaces, limited adaptability, and low interpretability. Recent advances in Large Language Models (LLMs) have opened new opportunities to automate and enhance ML workflows by leveraging their capabilities in language understanding, reasoning, interaction, and code generation, posing new practical and theoretical challenges for software engineering (SE). This survey provides the first SE-oriented, stage-wise review of LLM-based ML workflow automation. We introduce a taxonomy covering all three workflow stages, systematically compare and analyze state-of-the-art methods, and synthesize both stage-specific and cross-stage trends. Our analysis yields SE-oriented implications, including the need for robust verification, quality management, context-aware deployment, and risk mitigation, alongside ensuring key quality attributes such as usability, modularity, traceability, and performance. The findings also call for adapting development models, rethinking lifecycle boundaries, and formalizing uncertainty handling to address the probabilistic and collaborative nature of LLM-assisted workflow generation. We further identify major open challenges and outline future research directions to guide the reliable and effective adoption of LLMs in ML workflow development. Our artifacts are publicly available at .","New York, NY, USA",,"Gu, Yang and You, Hengyu and Cao, Jian and Yu, Muran and Fan, Haoran and Qian, Shiyou",,10.1145/3773084,,1049-331X,,ACM Trans. Softw. Eng. Methodol.,"Machine Learning Workflows, Large Language Models, Software Engineering, AutoML, Survey",,Just Accepted,,,,Association for Computing Machinery,,Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey,https://doi.org/10.1145/3773084,,2025
article,10.1145/3759440,"This research aims to develop a novel framework that uncovers the causal influence of global events on public sentiment through temporal graph modeling and neural causal inference. Global events, such as pandemics, elections, and economic crises, profoundly affect public sentiments, shaping social behaviors and economic outcomes. Traditional models often fall short in capturing the complex, dynamic, and non-linear relationships between these events and sentiments. This article presents the Neural Temporal Causal Graph Network (NTCGN), a unified framework that integrates temporal graph neural networks with a Causal Attention Network (CAN) to model and interpret these relationships. NTCGN constructs a temporal graph from event data and sentiment-labeled texts, learning dependencies and causal influences through advanced neural architectures. A thorough comparative analysis with state-of-the-art models such as Logistic Regression, SVM, LSTM, and transformer-based models demonstrates NTCGN’s superior performance. Experimental evaluation using the Sentiment140 and Global Database of Events, Language and Tone (GDELT) 2.0 datasets shows NTCGN achieving an accuracy of 0.798 and an F1 score of 0.795, outperforming these baseline models. The model’s causal inference capabilities are validated using the Causal Impact Score (CIS) and Causal Discovery Precision (CDP), highlighting its reliability in identifying true causal links. Visualizations of attention maps and causal pathways enhance interpretability, demonstrating how specific events influence public sentiments. This work provides a robust and interpretable tool for analyzing event-driven sentiment dynamics in real-world applications.","New York, NY, USA",47,"E, Subha and V, Jothi Prakash and S, Arul Antran Vijay",,10.1145/3759440,,1559-1131,November 2025,ACM Trans. Web,"Temporal graph neural networks, causal inference, sentiment analysis, event-sentiment dynamics, attention mechanisms, causal graphs, deep learning, interpretability",,,4,31,,Association for Computing Machinery,,A Graph-Based Framework for Temporal and Causal Analysis of Sentiments,https://doi.org/10.1145/3759440,19,2025
inproceedings,10.1145/3627673.3679751,"Continuous-time dynamics models, e.g., neural ordinary differential equations, enable accurate modeling of underlying dynamics in time-series data. However, employing neural networks for parameterizing dynamics makes it challenging for humans to identify dependence structures, especially in the presence of delayed effects. In consequence, these models are not an attractive option when capturing dependence carries more importance than accurate modeling, e.g., in tsunami forecasting.In this paper, we present a novel method for identifying dependence structures in continuous-time dynamics models. We take a two-step approach: (1) During training, we promote weight sparsity in the model's first layer during training. (2) We prune the sparse weights after training to identify dependence structures. In evaluation, we test our method in scenarios where the exact dependence-structures of time-series are known. Compared to baselines, our method is more effective in uncovering dependence structures in data even when there are delayed effects. Moreover, we evaluate our method to a real-world tsunami forecasting, where the exact dependence structures are unknown beforehand. Even in this challenging scenario, our method still effective learns physically-consistent dependence structures and achieves high accuracy in forecasting.","New York, NY, USA",,"Wu, Fan and Cho, Woojin and Korotky, David and Hong, Sanghyun and Rim, Donsub and Park, Noseong and Lee, Kookjin",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679751,9798400704369,,,,"causality learning, neural ordinary differential equations, tsunami modeling","Boise, ID, USA",,,10,2534–2543,Association for Computing Machinery,CIKM '24,Identifying Contemporaneous and Lagged Dependence Structures by Promoting Sparsity in Continuous-time Neural Networks,https://doi.org/10.1145/3627673.3679751,,2024
inproceedings,10.1145/3627673.3679610,"Next Set Recommendation (NSRec), encompassing related tasks such as next basket recommendation and temporal sets prediction, stands as a trending research topic. Although numerous attempts have been made on this topic, there are certain drawbacks: (i) Existing studies are still confined to utilizing objective functions commonly found in Next Item Recommendation (NIRec), such as binary cross entropy and BPR, which are calculated based on individual item comparisons; (ii) They place emphasis on building sophisticated learning models to capture intricate dependency relationships across sequential sets, but frequently overlook pivotal dependency in their objective functions; (iii) Diversity factor within sequential sets is frequently overlooked. In this research, we endeavor to unveil a universal and Sets-level optimization framework for Next Set Recommendation (SNSRec), offering a holistic fusion of diversity distribution and intricate dependency relationships within temporal sets. To realize this, the following contributions are made: (i) We directly model the temporal set in a sequence as a cohesive entity, leveraging the Structured Determinantal Point Process (SDPP), wherein the probabilistic DPP distribution prioritizes collections of structures (sequential sets) instead of individual items; (ii) We introduce a co-occurrence representation to discern and acknowledge the importance of different sets; (iii) We propose a sets-level optimization criterion, which integrates the diversity distribution and dependency relations across the entire sequence of sets, guiding the model to recommend relevant and diversified set. Extensive experiments on real-world datasets show that our approach consistently outperforms previous methods on both relevance and diversity.","New York, NY, USA",,"Liu, Yuli and Liu, Min and Walder, Christian and Xie, Lexing",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679610,9798400704369,,,,"next set prediction, optimization approach, sdpps","Boise, ID, USA",,,11,1544–1554,Association for Computing Machinery,CIKM '24,A Universal Sets-level Optimization Framework for Next Set Recommendation,https://doi.org/10.1145/3627673.3679610,,2024
article,10.1145/3749156,"With recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their ability to retrieve graph signals in the spectral domain. These models feature uniqueness in efficient computation as well as rich expressiveness, which stems from advanced management and profound understanding of graph data. However, few systematic studies have been conducted to assess spectral GNNs, particularly in benchmarking their efficiency, memory consumption, and effectiveness in a unified and fair manner. There is also a pressing need to select spectral models suitable for learning specific graph data and deploying them to massive web-scale graphs, which is currently constrained by the varied model designs and training settings.In this work, we extensively benchmark spectral GNNs with a focus on the spectral perspective, demystifying them as spectral graph filters. We analyze and categorize 35 GNNs with 27 corresponding filters, spanning diverse formulations and utilizations of the graph data. Then, we implement the filters within a unified spectral-oriented framework with dedicated graph computations and efficient training schemes. In particular, our implementation enables the deployment of spectral GNNs over million-scale graphs and various tasks with comparable performance and less overhead. Thorough experiments are conducted on the graph filters with comprehensive metrics on effectiveness and efficiency, offering novel observations and practical guidelines that are only available from our evaluations across graph scales. Different from the prevailing belief, our benchmark reveals an intricate landscape regarding the effectiveness and efficiency of spectral graph filters, demonstrating the potential to achieve desirable performance through tailored spectral manipulation of graph data.","New York, NY, USA",238,"Liao, Ningyi and Liu, Haoyu and Zhu, Zulun and Luo, Siqiang and Lakshmanan, Laks V.S.",,10.1145/3749156,,,September 2025,Proc. ACM Manag. Data,"efficiency and scalability, graph neural networks, graph spectrum",,,4,29,,Association for Computing Machinery,,"A Comprehensive Benchmark on Spectral GNNs: The Impact on Efficiency, Memory, and Effectiveness",https://doi.org/10.1145/3749156,3,2025
inproceedings,10.1145/3589132.3625641,"Although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. Urban flow, a typical kind of geospatial data, is critical for a wide range of applications from public safety and traffic management to urban planning. Existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. Some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. In this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. To capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. We first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which we design a knowledge-enhanced spatio-temporal diffusion model (KSTDiff) to generate urban flow for each region. Specifically, to accurately generate urban flow for regions with different flow volumes, we design a novel diffusion process guided by a volume estimator, which is learnable and customized for each region. Moreover, we propose a knowledge-enhanced denoising network to capture the spatio-temporal dependencies of urban flow as well as the impact of urban environment in the denoising process. Extensive experiments on four real-world datasets validate the superiority of our model over state-of-the-art baselines in urban flow generation. Further in-depth studies demonstrate the utility of generated urban flow data and the ability of our model for long-term flow generation and urban flow prediction. Our code is released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.","New York, NY, USA",91,"Zhou, Zhilun and Ding, Jingtao and Liu, Yu and Jin, Depeng and Li, Yong",Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems,10.1145/3589132.3625641,9798400701689,,,,"generative model, urban flow, knowledge graph, diffusion model","Hamburg, Germany",,,12,,Association for Computing Machinery,SIGSPATIAL '23,Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion,https://doi.org/10.1145/3589132.3625641,,2023
inproceedings,10.1145/3748777.3748784,"Accurate vessel trajectory prediction facilitates improved navigational safety, routing, and environmental protection. However, existing prediction methods are challenged by the irregular sampling time intervals of the vessel tracking data from the global AIS system and the complexity of vessel movement. These aspects complicate model learning and generalization. To address these challenges and improve vessel trajectory prediction, we propose Multi-modAl Knowledge-Enhanced fRamework (MAKER) for vessel trajectory prediction. To contend better with the irregular sampling time intervals, MAKER features a Large language model-guided Knowledge Transfer (LKT) module that leverages pre-trained language models to transfer trajectory-specific contextual knowledge effectively. To enhance the ability to learn complex trajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning (KSL) module. This module employs kinematic knowledge to progressively integrate complex patterns during training, allowing for adaptive learning and enhanced generalization. Experimental results on two vessel trajectory datasets show that MAKER can improve the prediction accuracy of state-of-the-art methods by 12.08\%—17.86\%.","New York, NY, USA",,"Yu, Haomin and Li, Tianyi and Torp, Kristian and Jensen, Christian S.",Proceedings of the 19th International Symposium on Spatial and Temporal Data,10.1145/3748777.3748784,,,,,"Trajectory prediction, self-paced learning, vessel trajectory, large language model","
",,,11,44–54,Association for Computing Machinery,SSTD '25,A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction,https://doi.org/10.1145/3748777.3748784,,2025
inproceedings,10.1145/3746252.3761182,"Dynamic link prediction aims to predict the future links in dynamic graphs. Existing generative dynamic link prediction studies utilize the global degree distribution for mitigating the over-estimation problem, which can model the time-invariant features while neglecting the time-varying features, resulting in capturing inaccurate evolution patterns. However, such time related features are intrinsically coupled, which makes simultaneously and independently modeling both features infeasible. Motivated by these issues, we propose a Time-wise causal debiasing framework (Tide) for generative dynamic link prediction, which does not resort to any extra trainable modules. Instead, to obtain the time-invariant features, we first utilize a time-invariant deconfounded learning mechanism for decoupling the prediction score with the degree distribution. To leverage the time-varying features, we intervene in the model during the inference stage by a predicted future degree distribution, aiming to make the accurate predictions for dynamic graphs. Experiments conducted on four public datasets under both inductive and transductive settings present that our Tide enhanced models can outperform their corresponding vanilla versions by up to 21.42\% and 27.73\% in terms of NDCG and Jaccard, respectively.","New York, NY, USA",,"Zhang, Xin and Zheng, Jianming and Cai, Fei and Pan, Zhiqiang and Chen, Wanyu and Chen, Chonghao and Chen, Honghui",Proceedings of the 34th ACM International Conference on Information and Knowledge Management,10.1145/3746252.3761182,9798400720406,,,,"causal inference, dynamic link prediction, generative model","Seoul, Republic of Korea",,,10,4232–4241,Association for Computing Machinery,CIKM '25,Tide: A Time-Wise Causal Debiasing Framework for Generative Dynamic Link Prediction,https://doi.org/10.1145/3746252.3761182,,2025
inproceedings,10.1145/3578741.3578757,"In practice, accurate and timely forecasting of short-term intense rainfall is critical, but the problem is extremely difficult because to its complicated spatial-temporal association. Although several spatial-temporal series forecasting methods have been used to rainfall prediction, these models continue to suffer from inadequate modeling of data’s complicated intrinsic connection. We provide a new short-term intense rainfall prediction model that use two graph generators to model data correlations under distinct semantics, followed by a graph convolution module for information integration to fully extract data spatial-temporal information. Finally, a variant of recurrent neural network is employed to extract the temporal dependence. The experimental results on both datasets show that the model can model the spatial and temporal dependence across the data more effectively than the baseline model, and further improve the model’s predictive performance for short-term intense rainfall.","New York, NY, USA",,"Xie, Huo Shen and Wang, Weijie",Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing,10.1145/3578741.3578757,9781450399067,,,,"Graph Convolution, Short-term intense rainfall, spatial-temporal correlation","Sanya, China",,,7,74–80,Association for Computing Machinery,MLNLP '22,Long Short-term Dynamic Graph Neural Networks: for short-term intense rainfall forecasting,https://doi.org/10.1145/3578741.3578757,,2023
inproceedings,10.1145/3603781.3603907,"Abstract—Traffic flow prediction is of great importance in applications such as traffic management and urban planning. The complex spatial and temporal dependence of traffic flow between different roads poses a great challenge for accurate real-time traffic flow prediction. Traditional traffic flow prediction methods rely on the assumption of data smoothness, and the prediction accuracy decreases significantly in the face of complex, variable and large amount of traffic flow data. Spatio-temporal prediction models based on graph neural networks and recurrent neural networks can achieve better prediction accuracy, but there are still some problems, such as the need for a known static graph structure, inadequate spatial extraction and long training time of the model. To improve traffic flow prediction accuracy and real-time performance, this paper proposes a novel end-to-end deep learning framework called graph attention echo state network (GAESN), which uses attention mechanism and echo state network to extract spatio-temporal features. Experimental results on four real traffic flow datasets show that our proposed model achieves 17.35, 21.34, 24.12 and 17.31 in mean absolute error(MAE); 29.31, 32.67, 37.51and 26.84 in root mean square error(RMSE); 16.76\%, 15.44\%, 10.33\% and 10.94\% in mean absolute percentage error(MAPE), respectively. Compared with other existing models, this model reduces the number of parameters to be trained and the time required for model training, and also improves the accuracy of traffic flow prediction.","New York, NY, USA",,"Gan, Tian and Xu, Beining and Li, Jin","Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things",10.1145/3603781.3603907,9798400700705,,,,"Echo state network, Graph attention network, Multi-headed attention mechanism, Spatio-temporal dependence, Traffic prediction","Xiamen, China",,,6,708–713,Association for Computing Machinery,CNIOT '23,Transportation Flow Prediction Based on Graph Attention Echo State Network,https://doi.org/10.1145/3603781.3603907,,2023
inproceedings,10.1145/3589883.3589894,"Accurate traffic flow prediction is a keystone for building intelligent traffic management systems which have gained attention from researchers because of the availability of the massive volume of traffic data and advances in deep learning technologies. However, there are many cities in the world, that suffer from terrible traffic congestion but there are no infrastructure facilities to collect traffic data. To address this problem we develop a tool that collects traffic data from Google Maps without using its paid API. After that, we proposed an Attention-based Deep Hybrid network (ADHN) for traffic flow prediction using Google map data. The proposed ADHN combines two Convolutional Long Short-Term Memory (ConvLSTM) to capture dynamic spatial temporal dependencies of the traffic flow and applies attention mechanism on traffic features. The experiment result shows that our proposed ADHN can provide higher prediction accuracy compared with the other state-of-the-art approaches. Our code and data are available at https://github.com/Moshiurcse13/trafficDataCollectionTool.","New York, NY, USA",,"Rahman, Md. Moshiur and Nower, Naushin",Proceedings of the 2023 8th International Conference on Machine Learning Technologies,10.1145/3589883.3589894,9781450398329,,,,"Attention Mechanism, ConvLSTM, Google Maps, Traffic flow prediction","Stockholm, Sweden",,,8,74–81,Association for Computing Machinery,ICMLT '23,Attention based Deep Hybrid Networks for Traffic Flow Prediction using Google Maps Data,https://doi.org/10.1145/3589883.3589894,,2023
inproceedings,10.1145/3488560.3498522,"In online shopping, ever-changing fashion trends make merchants need to prepare more differentiated products to meet the diversified demands, and e-commerce platforms need to capture the market trend with a prophetic vision. For the trend prediction, the attribute tags, as the essential description of items, can genuinely reflect the decision basis of consumers. However, few existing works explore the attribute trend in the specific community for e-commerce. In this paper, we focus on the community trend prediction on the item attribute and propose a unified framework that combines the dynamic evolution of two graph patterns to predict the attribute trend in a specific community. Specifically, we first design a community-attribute bipartite graph at each time step to learn the collaboration of different communities. Next, we transform the bipartite graph into a hypergraph to exploit the associations of different attribute tags in one community. Lastly, we introduce a dynamic evolution component based on the recurrent neural networks to capture the fashion trend of attribute tags. Extensive experiments on three real-world datasets in a large e-commerce platform show the superiority of the proposed approach over several strong alternatives and demonstrate the ability to discover the community trend in advance.","New York, NY, USA",,"Yuan, Jiahao and Li, Zhao and Zou, Pengcheng and Gao, Xuan and Pan, Jinwei and Ji, Wendi and Wang, Xiaoling",Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,10.1145/3488560.3498522,9781450391320,,,,"community trend, dynamic evolution, e-commerce, heterogeneous graph","Virtual Event, AZ, USA",,,9,1319–1327,Association for Computing Machinery,WSDM '22,Community Trend Prediction on Heterogeneous Graph in E-commerce,https://doi.org/10.1145/3488560.3498522,,2022
inproceedings,10.1145/3627673.3679810,"Urban undisciplined events (UUE) are of increasing concern to urban officials because they reduce the quality of life and cause societal disorder. How to accurately predict future occurrences is a key point in preventing these events. However, existing supervised methods struggle to perform well on sparse UUEs while self-supervised MAE-based methods adopt a traditional random masking strategy which leads to limited performance on UUE forecasting. Fortunately, we have designed an innovative spatiotemporal masking strategy and its corresponding pre-training task called &lt;u&gt;M&lt;/u&gt;asked &lt;u&gt;S&lt;/u&gt;patio-&lt;u&gt;T&lt;/u&gt;emporal &lt;u&gt;E&lt;/u&gt;vent Series &lt;u&gt;M&lt;/u&gt;odeling (MSTEM). Through Cluster-assisted region masking, MSTEM efficiently distributes masked regions evenly among different clusters, enhancing the model's ability to capture spatial correlation and heterogeneity while addressing sparse region distribution of UUEs. Frequency-enhanced patch masking helps the model to sufficiently extract the temporal features of UUEs by reconstructing multiple views. Additionally, we propose future merge and cluster label modeling to enhance the extraction of spatiotemporal dependencies, thereby improving the performance of MSTEM on downstream prediction tasks. Experimental evaluations on four real-world datasets including crimes and disorderly conduct show that our masked autoencoder with MSTEM outperforms most of the state-of-the-art baselines.","New York, NY, USA",,"Gu, Zehao and Zhou, Shiyang and Xiong, Yun and Luo, Yang and Ren, Hongrun and Wang, Qiang and Gao, Xiaofeng and Yu, Philip",Proceedings of the 33rd ACM International Conference on Information and Knowledge Management,10.1145/3627673.3679810,9798400704369,,,,"masked autoencoder, self-supervised learning, smart city, spatiotemporal prediction","Boise, ID, USA",,,10,685–694,Association for Computing Machinery,CIKM '24,MSTEM: Masked Spatiotemporal Event Series Modeling for Urban Undisciplined Events Forecasting,https://doi.org/10.1145/3627673.3679810,,2024
article,10.5555/3722577.3722800,"Time-series datasets are central in machine learning with applications in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Learning the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical Granger approaches). However, there are no joint approaches combining static and dynamic graphical modeling within the context of SSMs. This work proposes a novel approach to fill this gap by introducing a joint graphical modeling framework that bridges the graphical Lasso model and a causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new inference method within this framework that implements an efficient block alternating majorization-minimization algorithm. The algorithm's convergence is established by departing from modern tools from nonlinear analysis. Experimental validation on various synthetic data showcases the effectiveness of the proposed model and inference algorithm. This work will significantly contribute to the understanding and utilization of time-series data in diverse scientific and engineering applications where incorporating a graphical approach is essential to perform the inference.",,223,"Chouzenoux, Emilie and Elvira, V\'{\i",,,,1532-4435,January 2024,J. Mach. Learn. Res.,"state-space models, graph inference, sparsity, graphical lasso, majorization-minimization, proximal algorithm",,,1,53,,JMLR.org,,Sparse graphical linear dynamical systems,,25,2024
inproceedings,10.1145/3690624.3709201,"Traffic prediction is essential for intelligent transportation systems and urban computing. It aims to establish a relationship between historical traffic data X and future traffic states Y by employing various statistical or deep learning methods. However, the relations of X → Y are often influenced by external confounders that simultaneously affect both X and Y, such as weather, accidents, and holidays. Existing deep-learning traffic prediction models adopt the classic front-door and back-door adjustments to address the confounder issue. However, these methods have limitations in addressing continuous or undefined confounders, as they depend on predefined discrete values that are often impractical in complex, real-world scenarios. To overcome this challenge, we propose the Spatial-Temporal sElf-superVised confoundEr learning (STEVE) model. This model introduces a basis vector approach, creating a base confounder bank to represent any confounder as a linear combination of a group of basis vectors. It also incorporates self-supervised auxiliary tasks to enhance the expressive power of the base confounder bank. Afterward, a confounder-irrelevant relation decoupling module is adopted to separate the confounder effects from direct X → Y relations. Extensive experiments across four large-scale datasets validate our model's superior performance in handling spatial and temporal distribution shifts and underscore its adaptability to unseen confounders. Our model implementation is available at https://github.com/bigscity/STEVE_CODE.","New York, NY, USA",,"Ji, Jiahao and Zhang, Wentao and Wang, Jingyuan and Huang, Chao",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709201,9798400712456,,,,"continuous and undefined confounder, spatial-temporal forecasting, urban computing","Toronto ON, Canada",,,12,577–588,Association for Computing Machinery,KDD '25,Seeing the Unseen: Learning Basis Confounder Representations for Robust Traffic Prediction,https://doi.org/10.1145/3690624.3709201,,2025
inproceedings,10.1145/3583788.3583799,"Effective crime prediction plays a key role in sustaining the stability of society. In recent years, researchers have proposed a number of prediction methods that extract spatial and temporal features separately and fuse afterward. However, the strict distinction between spatial feature extraction and temporal feature extraction can result in the loss of useful information. To this end, we propose a spatio-temporal deep fusion graph convolution network (STDGCN), which embodies the intra-region spatio-temporal features and the inter-region spatio-temporal associations on a single graph. STDGCN performs the convolution without distinguishing between space and time to simultaneously extract spatio-temporal features. Our evaluations of two real-world datasets demonstrate the effectiveness of STDGCN.","New York, NY, USA",,"Chen, Bingbing and Liao, Yong",Proceedings of the 2023 7th International Conference on Machine Learning and Soft Computing,10.1145/3583788.3583799,9781450398633,,,,"Crime Prediction, Graph Neural Network, Spatio-Temporal Prediction","Chongqing, China",,,7,75–81,Association for Computing Machinery,ICMLSC '23,Spatio-Temporal Deep Fusion Graph Convolutional Networks for Crime Prediction,https://doi.org/10.1145/3583788.3583799,,2023
inproceedings,10.1145/3606043.3606045,"In recent years, with the expansion of higher education institutions year by year, the total number of fresh undergraduates has been rapidly increasing. This paper proposes a prediction algorithm for the number of undergraduates who will enter graduate school through long short-term memory (LSTM) based on the current development trend of graduate school and the number of admissions to graduate school in recent years.Firstly, the parameters that have the greatest influence on the prediction of the number of applicants for the examinations are statistically analyzed, and then a deep learning prediction model based on LSTM is built to predict the number of applicants for the examinations, and the results are displayed in the visualization interface. The experimental results show that the trained LSTM model works better than the Support Vector Machine (SVM) results. The prediction model will be provided to students before registering for the exam, which is of practical significance to facilitate students to make reasonable decisions.","New York, NY, USA",,"Ying, Youwei and Wang, Zhen and Li, Hui and Yang, Wenying and Zhu, Xiaodan and Kou, Lei","Proceedings of the 2023 7th International Conference on High Performance Compilation, Computing and Communications",10.1145/3606043.3606045,9781450399883,,,,,"Jinan, China",,,7,8–14,Association for Computing Machinery,HP3C '23,Prediction of the Number of Postgraduate Entrance Examination Applicants Based on LSTM and Statistical Analysis Method,https://doi.org/10.1145/3606043.3606045,,2023
article,10.14778/3725688.3725697,"While existing spatiotemporal prediction models have shown promising performance, they often rely on the assumption of input-label spatiotemporal consistency, and their high complexity raises concerns about scalability. To enhance both efficiency and performance, we integrate label information into the learning process and propose a spatiotemporal dynamic theory that outlines a bi-directional learning paradigm. Building on this paradigm, we design BiST, a lightweight yet effective Bi-directional Spatio-Temporal prediction model. BiST incorporates two key processes: a forward spatiotemporal learning process and a backward correction process. The forward process utilizes MLP layers exclusively to model input correlations and generate base prediction. In the backward process, we implement a spatiotemporal decoupling module, which can learn the residual modeling deviation between input and label representations from a decoupled perspective. After smoothing the residual with a diffusion module, we can obtain the correction term to correct the base predictions. This innovative design enables BiST to achieve competitive performance while remaining lightweight. We evaluate BiST against 26 baselines across 13 datasets, including a large-scale dataset with ten thousand nodes and a longrange dataset spanning 20 years. An impressive experimental result demonstrates that BiST achieves a 8.13\% improvement in performance compared to state-of-the-art models while consuming only 1.86\% of the training time and 7.36\% of the memory usage.",,,"Ma, Jiaming and Wang, Binwu and Wang, Pengkun and Zhou, Zhengyang and Wang, Xu and Wang, Yang",,10.14778/3725688.3725697,,2150-8097,February 2025,Proc. VLDB Endow.,,,,6,14,1663–1676,VLDB Endowment,,BiST: A Lightweight and Efficient Bi-Directional Model for Spatiotemporal Prediction,https://doi.org/10.14778/3725688.3725697,18,2025
article,10.1145/3772723,"With the evolution of urban smart transportation, the complexity of urban traffic networks escalates, emphasizing the importance of large-scale traffic data prediction in traffic management and urban planning. Traditional spatiotemporal graph models, such as Graph-WaveNet and MTGCN, face exponentially increasing computational complexity as the spatial dimensions expand. To address this challenge, we propose a novel Higher-order Adaptive Generative graph for Massive Traffic Forecasting (HAG-MTF) approach, which utilizes generative AI and high-order graph structures to model the intricate spatial dependencies in large-scale traffic data. The HAG-MTF incorporates a high-order dimensionality reduction module to optimize traffic node processing, utilizing prior graph relationships to generate a fusion graph that dynamically incorporates neighborhood information for efficient, localized graph convolution. The model further incorporates the high-order spatiotemporal relationship extraction module (H-net), enhancing the capacity and speed of traffic data processing while boosting prediction accuracy for complex spatial structures. Furthermore, HAG-MTF introduces a fusion loss function that hierarchically balances multiple objectives, ensuring both precision and computational efficiency. HAG-MTF adaptively handles large-scale real-world traffic data, meeting the needs of traffic controllers and urban planners for predicting massive datasets in practical settings. It supports efficient, flexible interactions via parameter tuning and model outputs, ultimately integrating human insights into traffic analysis and decision-making. This dynamic human-machine collaboration differs from non-Industry 5.0 approaches, which rely on purely automated systems without human input. Those lead to inflexible, brittle conclusions and recommendations, neglecting shifts in traffic patterns driven by human behavior. Extensive experiments on real-world traffic datasets demonstrate that HAG-MTF significantly improves processing efficiency for high-complexity spatial data while delivering precise, human-informed predictions through generative AI-driven operations.","New York, NY, USA",,"Wang, Lei and Wu, Huaming and Zhang, Fan and Li, Keqiu and Yu, Wei and Chen, Shuo",,10.1145/3772723,,1556-4665,,ACM Trans. Auton. Adapt. Syst.,"Spatiotemporal big data, Traffic prediction, High-order generative graph convolution",,Just Accepted,,,,Association for Computing Machinery,,HAG-MTF: Higher-Order Adaptive Generative Graph for Massive Traffic Forecasting in Industry 5.0,https://doi.org/10.1145/3772723,,2025
inproceedings,10.1145/3635638.3635641,"In order to solve the problem of short term power load forecasting and improve the accuracy of load forecasting in distribution station area, a short term power load forecasting model integrating multiple temporal series characteristics is proposed in this paper. Firstly, we solve the problems of missing and outliers in acquired load and weather data through data preprocessing. Then, we consider the historical load, weather, working day/holiday and other factors, then explore the long-short term, periodicity and particularity of the load and weather information. After that, we establish the short-term power load prediction feature model of the platform area. In addition, we use the deep residual network as the basic structure to eliminate the overfitting problem caused by the deep network. Experiments show that the proposed method has smaller prediction error compared with the existing methods.","New York, NY, USA",,"Hu, Hailin and Yan, Li",Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence,10.1145/3635638.3635641,9798400709456,,,,"Deep residual network, Distribution station area, Multiple temporal characteristics, Short-term load forecasting","Chongqing, China",,,7,17–23,Association for Computing Machinery,MLMI '23,Short-term Load Forecasting Method of Power Distribution Station Area Integrating Multiple Temporal Characteristics,https://doi.org/10.1145/3635638.3635641,,2024
inproceedings,10.1145/3690624.3709231,"Large-scale human mobility exhibits spatial and temporal patterns that can assist policymakers in decision making. Although traditional prediction models attempt to capture these patterns, they are often affected by nonperiodic public events, such as disasters and occasional celebrations. Since regular human mobility patterns are affected by these events, estimating their causal effects is critical to accurate mobility predictions. News articles provide unique perspectives on these events, though processing them is a challenge. In this study, we propose a causality based prediction model, CausalMob, to analyze the causal effects of public events. We first utilize large language models (LLMs) to extract human intentions from news and transform them into features that act as causal treatments. Next, the model learns representations of spatio-temporal regional covariates from multiple data sources to serve as confounders for causal inference. Finally, we present a causal effect estimation framework to ensure that event features remain independent of confounders during prediction. Based on large-scale real-world data, the experimental results show that the proposed model excels in human mobility prediction, outperforming state-of-the-art models.","New York, NY, USA",,"Yang, Xiaojie and Ge, Hangli and Wang, Jiawei and Fan, Zipei and Jiang, Renhe and Shibasaki, Ryosuke and Koshizuka, Noboru",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1,10.1145/3690624.3709231,9798400712456,,,,"causal inference, human mobility prediction, llms","Toronto ON, Canada",,,12,1773–1784,Association for Computing Machinery,KDD '25,CausalMob: Causal Human Mobility Prediction with LLMs-derived Human Intentions toward Public Events,https://doi.org/10.1145/3690624.3709231,,2025
article,10.1145/3728370,Accurate measurement of oxygen uptake ( (dot{mathrm{V,"New York, NY, USA",39,"Yang, Luyao and Amin, Osama and Faisal, Azmy and Shihada, Basem",,10.1145/3728370,,,July 2025,ACM Trans. Comput. Healthcare,"Oxygen Uptake, Deep learning, (dot{mathrm{V",,,3,20,,Association for Computing Machinery,,Oxygen Uptake Estimation during Cardiopulmonary Exercise Testing Using Temporal Fusion Networks,https://doi.org/10.1145/3728370,6,2025
inproceedings,10.1145/3711896.3737149,"In this paper, we investigate the discovery of temporal anomalous subgraphs in large-scale financial networks, aiming to identify abnormal transaction behaviors among users over time. This task is crucial for the real-time detection of transaction anomalies in financial networks, such as money laundering and trading fraud. However, it poses significant challenges due to the diverse distribution of transactions, the dynamic nature of temporal networks, and the absence of theoretical foundation. To tackle these challenges, we introduce a novel Temporal Anomalous Subgraph Discovery (TempASD) algorithm with theoretical analysis. First, we propose a temporal candidate detection module that quickly pinpoints abnormal candidates by detecting anomalies in both the temporal structure and transaction distribution. Then, we introduce a carefully crafted reinforcement-learning-based refiner to optimize these candidates toward the most abnormal directions. We conducted extensive evaluations against thirteen advanced competitors. TempASD achieves an average improvement of 7x in abnormal degree compared to the state-of-the-art and is efficient in large-scale dynamic financial networks.","New York, NY, USA",,"Han, Xiaolin and Zhang, Yikun and Ma, Chenhao and Song, Lingyun and Cheng, Reynold and Shang, Xuequn",Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2,10.1145/3711896.3737149,9798400714542,,,,"dynamic networks, temporal anomalous subgraph discovery","Toronto ON, Canada",,,12,826–837,Association for Computing Machinery,KDD '25,TempASD: Temporal Anomalous Subgraph Discovery in Large-Scale Dynamic Financial Networks,https://doi.org/10.1145/3711896.3737149,,2025
inproceedings,10.1145/3565291.3565301,"Accurate forecasting taxi demand help reduce waiting time for drivers and passengers as well as ease traffic congestion. However, most of the current research work has mostly ignored the impact of historical cab inflows and potential spatial dependencies between different regions on taxi demand. In view of this, this paper integrates several attributes affecting taxi demand and develops a multi-attribute spatial-temporal graphical convolutional network model (MASTGCN) with the expectation of accurately predicting the MASTGCN model is designed to accurately predict the demand for rental cars. Specifically, the MASTGCN model is designed with four components, which model the temporal dependence of taxi demand on the demand series at the near moment, the daily demand series, the historical taxi inflow series, and the daily demand series, respectively. The components are designed to model the temporal dependence of taxi demand on proximity demand series, daily demand series, historical cab inflow series, and the potential spatial dependence among different regions. To demonstrate the effectiveness of the MASTGCN model, we compare it with five benchmark models commonly used for traffic forecasting and three metrics, RMSE, MAE and MAPE, are used for evaluation. The experimental results show that the MASTGCN model, which incorporates multiple attributes, can more accurately the multiattribute MASTGCN model can predict taxi demand more accurately.","New York, NY, USA",,"Xu, Lei and Xia, Leiming and Pan, Shourun",Proceedings of the 5th International Conference on Big Data Technologies,10.1145/3565291.3565301,9781450396875,,,,"Potential spatial dependence, Spatial-temporal convolution blocks, Taxi demand","Qingdao, China",,,7,62–68,Association for Computing Machinery,ICBDT '22,Multi-Attribute Spatial-temporal Graph Convolutional Network for Taxi Demand Forecasting,https://doi.org/10.1145/3565291.3565301,,2022
inproceedings,10.1145/3637528.3671753,"With the proliferation of mobile sensing techniques, huge amounts of time series data are generated and accumulated in various domains, fueling plenty of real-world applications. In this setting, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal sample distribution in time series. Existing approaches generally assume that all the time series is available at a central location. However, we are witnessing the decentralized collection of time series due to the deployment of various edge devices. To bridge the gap between the decentralized time series data and the centralized anomaly detection algorithms, we propose a &lt;u&gt;P&lt;/u&gt;arameter-&lt;u&gt;e&lt;/u&gt;fficient &lt;u&gt;F&lt;/u&gt;ederated &lt;u&gt;A&lt;/u&gt;nomaly &lt;u&gt;D&lt;/u&gt;etection framework named PeFAD with the increasing privacy concerns. PeFAD for the first time employs the pre-trained language model (PLM) as the body of the client's local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and local model adaptation cost, we propose a parameter-efficient federated training module such that clients only need to fine-tune small-scale parameters and transmit them to the server for update. PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A knowledge distillation operation on a synthetic privacy-preserving dataset that is shared by all the clients is also proposed to address the data heterogeneity issue across clients. We conduct extensive evaluations on four real datasets, where PeFAD outperforms existing state-of-the-art baselines by up to 28.74\%.","New York, NY, USA",,"Xu, Ronghui and Miao, Hao and Wang, Senzhang and Yu, Philip S. and Wang, Jianxin",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671753,9798400704901,,,,"federated learning, pre-trained language model, time series anomaly detection","Barcelona, Spain",,,12,3621–3632,Association for Computing Machinery,KDD '24,PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection,https://doi.org/10.1145/3637528.3671753,,2024
inproceedings,10.1145/3583780.3615097,"Multivariate time series(MTS) is a universal data type related to various real-world applications. Data imputation methods are widely used in MTS applications to deal with the frequent data missing problem. However, these methods inevitably introduce biased imputation and training-redundancy problems in downstream training. To address these challenges, we propose TriD-MAE, a generic pre-trained model for MTS data with missing values. Firstly, we introduce TriD-TCN, an end-to-end module based on TCN that effectively extracts temporal features by integrating dynamic kernel mechanisms and a time-flipping trick. Building upon that, we designed an MAE-based pre-trained model as the precursor of specialized downstream models. Our model cooperates with a dynamic positional embedding mechanism to represent the missing information and generate transferable representation through our proposed encoder units. The overall mixed data feed-in strategy and weighted loss function are established to ensure adequate training of the whole model. Comparative experiment results in time series prediction and classification manifest that our TriD-MAE model outperforms the other state-of-the-art methods within six real-world datasets. Moreover, ablation and interpretability experiments are delivered to verify the validity of TriD-MAE's","New York, NY, USA",,"Zhang, Kai and Li, Chao and Yang, Qinmin",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3615097,9798400701245,,,,"missing data, pre-trained model, time series","Birmingham, United Kingdom",,,10,3164–3173,Association for Computing Machinery,CIKM '23,TriD-MAE: A Generic Pre-trained Model for Multivariate Time Series with Missing Values,https://doi.org/10.1145/3583780.3615097,,2023
article,10.14778/3734839.3734862,"Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.",,,"Kong, Weiyang and Wu, Kaiqi and Zhang, Sen and Liu, Yubao",,10.14778/3734839.3734862,,2150-8097,March 2025,Proc. VLDB Endow.,,,,7,13,2295–2307,VLDB Endowment,,GraphSparseNet: A Novel Method for Large Scale Traffic Flow Prediction,https://doi.org/10.14778/3734839.3734862,18,2025
inproceedings,10.1145/3639631.3639648,"This research proposes a novel approach for forecasting cryptocurrency prices, specifically Bitcoin which dominates the market. Accurately predicting cryptocurrency values is challenging due to their highly volatile nature. The proposed hybrid model uses ResNet Convolutional Neural Network to encode Bitcoin price time series data into discriminative representations. These representations capture long-range dependencies using XGBoost regression. Additionally, wavelet denoising is applied to filter noise from the price data. The combined ResNet-XGBoost-Wavelet model achieves satisfactory results for Bitcoin price forecasting and has practical applications for developing quantitative trading strategies. While incorporating sentiment analysis and additional influencing factors could further improve predictions, this work presents a competitive approach for minimizing investment risks and maximizing profits in the complex domain of cryptocurrency markets.","New York, NY, USA",,"Yamak, Peter T and Li, Yujian and Zhang, Ting and Dakurah, Kyefondeme C.","Proceedings of the 2023 6th International Conference on Algorithms, Computing and Artificial Intelligence",10.1145/3639631.3639648,9798400709203,,,,"Bitcoin, CNN, ResNet, Time Series, Wavelet-Denoised, XGBoost","Sanya, China",,,10,103–112,Association for Computing Machinery,ACAI '23,Leveraging ResNet CNN and XGBoost for Enhanced Bitcoin Price Forecasting,https://doi.org/10.1145/3639631.3639648,,2024
article,10.1145/3565973,"The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.","New York, NY, USA",47,"Dong, Guimin and Tang, Mingyue and Wang, Zhiyuan and Gao, Jiechao and Guo, Sikun and Cai, Lihua and Gutierrez, Robert and Campbel, Bradford and Barnes, Laura E. and Boukhechba, Mehdi",,10.1145/3565973,,1550-4859,May 2023,ACM Trans. Sen. Netw.,"Graph neural network, Internet of Things, sensor network, survey",,,2,50,,Association for Computing Machinery,,Graph Neural Networks in IoT: A Survey,https://doi.org/10.1145/3565973,19,2023
article,10.1145/3451394,"Crowd flow prediction is an essential task benefiting a wide range of applications for the transportation system and public safety. However, it is a challenging problem due to the complex spatio-temporal dependence and the complicated impact of urban structure on the crowd flow patterns. In this article, we propose a novel framework, 3-Dimensional Graph Convolution Network (3DGCN), to predict citywide crowd flow. We first model it as a dynamic spatio-temporal graph prediction problem, where each node represents a region with time-varying flows, and each edge represents the origin–destination (OD) flow between its corresponding regions. As such, OD flows among regions are treated as a proxy for the spatial interactions among regions. To tackle the complex spatio-temporal dependence, our proposed 3DGCN can model the correlation among graph spatial and temporal neighbors simultaneously. To learn and incorporate urban structures in crowd flow prediction, we design the GCN aggregator to be learned from both crowd flow prediction and region function inference at the same time. Extensive experiments with real-world datasets in two cities demonstrate that our model outperforms state-of-the-art baselines by 9.6\%∼19.5\% for the next-time-interval prediction.","New York, NY, USA",110,"Xia, Tong and Lin, Junjie and Li, Yong and Feng, Jie and Hui, Pan and Sun, Funing and Guo, Diansheng and Jin, Depeng",,10.1145/3451394,,1556-4681,June 2021,ACM Trans. Knowl. Discov. Data,"Graph neural network, traffic flow prediction, urban computing",,,6,21,,Association for Computing Machinery,,3DGCN: 3-Dimensional Dynamic Graph Convolutional Network for Citywide Crowd Flow Prediction,https://doi.org/10.1145/3451394,15,2021
inproceedings,10.1145/3589132.3625631,"Mining spatio-temporal correlation patterns for traffic prediction is a well-studied field. However, most approaches are based on the assumption of the availability of and accessibility to a sufficiently dense data source, which is rather the rare case in reality. Traffic sensors in road networks are generally highly sparse in their distribution: fleet-based traffic sensing is sparse in space but also sparse in time. There are also other traffic application, besides road traffic, like moving objects in the marine space, where observations are sparsely and arbitrarily distributed in space. In this paper, we tackle the problem of traffic prediction on sparse and spatially irregular and non-deterministic traffic observations. We draw a border between imputations and this work as we consider high sparsity rates and no fixed sensor locations. We advance correlation mining methods with a Sparse Unstructured Spatio Temporal Reconstruction (SUSTeR) framework that reconstructs traffic states from sparse non-stationary observations. For the prediction the framework creates a hidden context traffic state which is enriched in a residual fashion with each observation. Such an assimilated hidden traffic state can be used by existing traffic prediction methods to predict future traffic states. We query these states with query locations from the spatial domain.","New York, NY, USA",81,W\,Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems,10.1145/3589132.3625631,9798400701689,,,,"sparse data, spatio-temporal, unstructured observations, imputation, traffic prediction","Hamburg, Germany",,,10,,Association for Computing Machinery,SIGSPATIAL '23,SUSTeR: Sparse Unstructured Spatio Temporal Reconstruction on Traffic Prediction,https://doi.org/10.1145/3589132.3625631,,2023
inproceedings,10.1145/3604237.3626851,"Thanks to the access to the labeled orders on the CAC40 data from Euronext, we are able to analyze agents’ behaviors in the market based on their placed orders. In this study, we construct a self-supervised learning model using triplet loss to effectively learn the representation of agent market orders. By acquiring this learned representation, various downstream tasks become feasible. In this work, we utilize the K-means clustering algorithm on the learned representation vectors of agent orders to identify distinct behavior types within each cluster.","New York, NY, USA",,"Ruan, Ruihua",Proceedings of the Fourth ACM International Conference on AI in Finance,10.1145/3604237.3626851,9798400702402,,,,"Liquidity takers, agent behaviors, agent-based model, clustering, contrastive learning, triplet loss","Brooklyn, NY, USA",,,9,601–609,Association for Computing Machinery,ICAIF '23,Liquidity takers behavior representation through a contrastive learning approach,https://doi.org/10.1145/3604237.3626851,,2023
inproceedings,10.1109/ASONAM55673.2022.10068595,"With the increase in volume of daily online news items, it is more and more difficult for readers to identify news articles relevant to their interests. Thus, effective recommendation systems are critical for an effective user news consumption experience. Existing news recommendation methods usually rely on the news click history to model user interest. However, there are other signals about user behaviors, such as user commenting activity, which have not been used before. We propose a recommendation algorithm that predicts articles a user may be interested in, given her historical sequential commenting behavior on news articles. We show that following this sequential user behavior the news recommendation problem falls into in the class of session-based recommendation. The techniques in this class seek to model users' sequential and temporal behaviors. While we seek to follow the general directions in this space, we face unique challenges specific to news in modeling temporal dynamics, e.g., users' interests shift over time, users comment irregularly on articles, and articles are perishable items with limited lifespans. We propose a recency-regularized neural attentive framework for session-based news recommendation. The proposed method is able to capture the temporal dynamics of both users and news articles, while maintaining interpretability. We design a lag-aware attention and a recency regularization to model the time effect of news articles and comments. We conduct extensive empirical studies on 3 real-world news datasets to demonstrate the effectiveness of our method.",,,"Shen, Chen and Han, Chao and He, Lihong and Mukherjee, Arjun and Obradovic, Zoran and Dragut, Eduard",Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,10.1109/ASONAM55673.2022.10068595,9781665456616,,,,"recommender systems, neural networks, session based recommendation","Istanbul, Turkey",,,8,163–170,IEEE Press,ASONAM '22,Session-Based News Recommendation from Temporal User Commenting Dynamics,https://doi.org/10.1109/ASONAM55673.2022.10068595,,2023
inproceedings,10.1145/3580305.3599303,"Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collections of related datasets (a.k.a. meta-datasets). Through extensive experiments on three large-scale meta-datasets, we demonstrate that pipeline embeddings yield state-of-the-art results in Pipeline Optimization.","New York, NY, USA",,"Pineda Arango, Sebastian and Grabocka, Josif",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599303,9798400701030,,,,"automl, deep kernel gaussian processes, meta-learning, pipeline optimization","Long Beach, CA, USA",,,13,1907–1919,Association for Computing Machinery,KDD '23,Deep Pipeline Embeddings for AutoML,https://doi.org/10.1145/3580305.3599303,,2023
article,10.1109/TNET.2021.3136707,"Precise citywide mobile traffic prediction is of great significance for intelligent network planning and proactive service provisioning. Current traffic prediction approaches mainly focus on training a well-performed model for the cities with a large amount of mobile traffic data. However, for the cities with scarce data, the prediction performance will be greatly limited. To tackle this problem, in this paper we propose a novel cross-city deep transfer learning framework named CCTP for citywide mobile traffic prediction in cities with data scarcity. Specifically, we first present a novel spatial-temporal learning model and pre-train the model by abundant data of a source city to obtain prior knowledge of mobile traffic dynamics. We then devise an efficient generative adversarial network (GAN) based cross-domain adapter for distribution alignment between target data and source data. To deal with data scarcity issue in some clusters of target city, we further design an inter-cluster transfer learning strategy for performance enhancement. Extensive experiments conducted on real-world mobile traffic datasets demonstrate that our proposed CCTP framework can achieve superior performance in citywide mobile traffic prediction with data scarcity.",,,"Wu, Qiong and He, Kaiwen and Chen, Xu and Yu, Shuai and Zhang, Junshan",,10.1109/TNET.2021.3136707,,1063-6692,June 2022,IEEE/ACM Trans. Netw.,,,,3,13,1255–1267,IEEE Press,,Deep Transfer Learning Across Cities for Mobile Traffic Prediction,https://doi.org/10.1109/TNET.2021.3136707,30,2021
inproceedings,10.1145/3583780.3614962,"Urban time series data forecasting featuring significant contributions to sustainable development is widely studied as an essential task of the smart city. However, with the dramatic and rapid changes in the world environment, the assumption that data obey Independent Identically Distribution is undermined by the subsequent changes in data distribution, known as concept drift, leading to weak replicability and transferability of the model over unseen data. To address the issue, previous approaches typically retrain the model, forcing it to fit the most recent observed data. However, retraining is problematic in that it leads to model lag, consumption of resources, and model re-invalidation, causing the drift problem to be not well solved in realistic scenarios. In this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network. Experiments on real-world datasets show that our design significantly outperforms state-of-the-art methods and can be well generalized to existing prediction backbones by reducing their sensitivity to distribution changes.","New York, NY, USA",,"Cai, Zekun and Jiang, Renhe and Yang, Xinyu and Wang, Zhaonan and Guo, Diansheng and Kobayashi, Hill Hiroki and Song, Xuan and Shibasaki, Ryosuke",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,10.1145/3583780.3614962,9798400701245,,,,"concept drift, domain adaptation, time series, urban computing","Birmingham, United Kingdom",,,10,193–202,Association for Computing Machinery,CIKM '23,MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation,https://doi.org/10.1145/3583780.3614962,,2023
inproceedings,10.1145/3511808.3557243,"Traffic prediction plays an important role in many intelligent transportation systems. Many existing works design static neural network architecture to capture complex spatio-temporal correlations, which is hard to adapt to different datasets. Although recent neural architecture search approaches have addressed this problem, it still adopts a coarse-grained search with pre-defined and fixed components in the search space for spatio-temporal modeling. In this paper, we propose a novel neural architecture search framework, entitled AutoSTS, for automated spatio-temporal synchronous modeling in traffic prediction. To be specific, we design a graph neural network (GNN) based architecture search module to capture localized spatio-temporal correlations, where multiple graphs built from different perspectives are jointly utilized to find a better message passing way for mining such correlations. Further, we propose a convolutional neural network (CNN) based architecture search module to capture temporal dependencies with various ranges, where gated temporal convolutions with different kernel sizes and convolution types are designed in search space. Extensive experiments on six public datasets demonstrate that our model can achieve 4\%-10\% improvements compared with other methods.","New York, NY, USA",,"Li, Fuxian and Yan, Huan and Jin, Guangyin and Liu, Yue and Li, Yong and Jin, Depeng",Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management,10.1145/3511808.3557243,9781450392365,,,,"graph convolution, spatio-temporal modeling, traffic prediction","Atlanta, GA, USA",,,10,1084–1093,Association for Computing Machinery,CIKM '22,Automated Spatio-Temporal Synchronous Modeling with Multiple Graphs for Traffic Prediction,https://doi.org/10.1145/3511808.3557243,,2022
inproceedings,10.1145/3637528.3671751,"Missing data is a pervasive issue in both scientific and engineering tasks, especially for the modeling of spatiotemporal data. Existing imputation solutions mainly include low-rank models and deep learning models. The former assumes general structural priors but has limited model capacity. The latter possesses salient expressivity, but lacks prior knowledge of the underlying spatiotemporal structures. Leveraging the strengths of both two paradigms, we demonstrate a low rankness-induced Transformer to achieve a balance between strong inductive bias and high expressivity. The exploitation of the inherent structures of spatiotemporal data enables our model to learn balanced signal-noise representations, making it generalizable for a variety of imputation tasks. We demonstrate its superiority in terms of accuracy, efficiency, and versatility in heterogeneous datasets, including traffic flow, solar energy, smart meters, and air quality. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rankness, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems.","New York, NY, USA",,"Nie, Tong and Qin, Guoyang and Ma, Wei and Mei, Yuewen and Sun, Jian",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3637528.3671751,9798400704901,,,,"data imputation, low-rank modeling, missing data, spatiotemporal data, time series, transformers","Barcelona, Spain",,,12,2260–2271,Association for Computing Machinery,KDD '24,ImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation,https://doi.org/10.1145/3637528.3671751,,2024
article,10.1145/3450287,"Events are occurrences in specific locations, time, and semantics that nontrivially impact either our society or the nature, such as earthquakes, civil unrest, system failures, pandemics, and crimes. It is highly desirable to be able to anticipate the occurrence of such events in advance to reduce the potential social upheaval and damage caused. Event prediction, which has traditionally been prohibitively challenging, is now becoming a viable option in the big data era and is thus experiencing rapid growth, also thanks to advances in high performance computers and new Artificial Intelligence techniques. There is a large amount of existing work that focuses on addressing the challenges involved, including heterogeneous multi-faceted outputs, complex (e.g., spatial, temporal, and semantic) dependencies, and streaming data feeds. Due to the strong interdisciplinary nature of event prediction problems, most existing event prediction methods were initially designed to deal with specific application domains, though the techniques and evaluation procedures utilized are usually generalizable across different domains. However, it is imperative yet difficult to cross-reference the techniques across different domains, given the absence of a comprehensive literature survey for event prediction. This article aims to provide a systematic and comprehensive survey of the technologies, applications, and evaluations of event prediction in the big data era. First, systematic categorization and summary of existing techniques are presented, which facilitate domain experts’ searches for suitable techniques and help model developers consolidate their research at the frontiers. Then, comprehensive categorization and summary of major application domains are provided to introduce wider applications to model developers to help them expand the impacts of their research. Evaluation metrics and procedures are summarized and standardized to unify the understanding of model performance among stakeholders, model developers, and domain experts in various application domains. Finally, open problems and future directions are discussed. Additional resources related to event prediction are included in the paper website: http://cs.emory.edu/∼lzhao41/projects/event_prediction_site.html.","New York, NY, USA",94,"Zhao, Liang",,10.1145/3450287,,0360-0300,June 2022,ACM Comput. Surv.,"Event prediction, artificial intelligence, big data",,,5,37,,Association for Computing Machinery,,Event Prediction in the Big Data Era: A Systematic Survey,https://doi.org/10.1145/3450287,54,2021
article,10.14778/3681954.3681996,"Transformer-based models have facilitated numerous applications with superior performance. A key challenge in transformers is the quadratic dependency of its training time complexity on the length of the input sequence. A recent popular solution is using random feature attention (RFA) to approximate the costly vanilla attention mechanism. However, RFA relies on only a single, fixed projection for approximation, which does not capture the input distribution and can lead to low efficiency and accuracy, especially on time series data. In this paper, we propose DARKER, an efficient transformer with a novel DAta-dRiven KERnel-based attention mechanism. To precisely present the technical details, this paper discusses them with a fundamental time series task, namely, time series classification (tsc). First, the main novelty of DARKER lies in approximating the softmax kernel by learning multiple machine learning models with trainable weights as multiple projections offline, moving beyond the limitation of a fixed projection. Second, we propose a projection index (called pIndex) to efficiently search the most suitable projection for the input for training transformer. As a result, the overall time complexity of DARKER is linear with the input length. Third, we propose an indexing technique for efficiently computing the inputs required for transformer training. Finally, we evaluate our method on 14 real-world and 2 synthetic time series datasets. The experiments show that DARKER is 3\texttimes{",,,"Zuo, Rundong and Li, Guozhong and Cao, Rui and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S",,10.14778/3681954.3681996,,2150-8097,July 2024,Proc. VLDB Endow.,,,,11,14,3229–3242,VLDB Endowment,,DARKER: Efficient Transformer with Data-Driven Attention Mechanism for Time Series,https://doi.org/10.14778/3681954.3681996,17,2024
inproceedings,10.1145/3580305.3599893,"Online learning is a powerful technique that allows models to adjust to concept drift in dynamically changing graphs. This approach is crucial for large mobility-based companies like Grab, where batch-learning methods fail to keep up with the large amount of training data. Our work focuses on scaling graph neural network mixture of expert (MoE) models for real-time traffic speed prediction on road networks, while meeting high accuracy and low latency requirements. Conventional spatio-temporal and incremental MoE frameworks struggle with poor inference accuracy and linear time complexity when scaling experts, for the latter, leading to prohibitively high latency in model updates. To address this issue, we introduce the Indexed Router, a novel method that categorizes experts into a structured hierarchy called the indexed tree. This approach reduces the time to scale and search N number of experts from O(N) to O(log N), making it ideal for online learning under tight service level agreements. Our experiments show that these time savings do not compromise inference accuracy, and our Indexed Router outperforms state-of-the-art spatio-temporal and incremental MoE models in terms of traffic speed prediction accuracy on real-life GPS traces from Grab's database and publicly available records. In summary, the Indexed Router enables MoE models to scale across large numbers of experts with low latency, while accurately identifying the relevant experts for inference.","New York, NY, USA",,"Kang, Johan Kok Zhi and Tan, Sien Yi and He, Bingsheng and Zhang, Zhen",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599893,9798400701030,,,,"mixture of experts, online learning, temporal concept drift","Long Beach, CA, USA",,,12,4308–4319,Association for Computing Machinery,KDD '23,Real Time Index and Search Across Large Quantities of GNN Experts for Low Latency Online Learning,https://doi.org/10.1145/3580305.3599893,,2023
inproceedings,10.1145/3580305.3599925,"Travel Time Estimation (TTE) aims to accurately forecast the expected trip duration from an origin to a destination. As one of the world's largest ride-hailing platforms, DiDi answers billions of TTE queries per day. The quality of TTE directly decides the customer's experience and the effectiveness of passenger-to-driver matching. However, existing studies mainly regard TTE as a deterministic regression problem and focus on improving the prediction accuracy of a single label, which overlooks the travel time uncertainty induced by various dynamic contextual factors. To this end, in this paper, we propose a probabilistic framework, ProbTTE, for uncertainty-aware travel time prediction. Specifically, the framework first transforms the single-label regression task to a multi-class classification problem to estimate the implicit travel time distribution. Moreover, we propose an adaptive local label-smoothing scheme to capture the ordinal inter-class relationship among soft travel time labels. Furthermore, we construct a route-wise log-normal distribution regularizer to absorb prior knowledge from large-scale historical trip data. By explicitly considering the travel uncertainty, the proposed approach not only improves the TTE accuracy but also provides additional travel time information to benefit downstream tasks in ride-hailing. Extensive experiments on real-world datasets demonstrate the superiority of the proposed framework compared with state-of-the-art travel time prediction algorithms. In addition, ProbTTE has been deployed in production at DiDi in late 2022 to empower various order dispatching services, and improves passenger and driver experiences significantly.","New York, NY, USA",,"Liu, Hao and Jiang, Wenzhao and Liu, Shui and Chen, Xi",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599925,9798400701030,,,,"deep neural networks, order dispatching, probabilistic forecasting, travel time estimation","Long Beach, CA, USA",,,11,4516–4526,Association for Computing Machinery,KDD '23,Uncertainty-Aware Probabilistic Travel Time Prediction for On-Demand Ride-Hailing at DiDi,https://doi.org/10.1145/3580305.3599925,,2023
inproceedings,10.1145/3534678.3539397,"Spatial temporal forecasting plays an important role in improving the quality and performance of Intelligent Transportation Systems. This task is rather challenging due to the complicated and long-range spatial temporal dependencies in traffic network. Existing studies typically employ different deep neural networks to learn the spatial and temporal representations so as to capture the complex and dynamic dependencies. In this paper, we argue that it is insufficient to capture the long-range spatial dependencies from the implicit representations learned by temporal extracting modules. To address this problem, we propose Multi-Step Dependency Relation (MSDR), a brand new variant of recurrent neural network. Instead of only looking at the hidden state from only one latest time step, MSDR explicitly takes those of multiple historical time steps as the input of each time unit. We also develop two strategies to incur the spatial information into the dependency relation embedding between multiple historical time steps and the current one in MSDR. On the basis of it, we propose the Graph-based MSDR (GMSDR) framework to support general spatial temporal forecasting applications by seamlessly integrating graph-based neural networks with MSDR. We evaluate our proposed approach on several popular datasets. The results show that the proposed GMSDR framework outperforms state-of-the-art methods by an obvious margin.","New York, NY, USA",,"Liu, Dachuan and Wang, Jin and Shang, Shuo and Han, Peng",Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3534678.3539397,9781450393850,,,,"multi-step dependency, neural networks, relation embedding, traffic forecasting","Washington DC, USA",,,9,1042–1050,Association for Computing Machinery,KDD '22,MSDR: Multi-Step Dependency Relation Networks for Spatial Temporal Forecasting,https://doi.org/10.1145/3534678.3539397,,2022
article,10.1145/3729226,"Trajectory prediction aims to estimate an entity’s future path using its current position and historical movement data, benefiting fields like autonomous navigation, robotics, and human movement analytics. Deep learning approaches have become key in this area, utilizing large-scale trajectory datasets to model movement patterns, but face challenges in managing complex spatial dependencies and adapting to dynamic environments. To address these challenges, we introduce TrajLearn, a novel model for trajectory prediction that leverages generative modeling of higher-order mobility flows based on hexagonal spatial representation. TrajLearn predicts the next k steps by integrating a customized beam search for exploring multiple potential paths while maintaining spatial continuity. We conducted a rigorous evaluation of TrajLearn, benchmarking it against leading state-of-the-art approaches and meaningful baselines. The results indicate that TrajLearn achieves significant performance gains, with improvements of up to ~40\% across multiple real-world trajectory datasets. In addition, we evaluated different prediction horizons (i.e., various values of k), conducted resolution sensitivity analysis, and performed ablation studies to assess the impact of key model components. Furthermore, we developed a novel algorithm to generate mixed-resolution maps by hierarchically subdividing hexagonal regions into finer segments within a specified observation area. This approach supports selective detailing, applying finer resolution to areas of interest or high activity (e.g., urban centers) while using coarser resolution for less significant regions (e.g., rural or uninhabited areas), effectively reducing data storage requirements and computational overhead. We promote reproducibility and adaptability by offering complete code, data, and detailed documentation with flexible configuration options for various applications.","New York, NY, USA",12,"Nadiri, Amirhossein and Li, Jing and Faraji, Ali and Abuoda, Ghadeer and Papagelis, Manos",,10.1145/3729226,,2374-0353,September 2025,ACM Trans. Spatial Algorithms Syst.,"Mobility data analytics, spatial data mining, trajectory prediction, deep generative models",,,3,33,,Association for Computing Machinery,,TrajLearn: Trajectory Prediction Learning using Deep Generative Models,https://doi.org/10.1145/3729226,11,2025
inproceedings,10.1145/3580305.3599391,"Existing anomaly detection models for time series are primarily trained with normal-point-dominant data and would become ineffective when anomalous points intensively occur in certain episodes. To solve this problem, we propose a new approach, called DiffAD, from the perspective of time series imputation. Unlike previous prediction- and reconstruction-based methods that adopt either partial or complete data as observed values for estimation, DiffAD uses a density ratio-based strategy to select normal observations flexibly that can easily adapt to the anomaly concentration scenarios. To alleviate the model bias problem in the presence of anomaly concentration, we design a new denoising diffusion-based imputation method to enhance the imputation performance of missing values with conditional weight-incremental diffusion, which can preserve the information of observed values and substantially improves data generation quality for stable anomaly detection. Besides, we customize a multi-scale state space model to capture the long-term dependencies across episodes with different anomaly patterns. Extensive experimental results on real-world datasets show that DiffAD performs better than state-of-the-art benchmarks.","New York, NY, USA",,"Xiao, Chunjing and Gou, Zehua and Tai, Wenxin and Zhang, Kunpeng and Zhou, Fan",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,10.1145/3580305.3599391,9798400701030,,,,"data imputation, diffusion models, state space model, time series","Long Beach, CA, USA",,,10,2742–2751,Association for Computing Machinery,KDD '23,Imputation-based Time-Series Anomaly Detection with Conditional Weight-Incremental Diffusion Models,https://doi.org/10.1145/3580305.3599391,,2023
inproceedings,10.1145/3450439.3451872,"Generating interpretable visualizations of multivariate time series in the intensive care unit is of great practical importance. Clinicians seek to condense complex clinical observations into intuitively understandable critical illness patterns, like failures of different organ systems. They would greatly benefit from a low-dimensional representation in which the trajectories of the patients' pathology become apparent and relevant health features are highlighted. To this end, we propose to use the latent topological structure of Self-Organizing Maps (SOMs) to achieve an interpretable latent representation of ICU time series and combine it with recent advances in deep clustering. Specifically, we (a) present a novel way to fit SOMs with probabilistic cluster assignments (PSOM), (b) propose a new deep architecture for probabilistic clustering (DPSOM) using a VAE, and (c) extend our architecture to cluster and forecast clinical states in time series (T-DPSOM). We show that our model achieves superior clustering performance compared to state-of-the-art SOM-based clustering methods while maintaining the favorable visualization properties of SOMs. On the eICU data-set, we demonstrate that T-DPSOM provides interpretable visualizations of patient state trajectories and uncertainty estimation. We show that our method rediscovers well-known clinical patient characteristics, such as a dynamic variant of the Acute Physiology And Chronic Health Evaluation (APACHE) score. Moreover, we illustrate how it can disentangle individual organ dysfunctions on disjoint regions of the two-dimensional SOM map.","New York, NY, USA",,"Manduchi, Laura and H\","Proceedings of the Conference on Health, Inference, and Learning",10.1145/3450439.3451872,9781450383592,,,,,"Virtual Event, USA",,,10,236–245,Association for Computing Machinery,CHIL '21,T-DPSOM: an interpretable clustering method for unsupervised learning of patient health states,https://doi.org/10.1145/3450439.3451872,,2021
article,10.1145/3441444,"The prediction of the Customer Lifetime Value (CLV) is an important asset for tool-supported marketing by customer relationship managers. Since standard methods based on purchase recency, frequency, and past profit and revenue statistics often have limited predictive power, advanced machine learning (ML) techniques were applied to this task in recent years. However, existing approaches are often not fully capable of modeling certain temporal patterns that can be commonly found in practice, such as periodic purchasing behavior of customers. To address these shortcomings, we propose a novel method for CLV prediction based on a combination of several ML techniques. At its core, our method consists of a tailored deep learning approach based on encoder–decoder sequence-to-sequence recurrent neural networks with augmented temporal convolutions. This model is then combined with gradient boosting machines (GBMs) and a set of novel features in a hybrid framework. Empirical evaluations based on real-world data from a larger e-commerce company and a public dataset from the domain of online retail show that already the sequence-based model leads to competitive performance results. Stacking it with the GBM model is synergistic and further improves accuracy, indicating that the two models capture different patterns in the data.","New York, NY, USA",80,"Bauer, Josef and Jannach, Dietmar",,10.1145/3441444,,1556-4681,October 2021,ACM Trans. Knowl. Discov. Data,"Customer lifetime value, machine learning, neural networks",,,5,37,,Association for Computing Machinery,,Improved Customer Lifetime Value Prediction With Sequence-To-Sequence Learning and Feature-Based Models,https://doi.org/10.1145/3441444,15,2021
inproceedings,10.1145/3447548.3467341,"Link prediction is a fundamental task for graph analysis and the topic has been studied extensively for static or dynamic graphs. Essentially, the link prediction is formulated as a binary classification problem about two nodes. However, for temporal graphs, links (or interactions) among node sets appear in sequential orders. And the orders may lead to interesting applications. While a binary link prediction formulation fails to handle such an order-sensitive case. In this paper, we focus on such an interaction order prediction problem among a given node set on temporal graphs. For the technical aspect, we develop a graph neural network model named Temporal ATtention network (TAT), which utilizes the fine-grained time information on temporal graphs by encoding continuous real-valued timestamps as vectors. For each transformation layer of the model, we devise an attention mechanism to aggregate neighborhoods' information based on their representations and time encodings attached to their specific edges. We also propose a novel training scheme to address the permutation-sensitive property of the problem. Experiments on several real-world temporal graphs reveal that TAT outperforms some state-of-the-art graph neural networks by 55\% on average under the AUC metric.","New York, NY, USA",,"Xia, Wenwen and Li, Yuchen and Tian, Jianwei and Li, Shenghong",Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining,10.1145/3447548.3467341,9781450383325,,,,"graph neural networks, interaction order prediction, temporal graphs","Virtual Event, Singapore",,,10,1884–1893,Association for Computing Machinery,KDD '21,Forecasting Interaction Order on Temporal Graphs,https://doi.org/10.1145/3447548.3467341,,2021
article,10.1145/3659597,"The increasing availability of low-cost wearable devices and smartphones has significantly advanced the field of sensor-based human activity recognition (HAR), attracting considerable research interest. One of the major challenges in HAR is the domain shift problem in cross-dataset activity recognition, which occurs due to variations in users, device types, and sensor placements between the source dataset and the target dataset. Although domain adaptation methods have shown promise, they typically require access to the target dataset during the training process, which might not be practical in some scenarios. To address these issues, we introduce CrossHAR, a new HAR model designed to improve model performance on unseen target datasets. CrossHAR involves three main steps: (i) CrossHAR explores the sensor data generation principle to diversify the data distribution and augment the raw sensor data. (ii) CrossHAR then employs a hierarchical self-supervised pretraining approach with the augmented data to develop a generalizable representation. (iii) Finally, CrossHAR fine-tunes the pretrained model with a small set of labeled data in the source dataset, enhancing its performance in cross-dataset HAR. Our extensive experiments across multiple real-world HAR datasets demonstrate that CrossHAR outperforms current state-of-the-art methods by 10.83\% in accuracy, demonstrating its effectiveness in generalizing to unseen target datasets.","New York, NY, USA",64,"Hong, Zhiqing and Li, Zelong and Zhong, Shuxin and Lyu, Wenjun and Wang, Haotian and Ding, Yi and He, Tian and Zhang, Desheng",,10.1145/3659597,,,June 2024,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,"Cross-dataset, Cross-domain, Human activity recognition, Self-supervised learning",,,2,26,,Association for Computing Machinery,,CrossHAR: Generalizing Cross-dataset Human Activity Recognition via Hierarchical Self-Supervised Pretraining,https://doi.org/10.1145/3659597,8,2024
article,10.1145/3597937,"Sea surface temperature (SST) is one critical parameter of global climate change, and accurate SST prediction is important to various applications, e.g., weather forecasting, fishing directions, and disaster warnings. The global ocean system is unified and complex, and the SST patterns in different oceanic regions are highly diverse and correlated. However, existing data-driven SST prediction methods mainly consider the local patterns within a certain oceanic region, e.g., El Nino region and the Black sea. It is challenging but necessary to model the global SST correlations rather than that in a specific region to enhance the prediction accuracy of SST. In this work, we proposed a new method called Hierarchical Graph Recurrent Network&nbsp;(HiGRN) to address the issue. First, to learn the dynamic and diverse local SST patterns of specific locations, we design an adaptive node embedding with self-learned parameters to learn various SST patterns. Then we develop a hierarchical cluster generator to aggregate the locations with similar patterns into regional clusters and utilize a graph convolution network to learn the spatial correlations among these clusters. Finally, we introduce a multi-level attention mechanism to fuse the local patterns and regional correlations, and the output is fed into a recurrent network to achieve SST predictions. Extensive experiments on two real-world datasets show that our method largely outperforms the state-of-the-art SST prediction methods. The source code is available at .","New York, NY, USA",73,"Yang, Hanchen and Li, Wengen and Hou, Siyun and Guan, Jihong and Zhou, Shuigeng",,10.1145/3597937,,2157-6904,August 2023,ACM Trans. Intell. Syst. Technol.,"Sea surface temperature prediction, graph neural networks, spatial-temporal modeling, hierarchical correlation",,,4,19,,Association for Computing Machinery,,HiGRN: A Hierarchical Graph Recurrent Network for Global Sea Surface Temperature Prediction,https://doi.org/10.1145/3597937,14,2023
inproceedings,10.1145/3383455.3422539,"This paper reviews Artificial Intelligence (AI), Machine Learning (ML) and associated algorithms in future Capital Markets. New AI algorithms are constantly emerging, with each 'strain' mimicking a new form of human learning, reasoning, knowledge, and decisionmaking. The current main disrupting forms of learning include Deep Learning, Adversarial Learning, Transfer and Meta Learning. Albeit these modes of learning have been in the AI/ML field more than a decade, they now are more applicable due to the availability of data, computing power and infrastructure. These forms of learning have produced new models (e.g., Long Short-Term Memory, Generative Adversarial Networks) and leverage important applications (e.g., Natural Language Processing, Adversarial Examples, Deep Fakes, etc.). These new models and applications will drive changes in future Capital Markets, so it is important to understand their computational strengths and weaknesses. Since ML algorithms effectively self-program and evolve dynamically, financial institutions and regulators are becoming increasingly concerned with ensuring there remains a modicum of human control, focusing on Algorithmic Interpretability/Explainability, Robustness and Legality. For example, the concern is that, in the future, an ecology of trading algorithms across different institutions may 'conspire' and become unintentionally fraudulent (cf. LIBOR) or subject to subversion through compromised datasets (e.g. Microsoft Tay). New and unique forms of systemic risks can emerge, potentially coming from excessive algorithmic complexity. The contribution of this paper is to review AI, ML and associated algorithms, their computational strengths and weaknesses, and discuss their future impact on the Capital Markets.","New York, NY, USA",14,"Koshiyama, Adriano and Firoozye, Nick and Treleaven, Philip",Proceedings of the First ACM International Conference on AI in Finance,10.1145/3383455.3422539,9781450375849,,,,"artificial intelligence, deep learning, finance, generative adversarial networks, machine learning, transfer learning","New York, New York",,,8,,Association for Computing Machinery,ICAIF '20,"Algorithms in future capital markets: a survey on AI, ML and associated algorithms in capital markets",https://doi.org/10.1145/3383455.3422539,,2021
inproceedings,10.1145/3437963.3441827,"The accurate and interpretable prediction of future events in time-series data often requires the capturing of representative patterns (or referred to as states) underpinning the observed data. To this end, most existing studies focus on the representation and recognition of states, but ignore the changing transitional relations among them. In this paper, we present evolutionary state graph, a dynamic graph structure designed to systematically represent the evolving relations (edges) among states (nodes) along time. We conduct analysis on the dynamic graphs constructed from the time-series data and show that changes on the graph structures (e.g., edges connecting certain state nodes) can inform the occurrences of events (i.e., time-series fluctuation). Inspired by this, we propose a novel graph neural network model, Evolutionary State Graph Network (EvoNet), to encode the evolutionary state graph for accurate and interpretable time-series event prediction. Specifically, EvoNet models both the node-level (state-to-state) and graph-level (segment-to-segment) propagation, and captures the node-graph (state-to-segment) interactions over time. Experimental results based on five real-world datasets show that our approach not only achieves clear improvements compared with 11 baselines, but also provides more insights towards explaining the results of event predictions.","New York, NY, USA",,"Hu, Wenjie and Yang, Yang and Cheng, Ziqiang and Yang, Carl and Ren, Xiang",Proceedings of the 14th ACM International Conference on Web Search and Data Mining,10.1145/3437963.3441827,9781450382977,,,,"evolutionary state graph, graph networks, time series prediction","Virtual Event, Israel",,,9,580–588,Association for Computing Machinery,WSDM '21,Time-Series Event Prediction with Evolutionary State Graph,https://doi.org/10.1145/3437963.3441827,,2021
inproceedings,10.1145/3490354.3494407,"In financial market, certain types of stochastic events are intrinsically impactful to the prediction of financial times series, such as stock return, while few existing research attempts have been made to incorporate stochastic event modeling to time series modeling in a principled way. In this paper, we present a pioneering study that fills this gap. In particular, we introduce a generic probabilistic model that captures 1) the inter-dependencies among stochastic events, and 2) the impact of these events on time series. To this end, we extend multivariate Hawkes process (MHP) and proximal graphical event model (PGEM) and apply this framework to modeling two financial events, companies' quarterly revenue releases and updates of consensus prediction of quarterly revenue, and their impacts on the mean and correlation structures of future stock return. Our model not only improves prediction of financial time series, but also promotes AI trust for finance by revealing the causal relationship among the events. Extensive experimental results based on real financial market data validate the effectiveness of our models in learning event impact and improving investment decision by incorporating stochastic event impacts.","New York, NY, USA",42,"Zhu, Yada and Chen, Wenyu and Zhang, Yang and Gao, Tian and Li, Jianbo",Proceedings of the Second ACM International Conference on AI in Finance,10.1145/3490354.3494407,9781450391481,,,,"graphical event, hawkes process, quarterly revenue, stock return, variance-covariance",Virtual Event,,,8,,Association for Computing Machinery,ICAIF '21,Probabilistic framework for modeling event shocks to financial time series,https://doi.org/10.1145/3490354.3494407,,2022
article,10.1145/3672556,"Successfully tackling many urgent challenges in socio-economically critical domains, such as public health and sustainability, requires a deeper understanding of causal relationships and interactions among a diverse spectrum of spatio-temporally distributed entities. In these applications, the ability to leverage spatio-temporal data to obtain causally based situational awareness and to develop informed forecasts to provide resilience at different scales is critical. While the promise of a causally grounded approach to these challenges is apparent, the core data technologies needed to achieve these are in the early stages and lack a framework to help realize their potential. In this article, we argue that there is an urgent need for a novel paradigm of spatio-causal research built on computational advances in spatio-temporal data and model integration, causal learning and discovery, large scale data- and model-driven simulations, emulations, and forecasting, as well as spatio-temporal data-driven and model-centric operational recommendations, and effective causally driven visualization and explanation. We thus provide a vision, and a road map, for spatio-causal situation awareness, forecasting, and planning.","New York, NY, USA",14,"Azad, Fahim Tasneema and Candan, K. Sel\c{c",,10.1145/3672556,,2374-0353,June 2024,ACM Trans. Spatial Algorithms Syst.,"Spatial algorithms, spatial big data, causal discovery",,,2,42,,Association for Computing Machinery,,"(Vision Paper) A Vision for Spatio-Causal Situation Awareness, Forecasting, and Planning",https://doi.org/10.1145/3672556,10,2024
inproceedings,10.1145/3468264.3468543,"In large-scale online service systems, software changes are inevitable and frequent. Due to importing new code or configurations, changes are likely to incur incidents and destroy user experience. Thus it is essential for engineers to identify bad software changes, so as to reduce the influence of incidents and improve system re- liability. To better understand bad software changes, we perform the first empirical study based on large-scale real-world data from a large commercial bank. Our quantitative analyses indicate that about 50.4\% of incidents are caused by bad changes, mainly be- cause of code defect, configuration error, resource contention, and software version. Besides, our qualitative analyses show that the current practice of detecting bad software changes performs not well to handle heterogeneous multi-source data involved in soft- ware changes. Based on the findings and motivation obtained from the empirical study, we propose a novel approach named SCWarn aiming to identify bad changes and produce interpretable alerts accurately and timely. The key idea of SCWarn is drawing support from multimodal learning to identify anomalies from heterogeneous multi-source data. An extensive study on two datasets with various bad software changes demonstrates our approach significantly outperforms all the compared approaches, achieving 0.95 F1-score on average and reducing MTTD (mean time to detect) by 20.4\%∼60.7\%. In particular, we shared some success stories and lessons learned from the practical usage.","New York, NY, USA",,"Zhao, Nengwen and Chen, Junjie and Yu, Zhaoyang and Wang, Honglin and Li, Jiesong and Qiu, Bin and Xu, Hongyu and Zhang, Wenchi and Sui, Kaixin and Pei, Dan",Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,10.1145/3468264.3468543,9781450385626,,,,"Anomaly Detection, Online Service Systems, Software Change","Athens, Greece",,,13,527–539,Association for Computing Machinery,ESEC/FSE 2021,Identifying bad software changes via multimodal anomaly detection for online service systems,https://doi.org/10.1145/3468264.3468543,,2021
article,10.1145/3539732,"The rapid growth in popularity of online social networks provides new opportunities in computer science, sociology, math, information studies, biology, business, and more. Social network analysis (SNA) is a paramount technique supporting understanding social relationships and networks. Accordingly, certain studies and reviews have been presented focusing on information dissemination, influence analysis, link prediction, and more. However, the ultimate aim is for social network background knowledge and analysis to solve real-world social network problems. SNA still has several research challenges in this context, including users’ privacy in online social networks. Inspired by these facts, we have presented a survey on social network analysis techniques, visualization, structure, privacy, and applications. This detailed study has started with the basics of network representation, structure, and measures. Our primary focus is on SNA applications with state-of-the-art techniques. We further provide a comparative analysis of recent developments on SNA problems in the sequel. The privacy preservation with SNA is also surveyed. In the end, research challenges and future directions are discussed to suggest to researchers a starting point for their research.","New York, NY, USA",137,"Singh, Shashank Sheshar and Srivastava, Vishal and Kumar, Ajay and Tiwari, Shailendra and Singh, Dilbag and Lee, Heung-No",,10.1145/3539732,,2375-4699,May 2023,ACM Trans. Asian Low-Resour. Lang. Inf. Process.,"Information diffusion, influence maximization, link prediction, community detection, social network analysis",,,5,47,,Association for Computing Machinery,,"Social Network Analysis: A Survey on Measure, Structure, Language Information Analysis, Privacy, and Applications",https://doi.org/10.1145/3539732,22,2023
article,10.1145/3441141,"Cyberbullying is rapidly becoming one of the most serious online risks for adolescents. This has motivated work on machine learning methods to automate the process of cyberbullying detection, which have so far mostly viewed cyberbullying as one-off incidents that occur at a single point in time. Comparatively less is known about how cyberbullying behavior occurs and evolves over time. This oversight highlights a crucial open challenge for cyberbullying-related research, given that cyberbullying is typically defined as intentional acts of aggression via electronic communication that occur repeatedly and persistently. In this article, we center our discussion on the challenge of modeling temporal patterns of cyberbullying behavior. Specifically, we investigate how temporal information within a social media session, which has an inherently hierarchical structure (e.g., words form a comment and comments form a session), can be leveraged to facilitate cyberbullying detection. Recent findings from interdisciplinary research suggest that the temporal characteristics of bullying sessions differ from those of non-bullying sessions and that the temporal information from users’ comments can improve cyberbullying detection. The proposed framework consists of three distinctive features: (1) a hierarchical structure that reflects how a social media session is formed in a bottom-up manner; (2) attention mechanisms applied at the word- and comment-level to differentiate the contributions of words and comments to the representation of a social media session; and (3) the incorporation of temporal features in modeling cyberbullying behavior at the comment-level. Quantitative and qualitative evaluations are conducted on a real-world dataset collected from Instagram, the social networking site with the highest percentage of users reporting cyberbullying experiences. Results from empirical evaluations show the significance of the proposed methods, which are tailored to capture temporal patterns of cyberbullying detection.","New York, NY, USA",8,"Cheng, Lu and Guo, Ruocheng and Silva, Yasin N. and Hall, Deborah and Liu, Huan",,10.1145/3441141,,2691-1922,May 2021,ACM/IMS Trans. Data Sci.,"Cyberbullying, temporal analysis, hierarchical attention network, social media",,,2,23,,Association for Computing Machinery,,Modeling Temporal Patterns of Cyberbullying Detection with Hierarchical Attention Networks,https://doi.org/10.1145/3441141,2,2021
article,10.5555/3722577.3722622,"Probabilistic forecasting relies on past observations to provide a probability distribution for a future outcome, which is often evaluated against the realization using a scoring rule. Here, we perform probabilistic forecasting with generative neural networks, which parametrize distributions on high-dimensional spaces by transforming draws from a latent variable. Generative networks are typically trained in an adversarial framework. In contrast, we propose to train generative networks to minimize a predictive-sequential (or prequential) scoring rule on a recorded temporal sequence of the phenomenon of interest, which is appealing as it corresponds to the way forecasting systems are routinely evaluated. Adversarial-free minimization is possible for some scoring rules; hence, our framework avoids the cumbersome hyperparameter tuning and uncertainty underestimation due to unstable adversarial training, thus unlocking reliable use of generative networks in probabilistic forecasting. Further, we prove consistency of the minimizer of our objective with dependent data, while adversarial training assumes independence. We perform simulation studies on two chaotic dynamical models and a benchmark data set of global weather observations; for this last example, we define scoring rules for spatial data by drawing from the relevant literature. Our method outperforms state-of-the-art adversarial approaches, especially in probabilistic calibration, while requiring less hyperparameter tuning.",,45,"Pacchiardi, Lorenzo and Adewoyin, Rilwan A. and Dueben, Peter and Dutta, Ritabrata",,,,1532-4435,January 2024,J. Mach. Learn. Res.,"generative networks, GAN, probabilistic forecasting, scoring rules, adversarial free",,,1,64,,JMLR.org,,Probabilistic forecasting with generative networks via scoring rule minimization,,25,2024
inproceedings,10.1145/3626246.3653378,"Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.","New York, NY, USA",,"Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i",Companion of the 2024 International Conference on Management of Data,10.1145/3626246.3653378,9798400704222,,,,"containers, kubernetes, resource optimization, vertical auto-scaling","Santiago AA, Chile",,,14,241–254,Association for Computing Machinery,SIGMOD '24,Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud,https://doi.org/10.1145/3626246.3653378,,2024
